<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>Miscellaneous Observations on Randomization</title><meta name="citation_title" content="Miscellaneous Observations on Randomization"><meta name="citation_pdf_url" content="https://peteroupc.github.io/randmisc.pdf"><meta name="citation_url" content="https://peteroupc.github.io/randmisc.html"><meta name="citation_date" content="2022/04/19"><meta name="citation_online_date" content="2022/04/19"><meta name="og:title" content="Miscellaneous Observations on Randomization"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/randmisc.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="Miscellaneous Observations on Randomization"><meta name="author" content="Peter Occil"/><meta name="citation_author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css">
            <script type="text/x-mathjax-config"> MathJax.Hub.Config({"HTML-CSS": { availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, preferredFont: "TeX" },
                    tex2jax: { displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], processEscapes: true } });
            </script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>Miscellaneous Observations on Randomization</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><a id=Contents></a></p>

<h2>Contents</h2>

<ul>
<li><a href="#Contents"><strong>Contents</strong></a></li>
<li><a href="#On_a_Binomial_Sampler"><strong>On a Binomial Sampler</strong></a></li>
<li><a href="#On_a_Geometric_Sampler"><strong>On a Geometric Sampler</strong></a></li>
<li><a href="#Sampling_Unbounded_Monotone_Density_Functions"><strong>Sampling Unbounded Monotone Density Functions</strong></a></li>
<li><a href="#Certain_Families_of_Distributions"><strong>Certain Families of Distributions</strong></a></li>
<li><a href="#Certain_Distributions"><strong>Certain Distributions</strong></a></li>
<li><a href="#Batching_Random_Samples_via_Randomness_Extraction"><strong>Batching Random Samples via Randomness Extraction</strong></a></li>
<li><a href="#Random_Variate_Generation_via_Quantiles"><strong>Random Variate Generation via Quantiles</strong></a></li>
<li><a href="#ExpoExact"><strong>ExpoExact</strong></a></li>
<li><a href="#A_sampler_for_distributions_with_nonincreasing_or_nondecreasing_weights"><strong>A sampler for distributions with nonincreasing or nondecreasing weights</strong></a></li>
<li><a href="#A_sampler_for_unimodal_distributions_of_weights"><strong>A sampler for unimodal distributions of weights</strong></a></li>
<li><a href="#Log_Uniform_Distribution"><strong>Log-Uniform Distribution</strong></a></li>
<li><a href="#Notes"><strong>Notes</strong></a></li>
<li><a href="#License"><strong>License</strong></a></li>
</ul>

<p><a id=On_a_Binomial_Sampler></a></p>

<h2>On a Binomial Sampler</h2>

<p>Take the following sampler of a binomial(<em>n</em>, 1/2) distribution (where <em>n</em> is even), which is equivalent to the one that appeared in (Bringmann et al. 2014)[^1], and adapted to be more programmer-friendly.</p>

<ol>
<li>If <em>n</em> is less than 4, generate <em>n</em> unbiased random bits (zeros or ones) and return their sum.  Otherwise, if <em>n</em> is odd, set <em>ret</em> to the result of this algorithm with <em>n</em> = <em>n</em> &minus; 1, then add an unbiased random bit&#39;s value to <em>ret</em>, then return <em>ret</em>.</li>
<li>Set <em>m</em> to floor(sqrt(<em>n</em>)) + 1.</li>
<li>(First, sample from an envelope of the binomial curve.) Generate unbiased random bits until a zero is generated this way.  Set <em>k</em> to the number of ones generated this way.</li>
<li>Set <em>s</em> to an integer in [0, <em>m</em>) chosen uniformly at random, then set <em>i</em> to <em>k</em>*<em>m</em> + <em>s</em>.</li>
<li>Generate an unbiased random bit.  If that bit is 0, set <em>ret</em> to (<em>n</em>/2)+<em>i</em>.  Otherwise, set <em>ret</em> to (<em>n</em>/2)&minus;<em>i</em>&minus;1.</li>
<li>(Second, accept or reject <em>ret</em>.) If <em>ret</em> &lt; 0 or <em>ret</em> &gt; <em>n</em>, go to step 3.</li>
<li>With probability choose(<em>n</em>, <em>ret</em>)*<em>m</em>*2<sup><em>k</em>&minus;<em>n</em>&minus;2</sup>, return <em>ret</em>.  Otherwise, go to step 3. (Here, choose(<em>n</em>, <em>k</em>) is a <em>binomial coefficient</em>, or the number of ways to choose <em>k</em> out of <em>n</em> labeled items.[^2])</li>
</ol>

<p>This algorithm has an acceptance rate of 1/16 regardless of the value of <em>n</em>.  However, step 7 will generally require a growing amount of storage and time to exactly calculate the given probability as <em>n</em> gets larger, notably due to the inherent factorial in the binomial coefficient.  The Bringmann paper suggests approximating this factorial via Spouge&#39;s approximation; however, it seems hard to do so without using floating-point arithmetic, which the paper ultimately resorts to. Alternatively, the logarithm of that probability can be calculated, then an exponential random variate can be generated, negated, and compared with that logarithm to determine whether the step succeeds.</p>

<p>More specifically, step 7 can be changed as follows:</p>

<ul>
<li>(7.) Let <em>p</em> be loggamma(<em>n</em>+1)&minus;loggamma(<em>ret</em>+1)&minus;loggamma((<em>n</em>&minus;<em>ret</em>)+1)+ln(<em>m</em>)+ln(2)*(<em>k</em>&minus;<em>n</em>&minus;2) (where loggamma(<em>x</em>) is the logarithm of the gamma function).</li>
<li>(7a.) Generate an exponential random variate with rate 1 (which is the negative natural logarithm of a uniform(0,1) random variate).  Set <em>h</em> to 0 minus that number.</li>
<li>(7b.) If <em>h</em> is greater than <em>p</em>, go to step 3.  Otherwise, return <em>ret</em>. (This step can be replaced by calculating lower and upper bounds that converge to <em>p</em>.  In that case, go to step 3 if <em>h</em> is greater than the upper bound, or return <em>ret</em> if <em>h</em> is less than the lower bound, or compute better bounds and repeat this step otherwise.  See also chapter 4 of (Devroye 1986)[^3].)</li>
</ul>

<p>My implementation of loggamma and the natural logarithm (<a href="https://peteroupc.github.io/betadist.py"><strong>betadist.py</strong></a>) relies on so-called &quot;constructive reals&quot; as well as a fast converging version of Stirling&#39;s formula for the factorial&#39;s natural logarithm (Schumacher 2016)[^4].</p>

<p>Also, according to the Bringmann paper, <em>m</em> can be set such that <em>m</em> is in the interval [sqrt(<em>n</em>), sqrt(<em>n</em>)+3], so I implement step 1 by starting with <em>u</em> = 2<sup>floor((1+<em>&beta;</em>(<em>n</em>))/2)</sup>, then calculating <em>v</em> = floor((<em>u</em>+floor(<em>n</em>/<em>u</em>))/2), <em>w</em> = <em>u</em>, <em>u</em> = <em>v</em>  until <em>v</em> &ge; <em>w</em>, then setting <em>m</em> to <em>w</em> + 1.  Here, <em>&beta;</em>(<em>n</em>) = ceil(ln(<em>n</em>+1)/ln(2)), or alternatively the minimum number of bits needed to store <em>n</em> (with <em>&beta;</em>(0) = 0).</p>

<blockquote>
<p><strong>Notes:</strong></p>

<ul>
<li>A binomial(<em>n</em>, 1/2) random variate, where <em>n</em> is odd, can be generated by adding an unbiased random bit&#39;s value (either zero or one with equal probability) to a binomial(<em>n</em>&minus;1, 1/2) random variate.</li>
<li>As pointed out by Farach-Colton and Tsai (2015)[^5], a binomial(<em>n</em>, <em>p</em>) random variate, where <em>p</em> is in the interval (0, 1), can be generated using binomial(<em>n</em>, 1/2) numbers using a procedure equivalent to the following:

<ol>
<li>Set <em>k</em> to 0 and <em>ret</em> to 0.</li>
<li>If the binary digit at position <em>k</em> after the point in <em>p</em>&#39;s binary expansion (that is, 0.bbbb... where each b is a zero or one) is 1, add a binomial(<em>n</em>, 1/2) random variate to <em>ret</em> and subtract the same variate from <em>n</em>; otherwise, set <em>n</em> to a binomial(<em>n</em>, 1/2) random variate.</li>
<li>If <em>n</em> is greater than 0, add 1 to <em>k</em> and go to step 2; otherwise, return <em>ret</em>. (Positions start at 0 where 0 is the most significant digit after the point, 1 is the next, etc.)</li>
</ol></li>
</ul>
</blockquote>

<p><a id=On_a_Geometric_Sampler></a></p>

<h2>On a Geometric Sampler</h2>

<p>The following algorithm is equivalent to the geometric(<em>px</em>/<em>py</em>) sampler that appeared in (Bringmann and Friedrich 2013)[^6], but adapted to be more programmer-friendly.  As used in that paper, a geometric(<em>p</em>) random variate expresses the number of failing trials before the first success, where each trial is independent and has success probability <em>p</em>. (Note that the terminology &quot;geometric random variate&quot; has conflicting meanings in academic works.  Note also that the algorithm uses the rational number <em>px</em>/<em>py</em>, not an arbitrary real number <em>p</em>; some of the notes in this section indicate how to adapt the algorithm to an arbitrary <em>p</em>.)</p>

<ol>
<li>Set <em>pn</em> to <em>px</em>, <em>k</em> to 0, and <em>d</em> to 0.</li>
<li>While <em>pn</em>*2 &le; <em>py</em>, add 1 to <em>k</em> and multiply <em>pn</em> by 2.  (Equivalent to finding the largest <em>k</em> &ge; 0 such that <em>p</em>*2<sup><em>k</em></sup> &le; 1.  For the case when <em>p</em> need not be rational, enough of its binary expansion can be calculated to carry out this step accurately, but in this case any <em>k</em> such that <em>p</em> is greater than 1/(2<sup><em>k</em>+2</sup>) and less than or equal to 1/(2<sup><em>k</em></sup>) will suffice, as the Bringmann paper points out.)</li>
<li>With probability (1&minus;<em>px</em>/<em>py</em>)<sup>2<sup><em>k</em></sup></sup>, add 1 to <em>d</em> and repeat this step. (To simulate this probability, the first sub-algorithm below can be used.)</li>
<li>Generate a uniform random integer in [0, 2<sup><em>k</em></sup>), call it <em>m</em>, then with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>m</em></sup>, return <em>d</em>*2<sup><em>k</em></sup>+<em>m</em>. Otherwise, repeat this step. (The Bringmann paper, though, suggests to simulate this probability by sampling only as many bits of <em>m</em> as needed to do so, rather than just generating <em>m</em> in one go, then using the first sub-algorithm on <em>m</em>.  However, the implementation, given as the second sub-algorithm below, is much more complicated and is not crucial for correctness.)</li>
</ol>

<p>The first sub-algorithm returns 1 with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>n</em></sup>, assuming that <em>n</em>*<em>px</em>/<em>py</em> &le; 1.  It implements the approach from the Bringmann paper by rewriting the probability using the binomial theorem. (More generally, to return 1 with probability (1&minus;<em>p</em>)<sup><em>n</em></sup>, it&#39;s enough to flip a coin that shows heads with probability <em>p</em>, <em>n</em> times or until it shows heads, whichever comes first, and then return either 1 if all the flips showed tails, or 0 otherwise.  See also &quot;<a href="https://peteroupc.github.io/bernoulli.html"><strong>Bernoulli Factory Algorithms</strong></a>&quot;.)</p>

<ol>
<li>Set <em>pnum</em>, <em>pden</em>, and <em>j</em>  to 1, then set <em>r</em> to 0, then set <em>qnum</em> to <em>px</em>, and <em>qden</em> to <em>py</em>, then set <em>i</em> to 2.</li>
<li>If <em>j</em> is greater than <em>n</em>, go to step 5.</li>
<li>If <em>j</em> is even, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> + <em>pden</em>*<em>qnum</em>*choose(<em>n</em>,<em>j</em>). Otherwise, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> &minus; <em>pden</em>*<em>qnum</em>*choose(<em>n</em>,<em>j</em>).</li>
<li>Multiply <em>pden</em> by <em>qden</em>, then multiply <em>qnum</em> by <em>px</em>, then multiply <em>qden</em> by <em>py</em>, then add 1 to <em>j</em>.</li>
<li>If <em>j</em> is less than or equal to 2 and less than or equal to <em>n</em>, go to step 2.</li>
<li>Multiply <em>r</em> by 2, then add an unbiased random bit&#39;s value (either 0 or 1 with equal probability) to <em>r</em>.</li>
<li>If <em>r</em> &le; floor((<em>pnum</em>*<em>i</em>)/<em>pden</em>) &minus; 2, return 1. If <em>r</em> &ge; floor((<em>pnum</em>*<em>i</em>)/<em>pden</em>) + 1, return 0.  If neither is the case, multiply <em>i</em> by 2 and go to step 2.</li>
</ol>

<p>The second sub-algorithm returns an integer <em>m</em> in [0, 2<sup><em>k</em></sup>) with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>m</em></sup>, or &minus;1 with the opposite probability.  It assumes that 2<sup><em>k</em></sup>*<em>px</em>/<em>py</em> &le; 1.</p>

<ol>
<li>Set <em>r</em> and <em>m</em> to 0.</li>
<li>Set <em>b</em> to 0, then while <em>b</em> is less than <em>k</em>:

<ol>
<li>(Sum <em>b</em>+2 summands of the binomial equivalent of the desired probability.  First, append an additional bit to <em>m</em>, from most to least significant.) Generate an unbiased random bit (either 0 or 1 with equal probability).  If that bit is 1, add 2<sup><em>k</em>&minus;<em>b</em></sup> to <em>m</em>.</li>
<li>(Now build up the binomial probability.) Set <em>pnum</em>, <em>pden</em>, and <em>j</em>  to 1, then set <em>qnum</em> to <em>px</em>, and <em>qden</em> to <em>py</em>.</li>
<li>If <em>j</em> is greater than <em>m</em> or greater than <em>b</em> + 2, go to the sixth substep.</li>
<li>If <em>j</em> is even, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> + <em>pden</em>*<em>qnum</em>*choose(<em>m</em>,<em>j</em>). Otherwise, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> &minus; <em>pden</em>*<em>qnum</em>*choose(<em>m</em>,<em>j</em>).</li>
<li>Multiply <em>pden</em> by <em>qden</em>, then multiply <em>qnum</em> by <em>px</em>, then multiply <em>qden</em> by <em>py</em>, then add 1 to <em>j</em>, then go to the third substep.</li>
<li>(Now check the probability.) Multiply <em>r</em> by 2, then add an unbiased random bit&#39;s value (either 0 or 1 with equal probability) to <em>r</em>.</li>
<li>If <em>r</em> &le; floor((<em>pnum</em>*2<sup><em>b</em></sup>)/<em>pden</em>) &minus; 2, add a uniform random integer in [0, 2<sup><em>k</em>*<em>b</em></sup>) to <em>m</em> and return <em>m</em> (and, if requested, the number <em>k</em>&minus;<em>b</em>&minus;1). If <em>r</em> &ge; floor((<em>pnum</em>*2<sup><em>b</em></sup>)/<em>pden</em>) + 1, return &minus;1 (and, if requested, an arbitrary value).  If neither is the case, add 1 to <em>b</em>.</li>
</ol></li>
<li>Add an unbiased random bit to <em>m</em>. (At this point, <em>m</em> is fully sampled.)</li>
<li>Run the first sub-algorithm with <em>n</em> = <em>m</em>, except in step 1 of that sub-algorithm, set <em>r</em> to the value of <em>r</em> built up by this algorithm, rather than 0, and set <em>i</em> to 2<sup><em>k</em></sup>, rather than 2.  If that sub-algorithm returns 1, return <em>m</em> (and, if requested, the number &minus;1).  Otherwise, return &minus;1 (and, if requested, an arbitrary value).</li>
</ol>

<p>As used in the Bringmann paper, a bounded geometric(<em>p</em>, <em>n</em>) random variate is a geometric(<em>p</em>) random variate or <em>n</em> (an integer greater than 0), whichever is less.  The following algorithm is equivalent to the algorithm given in that paper, but adapted to be more programmer-friendly.</p>

<ol>
<li>Set <em>pn</em> to <em>px</em>, <em>k</em> to 0, <em>d</em> to 0, and <em>m2</em> to the smallest power of 2 that is greater than <em>n</em> (or equivalently, 2<sup><em>bits</em></sup> where <em>bits</em> is the minimum number of bits needed to store <em>n</em>).</li>
<li>While <em>pn</em>*2 &le; <em>py</em>, add 1 to <em>k</em> and multiply <em>pn</em> by 2.</li>
<li>With probability (1&minus;<em>px</em>/<em>py</em>)<sup>2<sup><em>k</em></sup></sup>, add 1 to <em>d</em> and then either return <em>n</em> if <em>d</em>*2<sup><em>k</em></sup> is greater than or equal to <em>m2</em>, or repeat this step if less. (To simulate this probability, the first sub-algorithm above can be used.)</li>
<li>Generate a uniform random integer in [0, 2<sup><em>k</em></sup>), call it <em>m</em>, then with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>m</em></sup>, return min(<em>n</em>, <em>d</em>*2<sup><em>k</em></sup>+<em>m</em>). In the Bringmann paper, this step is implemented in a manner equivalent to the following (this alternative implementation, though, is not crucial for correctness):

<ol>
<li>Run the second sub-algorithm above, except return two values, rather than one, in the situations given in the sub-algorithm.  Call these two values <em>m</em> and <em>mbit</em>.</li>
<li>If <em>m</em> &lt; 0, go to the first substep.</li>
<li>If <em>mbit</em> &ge; 0, add 2<sup><em>mbit</em></sup> times an unbiased random bit to <em>m</em> and subtract 1 from <em>mbit</em>.  If that bit is 1 or <em>mbit</em> &lt; 0, go to the next substep; otherwise, repeat this substep.</li>
<li>Return <em>n</em> if <em>d</em>*2<sup><em>k</em></sup> is greater than or equal to <em>m2</em>.</li>
<li>Add a uniform random integer in [0, 2<sup><em>mbit</em>+1</sup>) to <em>m</em>, then return min(<em>n</em>, <em>d</em>*2<sup><em>k</em></sup>+<em>m</em>).</li>
</ol></li>
</ol>

<p><a id=Sampling_Unbounded_Monotone_Density_Functions></a></p>

<h2>Sampling Unbounded Monotone Density Functions</h2>

<p>This section shows a preprocessing algorithm to generate a random variate in the closed interval [0, 1] from a distribution whose probability density function (PDF)&mdash;</p>

<ul>
<li>is continuous in the interval [0, 1],</li>
<li>is monotonically decreasing in [0, 1], and</li>
<li>has an unbounded peak at 0.</li>
</ul>

<p>The trick here is to sample the peak in such a way that the result is either forced to be 0 or forced to belong to the bounded part of the PDF.  This algorithm does not require the area under the curve of the PDF in [0, 1] to be 1; in other words, this algorithm works even if the PDF is known up to a normalizing constant.  The algorithm is as follows.</p>

<ol>
<li>Set <em>i</em> to 1.</li>
<li>Calculate the cumulative probability of the interval [0, 2<sup>&minus;<em>i</em></sup>] and that of [0, 2<sup>&minus;(<em>i</em> &minus; 1)</sup>], call them <em>p</em> and <em>t</em>, respectively.</li>
<li>With probability <em>p</em>/<em>t</em>, add 1 to <em>i</em> and go to step 2. (Alternatively, if <em>i</em> is equal to or higher than the desired number of fractional bits in the result, return 0 instead of adding 1 and going to step 2.)</li>
<li>At this point, the PDF at [2<sup>&minus;<em>i</em></sup>, 2<sup>&minus;(<em>i</em> &minus; 1)</sup>) is bounded from above, so sample a random variate in this interval using any appropriate algorithm, including rejection sampling.  Because the PDF is monotonically decreasing, the peak of the PDF at this interval is located at 2<sup>&minus;<em>i</em></sup>, so that rejection sampling becomes trivial.</li>
</ol>

<p>It is relatively straightforward to adapt this algorithm for monotonically increasing PDFs with the unbounded peak at 1, or to PDFs with a different domain than [0, 1].</p>

<p>This algorithm is similar to the &quot;inversion&ndash;rejection&quot; algorithm mentioned in section 4.4 of chapter 7 of Devroye&#39;s <em>Non-Uniform Random Variate Generation</em> (1986)[^3].  I was unaware of that algorithm at the time I started writing the text that became this section (Jul. 25, 2020).  The difference here is that it assumes the whole distribution has support [0, 1] (&quot;support&quot; is defined later), while the algorithm presented in this article doesn&#39;t make that assumption (for example, the interval [0, 1] can cover only part of the distribution&#39;s support).</p>

<p>By the way, this algorithm arose while trying to devise an algorithm that can generate an integer power of a uniform random variate, with arbitrary precision, without actually calculating that power (a naïve calculation that is merely an approximation and usually introduces bias); for more information, see my other article on <a href="https://peteroupc.github.io/exporand.html"><strong>partially-sampled random numbers</strong></a>.  Even so, the algorithm I have come up with in this note may be of independent interest.</p>

<p>In the case of powers of a uniform random variate in the interval [0, 1], call the variate <em>X</em>, namely <em>X</em><sup><em>n</em></sup>, the ratio <em>p</em>/<em>t</em> in this algorithm has a very simple form, namely (1/2)<sup>1/<em>n</em></sup>.  Note that this formula is the same regardless of <em>i</em>. (To return 1 with probability (1/2)<sup>1/<em>n</em></sup>, the algorithm for <strong>(<em>a</em>/<em>b</em>)<sup><em>x</em>/<em>y</em></sup></strong> in &quot;<a href="https://peteroupc.github.io/bernoulli.html"><strong>Bernoulli Factory Algorithms</strong></a>&quot; can be used with <em>a</em>=1, <em>b</em>=2, <em>x</em>=1, and <em>y</em>=<em>n</em>.)  This is found by taking the PDF <em>f</em>(<em>x</em>) = <em>x</em><sup>1/<em>n</em></sup>/(<em>x</em> * <em>n</em>)</sup> and finding the appropriate <em>p</em>/<em>t</em> ratios by integrating <em>f</em> over the two intervals mentioned in step 2 of the algorithm.</p>

<p><a id=Certain_Families_of_Distributions></a></p>

<h2>Certain Families of Distributions</h2>

<p>This section is a note on certain families of univariate (one-variable) probability distributions, with emphasis on sampling random variates from them.  Some of these families are described in Ahmad et al. (2019)[^7], Jones (2015)[^8].</p>

<p>The following definitions are used:</p>

<ul>
<li>A distribution&#39;s <em>quantile function</em> (also known as <em>inverse cumulative distribution function</em> or <em>inverse CDF</em>) is a nondecreasing function that maps uniform random variates in the closed interval [0, 1] to numbers that follow the distribution.</li>
<li>A distribution&#39;s <em>support</em> is the set of values the distribution can take on, plus that set&#39;s endpoints.  For example, the beta distribution&#39;s support is the closed interval [0, 1], and the normal distribution&#39;s support is the entire real line.</li>
<li>The <em>zero-truncated Poisson</em> distribution: To generate a random variate that follows this distribution (with parameter <em>&lambda;</em> &gt; 0), generate Poisson variates with parameter <em>&lambda;</em> until a variate other than 0 is generated this way, then take the last generated variate.</li>
</ul>

<p><strong>G families.</strong> In general, families of the form &quot;X-G&quot; (such as &quot;beta-G&quot; (Eugene et al., 2002)[^9]) use two distributions, X and G, where&mdash;</p>

<ul>
<li>X is a distribution whose support is the closed interval [0, 1], and</li>
<li>G is a distribution with an easy-to-compute quantile function.</li>
</ul>

<p>The following algorithm samples a random variate following a distribution from this kind of family:</p>

<ol>
<li>Generate a random variate that follows the distribution X. (Or generate a uniform <a href="https://peteroupc.github.io/exporand.html"><strong>partially-sampled random number (PSRN)</strong></a> that follows the distribution X.)  Call the number <em>x</em>.</li>
<li>Calculate the quantile for G of <em>x</em>, and return that quantile. (If <em>x</em> is a uniform PSRN, see &quot;Random Variate Generation via Quantiles&quot;, later.)</li>
</ol>

<p>Certain special cases of the &quot;X-G&quot; families, such as the following, use a specially designed distribution for X:</p>

<ul>
<li>The <em>exp-G</em> family (Barreto-Souza and Simas 2010/2013)[^10], where X is an exponential distribution, truncated to the interval [0, 1], with parameter <em>&lambda;</em> &ge; 0; step 1 is modified to read: &quot;Generate <em>U</em>, a uniform random variate in the interval [0, 1], then set <em>x</em> to &minus;ln((exp(&minus;<em>&lambda;</em>)&minus;1)*<em>U</em> + 1)/<em>&lambda;</em> if <em>&lambda;</em> != 0, and <em>U</em> otherwise.&quot; (The <em>alpha power</em> or <em>alpha power transformed</em> family (Mahdavi and Kundu 2017)[^11] uses the same distribution for X, but with <em>&lambda;</em>=&minus;ln(<em>&alpha;</em>) where <em>&alpha;</em> is in (0, 1]; see also Jones (2018)[^12].)</li>
<li>The <em>exponentiated</em> family (Mudholkar and Srivastava 1993)[^13]. The family uses a shape parameter <em>a</em> &gt; 1; step 1 is modified to read: &quot;Generate <em>u</em>, a uniform random variate in the interval [0, 1], then set <em>x</em> to <em>u</em><sup>1/<em>a</em></sup>.&quot;</li>
<li>The <em>transmuted-G</em> family (Shaw and Buckley 2007)[^14]. The family uses a shape parameter <em>&eta;</em> in the interval [&minus;1, 1]; step 1 is modified to read: &quot;Generate a piecewise linear random variate in [0, 1] with weight 1&minus;<em>&eta;</em> at 0 and weight 1+<em>&eta;</em> at 1, call the number <em>x</em>. (It can be generated as follows, see also (Devroye 1986, p. 71-72)[^3]: With probability min(1&minus;<em>&eta;</em>, 1+<em>&eta;</em>), generate <em>x</em>, a uniform random variate in the interval [0, 1]. Otherwise, generate two uniform random variates in the interval [0, 1], set <em>x</em> to the higher of the two, then if <em>&eta;</em> is less than 0, set <em>x</em> to 1&minus;<em>x</em>.)&quot;. ((Granzotto et al. 2017)[^15] mentions the same distribution, but with a parameter <em>&lambda;</em> = <em>&eta;</em> + 1 lying in the interval [0, 2].)</li>
<li>A <em>cubic rank transmuted</em> distribution (Granzotto et al. 2017)[^15] uses parameters <em>&lambda;</em><sub>0</sub> and <em>&lambda;</em><sub>1</sub> in the interval [0, 1]; step 1 is modified to read: &quot;Generate three uniform random variates in the interval [0, 1], then sort them in ascending order.  Then, choose 1, 2, or 3 with probability proportional to these weights: [<em>&lambda;</em><sub>0</sub>, <em>&lambda;</em><sub>1</sub>, 3&minus;<em>&lambda;</em><sub>0</sub>&minus;<em>&lambda;</em><sub>1</sub>].  Then set <em>x</em> to the first, second, or third variate if 1, 2, or 3 is chosen this way, respectively.&quot;</li>
<li>Biweight distribution (Al-Khazaleh and Alzoubi 2021)[^52]: Step 1 is modified to read: &quot;Generate a uniform random variate <em>X</em> in [0, 1], then accept <em>X</em> with probability (1&minus;<em>X</em><sup>2</sup>)<sup>2</sup>, or repeat this process otherwise&quot;; or &quot;Generate a uniform PSRN <em>X</em> in [0, 1], then run <strong>SampleGeometricBag</strong> on that PSRN four times.  If the first two results are not both 1 and if the last two results are not both 1, accept <em>X</em>; otherwise, repeat this process.&quot;</li>
</ul>

<p><strong>Transformed&ndash;transformer family.</strong> In fact, the &quot;X-G&quot; families are a special case of the so-called &quot;transformed&ndash;transformer&quot; family of distributions introduced by Alzaatreh et al. (2013)[^16] that uses two distributions, X and G, where X (the &quot;transformed&quot;) is an arbitrary distribution with a PDF; G (the &quot;transformer&quot;) is a distribution with an easy-to-compute quantile function; and <em>W</em> is a nondecreasing function that, among other conditions, maps a number in [0, 1] to a number with the same support as X.  The following algorithm samples a random variate from this kind of family:</p>

<ol>
<li>Generate a random variate that follows the distribution X. (Or generate a uniform PSRN that follows X.) Call the number <em>x</em>.</li>
<li>Calculate <em>w</em> = <em>W</em><sup>&minus;1</sup>(<em>x</em>) (where <em>W</em><sup>&minus;1</sup>(.) is the inverse of <em>W</em>), then calculate the quantile for G of <em>w</em> and return that quantile. (If <em>x</em> is a uniform PSRN, see &quot;Random Variate Generation via Quantiles&quot;, later.)</li>
</ol>

<p>The following are special cases of the &quot;transformed&ndash;transformer&quot; family:</p>

<ul>
<li>The &quot;T-R{<em>Y</em>}&quot; family (Aljarrah et al., 2014)[^17], in which <em>T</em> is an arbitrary distribution with a PDF (X in the algorithm above), <em>R</em> is a distribution with an easy-to-compute quantile function (G in the algorithm above), and <em>W</em> is the quantile function for the distribution <em>Y</em>, whose support must contain the support of <em>T</em> (so that <em>W</em><sup>&minus;1</sup>(<em>x</em>) is the cumulative distribution function for <em>Y</em>, or the probability that a <em>Y</em>-distributed number is <em>x</em> or less).</li>
<li>Several versions of <em>W</em> have been proposed for the case when distribution X&#39;s support is [0, &infin;), such as the Rayleigh and gamma distributions.  They include:

<ul>
<li><em>W</em>(<em>x</em>) = &minus;ln(1&minus;<em>x</em>) (<em>W</em><sup>&minus;1</sup>(<em>x</em>) = 1&minus;exp(&minus;<em>x</em>)).  Suggested in the original paper by Alzaatreh et al.</li>
<li><em>W</em>(<em>x</em>) = <em>x</em>/(1&minus;<em>x</em>) (<em>W</em><sup>&minus;1</sup>(<em>x</em>) = <em>x</em>/(1+<em>x</em>)).  Suggested in the original paper by Alzaatreh et al.  This choice forms the so-called &quot;odd X G&quot; family, and one example is the &quot;odd log-logistic G&quot; family (Gleaton and Lynch 2006)[^18].</li>
</ul></li>
</ul>

<p>Many special cases of the &quot;transformed&ndash;transformer&quot; family have been proposed in many papers, and usually their names suggest the distributions that make up this family.  Some members of the &quot;odd X G&quot; family have names that begin with the word &quot;generalized&quot;, and in most such cases this corresponds to <em>W</em><sup>&minus;1</sup>(<em>x</em>) = (<em>x</em>/(1+<em>x</em>))<sup>1/<em>a</em></sup>, where <em>a</em> &gt; 0 is a shape parameter; examples include the &quot;generalized odd gamma-G&quot; family (Hosseini et al. 2018)[^19].</p>

<p>A family very similar to the &quot;transformed&ndash;transformer&quot; family uses a <em>decreasing</em> <em>W</em>.</p>

<ul>
<li>When distribution X&#39;s support is [0, &infin;), one such <em>W</em> that has been proposed is <em>W</em>(<em>x</em>) = &minus;ln(<em>x</em>) (<em>W</em><sup>&minus;1</sup>(<em>x</em>) = exp(&minus;<em>x</em>); examples include the &quot;Rayleigh-G&quot; family or &quot;Rayleigh&ndash;Rayleigh&quot; distribution (Al Noor and Assi 2020)[^20], as well as the &quot;generalized gamma-G&quot; family, where &quot;generalized gamma&quot; refers to the Stacy distribution (Boshi et al. 2020)[^21]).</li>
</ul>

<p><strong>Minimums, maximums, and sums.</strong> Some distributions are described as a minimum, maximum, or sum of <em>N</em> independent random variates distributed as <em>X</em>, where <em>N</em> &ge; 1 is an independent integer distributed as the discrete distribution <em>Y</em>.</p>

<ul>
<li>Tahir and Cordeiro (2016)[^22] calls a distribution of minimums a <em>compound distribution</em>, and a distribution of maximums a <em>complementary compound distribution</em>.</li>
<li>Pérez-Casany et al. (2016)[^23] calls a distribution of minimums or of maximums a <em>random-stopped extreme distribution</em>.</li>
<li>A distribution of sums can be called a <em>stopped-sum distribution</em> (Johnson et al. 2005)[^24]. (In this case, <em>N</em> can be 0 so that <em>N</em> &ge; 0 is an integer distributed as <em>Y</em>.)</li>
</ul>

<p>A variate following a distribution of minimums or of maximums can be generated as follows (Duarte-López et al. 2021)[^25]:</p>

<ol>
<li>Generate a uniform random variate in (0, 1). (Or generate a uniform PSRN with integer part 0, positive sign, and empty fractional part.)  Call the number <em>x</em>.</li>
<li>For minimums, calculate the quantile for <em>X</em> of 1&minus;<em>W</em><sup>&minus;1</sup>(<em>x</em>) (where <em>W</em><sup>&minus;1</sup>(.) is the inverse of <em>Y</em>&#39;s probability generating function), and return that quantile.[^26] (If <em>x</em> is a uniform PSRN, see &quot;Random Variate Generation via Quantiles&quot;, later.  <em>Y</em>&#39;s probability generating function is <em>W</em>(<em>z</em>) = <em>a</em>[0]*<em>z</em><sup>0</sup> + <em>a</em>[1]*<em>z</em><sup>1</sup> + ..., where 0 &lt; <em>z</em> &lt; 1 and <em>a</em>[<em>i</em>] is the probability that a <em>Y</em>-distributed variate equals <em>i</em>.  See example below.)</li>
<li>For maximums, calculate the quantile for <em>X</em> of <em>W</em><sup>&minus;1</sup>(<em>x</em>), and return that quantile.</li>
</ol>

<blockquote>
<p><strong>Examples:</strong></p>

<table><thead>
<tr>
<th>This distribution:</th>
<th>Is a distribution of:</th>
<th>Where <em>X</em> is:</th>
<th>And <em>Y</em> is:</th>
</tr>
</thead><tbody>
<tr>
<td>Geometric zero-truncated Poisson (Akdoğan et al., 2020)[^27].</td>
<td>Maximums.</td>
<td>1 plus the number of failures before the first success, with each success having the same probability.</td>
<td>Zero-truncated Poisson.</td>
</tr>
<tr>
<td>Exponential Poisson (Kuş 2007)[^28].</td>
<td>Minimums.</td>
<td>Exponential.</td>
<td>Zero-truncated Poisson.</td>
</tr>
<tr>
<td>Poisson exponential (Cancho et al. 2011)[^29].</td>
<td>Maximums.</td>
<td>Exponential.</td>
<td>Zero-truncated Poisson.</td>
</tr>
<tr>
<td>Right-truncated Weibull(<em>a</em>, <em>b</em>, <em>c</em>) (Jodrá 2020)[^30] (<em>a</em>, <em>b</em>, and <em>c</em> are greater than 0).</td>
<td>Minimums.</td>
<td>Power function(<em>b</em>, <em>c</em>).</td>
<td>Zero-truncated Poisson(<em>a</em>*<em>c</em><sup><em>b</em></sup>).</td>
</tr>
</tbody></table>

<p><strong>Example:</strong> If <em>Y</em> is zero-truncated Poisson with parameter <em>&lambda;</em>, its probability generating function is $W(z)=\frac{1-\exp(z\lambda)}{1-\exp(\lambda)}$, and solving for <em>x</em> leads to its inverse: $W^{-1}(x)=\ln(1-x+x\times\exp(\lambda))/\lambda$.</p>
</blockquote>

<p><strong>Inverse distributions.</strong> An <em>inverse X distribution</em> (or <em>inverted X distribution</em>) is generally the distribution of 1 divided by a random variate distributed as <em>X</em>.  For example, an <em>inverse exponential</em> random variate (Keller and Kamath 1982)[^31] is 1 divided by an exponential random variate with rate 1 (and so is distributed as &minus;1/ln(<em>U</em>) where <em>U</em> is a uniform random variate in the interval [0, 1]) and may be multiplied by a parameter <em>&theta;</em> &gt; 0.</p>

<p><strong>Weighted distributions.</strong> A <em>weighted X distribution</em> uses a distribution X and a weight function <em>w</em>(<em>x</em>) whose values lie in [0, 1] everywhere in X&#39;s support.  The following algorithm samples from a weighted distribution (see also (Devroye 1986, p. 47)[^3]):</p>

<ol>
<li>Generate a random variate that follows the distribution X. (Or generate a uniform PSRN that follows X.) Call the number <em>x</em>.</li>
<li>With probability <em>w</em>(<em>x</em>), return <em>x</em>.  Otherwise, go to step 1.</li>
</ol>

<p>Some weighted distributions allow any weight function <em>w</em>(<em>x</em>) whose values are non-negative everywhere in X&#39;s support (Rao 1985)[^32].  (If <em>w</em>(<em>x</em>) = <em>x</em>, the distribution is often called a <em>length-biased</em> or <em>size-biased distribution</em>; if <em>w</em>(<em>x</em>) = <em>x</em><sup>2</sup>, <em>area-biased</em>.)  Their PDFs are proportional to the original PDFs multiplied by <em>w</em>(<em>x</em>).</p>

<p><strong>Inflated distributions.</strong> To generate an <em>inflated X</em> (also called <em>c-inflated X</em> or <em>c-adjusted X</em>) random variate with parameters <em>c</em> and <em>&alpha;</em>, generate&mdash;</p>

<ul>
<li><em>c</em> with probability <em>&alpha;</em>, and</li>
<li>a random variate distributed as X otherwise.</li>
</ul>

<p>For example, a <em>zero-inflated beta</em> random variate is 0 with probability <em>&alpha;</em> and a beta random variate otherwise (the parameter <em>c</em> is 0) (Ospina and Ferrari 2010)[^33]  A zero-and-one inflated X distribution is 0 or 1 with probability <em>&alpha;</em> and distributed as X otherwise.  For example, to generate a <em>zero-and-one-inflated unit Lindley</em> random variate (with parameters <em>&alpha;</em>, <em>&theta;</em>, and <em>p</em>) (Chakraborty and Bhattacharjee 2021)[^34]:</p>

<ol>
<li>With probability <em>&alpha;</em>, return a number that is 0 with probability <em>p</em> and 1 otherwise.</li>
<li>Generate a unit Lindley(<em>&theta;</em>) random variate, that is, generate <em>x</em>/(1+<em>x</em>) where <em>x</em> is a <a href="https://peteroupc.github.io/morealg.html#Lindley_Distribution_and_Lindley_Like_Mixtures"><strong>Lindley(<em>&theta;</em>) random variate</strong></a>.</li>
</ol>

<blockquote>
<p><strong>Note:</strong> A zero-inflated X distribution where X takes on 0 with probability 0 is also called a <em>hurdle distribution</em> (Mullahy 1986)[^35].</p>
</blockquote>

<p><strong>Unit distributions.</strong> To generate a <em>unit X</em> random variate (where X is a distribution whose support is the positive real line), generate a random variate distributed as X, call it <em>x</em>, then return exp(&minus;<em>x</em>) or 1 &minus;exp(&minus;<em>x</em>) (also known as &quot;Type I&quot; or &quot;Type II&quot;, respectively).  For example, a unit gamma distribution is also known as the <em>Grassia distribution</em> (Grassia 1977)[^36].</p>

<p><strong>CDF&ndash;quantile family.</strong> Given two distributions X and Y (which can be the same), a location parameter <em>&mu;</em> &ge; 0, and a dispersion parameter <em>&sigma;</em>&gt;0, a variate from this family of distributions can be generated as follows (Smithson and Shou 2019)[^42]:</p>

<ol>
<li>Generate a random variate that follows the distribution X. (Or generate a uniform PSRN that follows X.) Call the number <em>x</em>.</li>
<li>If distribution X&#39;s support is the positive real line, calculate <em>x</em> as ln(<em>x</em>).</li>
<li>Calculate <em>z</em> as <em>&mu;</em>+<em>&sigma;</em>*<em>x</em>.</li>
<li>If distribution Y&#39;s support is the positive real line, calculate <em>z</em> as exp(<em>z</em>).</li>
<li>Return <em>H</em>(<em>z</em>).</li>
</ol>

<p>In this algorithm:</p>

<ul>
<li>X and Y are distributions that each have support on either the whole real line or the positive real line.  However, the book intends X to have an easy-to-compute quantile function.</li>
<li><em>H</em>(<em>z</em>) is Y&#39;s <em>cumulative distribution function</em>, or the probability that a Y-distributed random variate is <em>z</em> or less.  The book likewise intends <em>H</em> to be easy to compute.</li>
</ul>

<blockquote>
<p><strong>Note:</strong> An important property for use in statistical estimation is <em>identifiability</em>.  A family of distributions is <em>identifiable</em> if it has the property that if two parameter vectors (<em>&theta;</em><sub>1</sub> and <em>&theta;</em><sub>2</sub>) determine the same distribution, then <em>&theta;</em><sub>1</sub> must equal <em>&theta;</em><sub>2</sub>.</p>
</blockquote>

<p><a id=Certain_Distributions></a></p>

<h2>Certain Distributions</h2>

<p>In the table below, <em>U</em> is a uniform random variate in the interval [0, 1], and all random variates are independently generated.</p>

<table><thead>
<tr>
<th>This distribution:</th>
<th>Is distributed as:</th>
<th>And uses these parameters:</th>
</tr>
</thead><tbody>
<tr>
<td>Power function(<em>a</em>, <em>c</em>).</td>
<td><em>c</em>*<em>U</em><sup>1/<em>a</em></sup>.</td>
<td><em>a</em> &gt; 0, <em>c</em> &gt; 0.</td>
</tr>
<tr>
<td>Lehmann Weibull(<em>a1</em>, <em>a2</em>, <em>&beta;</em>) (Elgohari and Yousof 2020)[^37].</td>
<td>(ln(1/<em>U</em>)/<em>&beta;</em>)<sup>1/<em>a1</em></sup>/<em>a2</em> or <em>E</em><sup>1/<em>a1</em></sup>/<em>a2</em></td>
<td><em>a1</em>, <em>a2</em>, <em>&beta;</em> &gt; 0. <em>E</em> is exponential with rate <em>&beta;</em>.</td>
</tr>
<tr>
<td>Marshall&ndash;Olkin(<em>&alpha;</em>) (Marshall and Olkin 1997)[^38]</td>
<td>(1&minus;<em>U</em>)/(<em>U</em>*(<em>&alpha;</em>&minus;1) + 1).</td>
<td><em>&alpha;</em> in [0, 1].</td>
</tr>
<tr>
<td>Lomax(<em>&alpha;</em>).</td>
<td>(1&minus;<em>U</em>)<sup>&minus;1/<em>&alpha;</em></sup>&minus;1.</td>
<td><em>&alpha;</em> &gt; 0.</td>
</tr>
<tr>
<td>Power Lomax(<em>&alpha;</em>, <em>&beta;</em>) (Rady et al. 2016)[^39].</td>
<td><em>L</em><sup>1/<em>&beta;</em></sup></td>
<td><em>&beta;</em> &gt; 0; <em>L</em> is Lomax(<em>&alpha;</em>).</td>
</tr>
<tr>
<td>Topp&ndash;Leone(<em>&alpha;</em>).</td>
<td>1&minus;sqrt(1&minus;<em>U</em><sup>1/<em>&alpha;</em></sup>).</td>
<td><em>&alpha;</em> &gt; 0.</td>
</tr>
<tr>
<td>Bell&ndash;Touchard(<em>a</em>, <em>b</em>) (Castellares et al. 2020)[^40].</td>
<td>Sum of <em>N</em> zero-truncated Poisson(<em>a</em>) random variates, where <em>N</em> is Poisson with parameter <em>b</em>*exp(<em>a</em>)&minus;<em>b</em>.[^41]</td>
<td><em>a</em>&gt;0, <em>b</em>&gt;0.</td>
</tr>
<tr>
<td>Bell(<em>a</em>) (Castellares et al. 2020)[^40].</td>
<td>Bell&ndash;Touchard(<em>a</em>, 0).</td>
<td><em>a</em>&gt;0.</td>
</tr>
<tr>
<td>Neyman type A(<em>&delta;</em>, <em>&tau;</em>) (Batsidis and Lemonte 2021)[^42]</td>
<td>Bell&ndash;Touchard(<em>&tau;</em>, <em>&delta;</em>*exp(&minus;<em>&tau;</em>)).</td>
<td><em>&delta;</em>&gt;0, <em>&tau;</em>&gt;0.</td>
</tr>
</tbody></table>

<p><a id=Batching_Random_Samples_via_Randomness_Extraction></a></p>

<h2>Batching Random Samples via Randomness Extraction</h2>

<p>Devroye and Gravel (2020)[^43] suggest the following randomness extractor to reduce the number of random bits needed to produce a batch of samples by a sampling algorithm.  The extractor works based on the probability that the algorithm consumes <em>X</em> random bits to produce a specific output <em>Y</em> (or <em>P</em>(<em>X</em> | <em>Y</em>) for short):</p>

<ol>
<li>Start with the interval [0, 1].</li>
<li>For each pair (<em>X</em>, <em>Y</em>) in the batch, the interval shrinks from below by <em>P</em>(<em>X</em>&minus;1 | <em>Y</em>) and from above by <em>P</em>(<em>X</em> | <em>Y</em>). (For example, if [0.2, 0.8] (range 0.6) shrinks from below by 0.1 and from above by 0.8, the new interval is [0.2+0.1*0.6, 0.2+0.8*0.6] = [0.26, 0.68].  For correctness, though, the interval is not allowed to shrink to a single point, since otherwise step 3 would run forever.)</li>
<li>Extract the bits, starting from the binary point, that the final interval&#39;s lower and upper bound have in common (or 0 bits if the upper bound is 1). (For example, if the final interval is [0.101010..., 0.101110...] in binary, the bits 1, 0, 1 are extracted, since the common bits starting from the point are 101.)</li>
</ol>

<p>After a sampling method produces an output <em>Y</em>, both <em>X</em> (the number of random bits the sampler consumed) and <em>Y</em> (the output) are added to the batch and fed to the extractor, and new bits extracted this way are added to a queue for the sampling method to use to produce future outputs. (Notice that the number of bits extracted by the algorithm above grows as the batch grows, so only the new bits extracted this way are added to the queue this way.)</p>

<p>The issue of finding <em>P</em>(<em>X</em> | <em>Y</em>) is now discussed.  Generally, if the sampling method implements a random walk on a binary tree that is driven by unbiased random bits and has leaves labeled with one outcome each (Knuth and Yao 1976)[^44], <em>P</em>(<em>X</em> | <em>Y</em>) is found as follows (and Claude Gravel clarified to me that this is the intention of the extractor algorithm): Take a weighted count of all leaves labeled <em>Y</em> up to depth <em>X</em> (where the weight for depth <em>z</em> is 1/2<sup><em>z</em></sup>), then divide it by a weighted count of all leaves labeled <em>Y</em> at all depths (for instance, if the tree has two leaves labeled <em>Y</em> at <em>z</em>=2, three at <em>z</em>=3, and three at <em>z</em>=4, and <em>X</em> is 3, then <em>P</em>(<em>X</em> | <em>Y</em>) is (2/2<sup>2</sup>+3/2<sup>3</sup>) / (2/2<sup>2</sup>+3/2<sup>3</sup>+3/2<sup>4</sup>)).  In the special case where the tree has at most 1 leaf labeled <em>Y</em> at every depth, this is implemented by finding <em>P</em>(<em>Y</em>), or the probability to output <em>Y</em>, then chopping <em>P</em>(<em>Y</em>) up to the <em>X</em><sup>th</sup> binary digit after the point and dividing by the original <em>P</em>(<em>Y</em>) (for instance, if <em>X</em> is 4 and P(<em>Y</em>) is 0.101011..., then <em>P</em>(<em>X</em> | <em>Y</em>) is 0.1010 / 0.101011...).</p>

<p>Unfortunately, <em>P</em>(<em>X</em> | <em>Y</em>) is not easy to calculate when the number of values <em>Y</em> can take on is large or even unbounded.  In this case, I can suggest the following ad hoc algorithm, which uses a randomness extractor that takes <em>bits</em> as input, such as the von Neumann, Peres, or Zhou&ndash;Bruck extractor (see &quot;<a href="https://peteroupc.github.io/randextract.html"><strong>Notes on Randomness Extraction</strong></a>&quot;).  The algorithm counts the number of bits it consumes (<em>X</em>) to produce an output, then feeds <em>X</em> to the extractor as follows.</p>

<ol>
<li>Let <em>z</em> be abs(<em>X</em>&minus;<em>lastX</em>), where <em>lastX</em> is either the last value of <em>X</em> fed to this extractor for this batch or 0 if there is no such value.</li>
<li>If <em>z</em> is greater than 0, feed the bits of <em>z</em> from most significant to least significant to a queue of extractor inputs.</li>
<li>Now, when the sampler consumes a random bit, it checks the input queue.  As long as 64 bits or more are in the input queue, the sampler dequeues 64 bits from it, runs the extractor on those bits, and adds the extracted bits to an output queue. (The number 64 can instead be any even number greater than 2.)  Then, if the output queue is not empty, the sampler dequeues a bit from that queue and uses that bit; otherwise it generates an unbiased random bit as usual.</li>
</ol>

<p><a id=Random_Variate_Generation_via_Quantiles></a></p>

<h2>Random Variate Generation via Quantiles</h2>

<p>This note is about generating random variates from a non-discrete distribution via inverse transform sampling (or via quantiles), using uniform <a href="https://peteroupc.github.io/exporand.html"><strong>partially-sampled random numbers (PSRNs)</strong></a>.  See &quot;Certain Families of Distributions&quot; for a definition of quantile functions.  A <em>uniform PSRN</em> is ultimately a number that lies in an interval; it contains a sign, an integer part, and a fractional part made up of digits sampled on demand.</p>

<p>Take the following situation:</p>

<ul>
<li>Let <em>f</em>(.) be a function applied to <em>a</em> or <em>b</em> before calculating the quantile.</li>
<li>Let <em>Q</em>(<em>z</em>) be the quantile function for the desired distribution.</li>
<li>Let <em>x</em> be a random variate in the form of a uniform PSRN, so that this PSRN will lie in the interval [<em>a</em>, <em>b</em>].  If <em>f</em>(<em>t</em>) = <em>t</em> (the identity function), the PSRN <em>x</em> must have a positive sign and an integer part of 0, so that the interval [<em>a</em>, <em>b</em>] is either the interval [0, 1] or a closed interval in [0, 1], depending on the PSRN&#39;s fractional part.  For example, if the PSRN is 2.147..., then the interval is [2.147, 2.148].</li>
<li>Let <em>&beta;</em> be the digit base of digits in <em>x</em>&#39;s fractional part (such as 2 for binary).</li>
</ul>

<p>Then the following algorithm transforms that number to a random variate for the desired distribution, which comes within the desired error tolerance of <em>&epsilon;</em> with probability 1 (see (Devroye and Gravel 2020)[^43]):</p>

<ol>
<li>Generate additional digits of <em>x</em> uniformly at random&mdash;thus shortening the interval [<em>a</em>, <em>b</em>]&mdash;until a lower bound of <em>Q</em>(<em>f</em>(<em>a</em>)) and an upper bound of <em>Q</em>(<em>f</em>(<em>b</em>)) differ by no more than 2*<em>&epsilon;</em>.  Call the two bounds <em>low</em> and <em>high</em>, respectively.</li>
<li>Return <em>low</em>+(<em>high</em>&minus;<em>low</em>)/2.</li>
</ol>

<p>In some cases, it may be possible to calculate the needed digit size in advance.</p>

<p>As one example, if <em>f</em>(<em>t</em>) = <em>t</em> (the identity function) and the quantile function is <em>Lipschitz continuous</em> on the interval [<em>a</em>, <em>b</em>][^45], then the following algorithm generates a quantile with error tolerance <em>&epsilon;</em>:</p>

<ol>
<li>Let <em>d</em> be ceil((ln(max(1,<em>L</em>)) &minus; ln(<em>&epsilon;</em>)) / ln(<em>&beta;</em>)), where <em>L</em> is an upper bound of the quantile function&#39;s maximum slope (also known as the <em>Lipschitz constant</em>). For each digit among the first <em>d</em> digits in <em>x</em>&#39;s fractional part, if that digit is unsampled, set it to a digit chosen uniformly at random.</li>
<li>The PSRN <em>x</em> now lies in the interval [<em>a</em>, <em>b</em>].  Calculate lower and upper bounds of <em>Q</em>(<em>a</em>) and <em>Q</em>(<em>b</em>), respectively, that are within <em>&epsilon;</em>/2 of the true quantiles, call the bounds <em>low</em> and <em>high</em>, respectively.</li>
<li>Return <em>low</em>+(<em>high</em>&minus;<em>low</em>)/2.</li>
</ol>

<p>This algorithm chooses a random interval of size equal to <em>&beta;</em><sup><em>d</em></sup>, and because the quantile function is Lipschitz continuous, the values at the interval&#39;s bounds are guaranteed to vary by no more than 2*<em>&epsilon;</em> (actually <em>&epsilon;</em>, but the calculation in step 2 adds an additional error of at most <em>&epsilon;</em>), which is needed to meet the tolerance <em>&epsilon;</em> (see also Devroye and Gravel 2020[^43]).</p>

<p>A similar algorithm can exist even if the quantile function <em>Q</em> is not Lipschitz continuous on the interval [<em>a</em>, <em>b</em>].</p>

<p>Specifically, if&mdash;</p>

<ul>
<li><em>f</em>(<em>t</em>) = <em>t</em> (the identity function),</li>
<li><em>Q</em> on the interval [<em>a</em>, <em>b</em>] is continuous and has a minimum and maximum, and</li>
<li><em>Q</em> on [<em>a</em>, <em>b</em>] admits a continuous and monotone increasing function <em>&omega;</em>(<em>&delta;</em>) as a <em>modulus of continuity</em>,</li>
</ul>

<p>then <em>d</em> in step 1 above can be calculated as&mdash;<br/>&nbsp;&nbsp;max(0, ceil(&minus;ln(<em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>))/ln(<em>&beta;</em>))),<br/>where <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>) is the inverse of the modulus of continuity.  (Loosely speaking, a modulus of continuity <em>&omega;</em>(<em>&delta;</em>) gives the quantile function&#39;s maximum range in a window of size <em>&delta;</em>, and the inverse modulus <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>) finds a window small enough that the quantile function differs by no more than <em>&epsilon;</em> in the window.[^46]).[^47]</p>

<p>For example&mdash;</p>

<ul>
<li>if <em>Q</em> is Lipschitz continuous with Lipschitz constant <em>L</em> on [<em>a</em>, <em>b</em>], then the function is no &quot;steeper&quot; than <em>&omega;</em>(<em>&delta;</em>) = <em>L</em>*<em>&delta;</em>, so <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>) = <em>&epsilon;</em>/<em>L</em>, and</li>
<li>if <em>Q</em> is <em>&alpha;</em>-Hölder continuous with Hölder constant <em>M</em> on that interval, then the function is no &quot;steeper&quot; than <em>&omega;</em>(<em>&delta;</em>) = <em>M</em>*<em>&delta;</em><sup><em>&alpha;</em></sup>, so <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>) = (<em>&epsilon;</em>/<em>M</em>)<sup>1/<em>&alpha;</em></sup>.</li>
</ul>

<p>The algorithms given earlier in this section have a disadvantage: the desired error tolerance has to be made known to the algorithm in advance.  To generate a quantile to any error tolerance (even if the tolerance is not known in advance), a rejection sampling approach is needed.  For this to work:</p>

<ul>
<li>The target distribution must have a probability density function (PDF), as is the case with the normal and exponential distributions.</li>
<li>That PDF, or a function proportional to it, must be known, must be bounded from above, and must be continuous &quot;almost everywhere&quot; (the set of discontinuous points is &quot;zero-volume&quot;, that is, has Lebesgue measure zero) (see also (Devroye and Gravel 2020)[^43]).</li>
</ul>

<p>Here is a sketch of how this rejection sampler might work:</p>

<ol>
<li>After using one of the algorithms given earlier in this section to sample digits of <em>x</em> as needed, let <em>a</em> and <em>b</em> be <em>x</em>&#39;s upper and lower bounds.  Calculate lower and upper bounds of the quantiles of <em>f</em>(<em>a</em>) and <em>f</em>(<em>b</em>) (the bounds are [<em>alow</em>, <em>ahigh</em>] and [<em>blow</em>, <em>bhigh</em>] respectively).</li>
<li>Given the target function&#39;s PDF or a function proportional to it, sample a uniform PSRN, <em>y</em>, in the interval [<em>alow</em>, <em>bhigh</em>] using an arbitrary-precision rejection sampler such as Oberhoff&#39;s method (described in an <a href="https://peteroupc.github.io/exporand.html#Oberhoff_s_Exact_Rejection_Sampling_Method"><strong>appendix to the PSRN article</strong></a>).</li>
<li>Accept <em>y</em> (and return it) if it clearly lies in [<em>ahigh</em>, <em>blow</em>].  Reject <em>y</em> (and go to the previous step) if it clearly lies outside [<em>alow</em>, <em>bhigh</em>].  If <em>y</em> clearly lies in [<em>alow</em>, <em>ahigh</em>] or in [<em>blow</em>, <em>bhigh</em>], generate more digits of <em>x</em>, uniformly at random, and go to the first step.</li>
<li>If <em>y</em> doesn&#39;t clearly fall in any of the cases in the previous step, generate more digits of <em>y</em>, uniformly at random, and go to the previous step.</li>
</ol>

<p><a id=ExpoExact></a></p>

<h2>ExpoExact</h2>

<p>This algorithm <code>ExpoExact</code>, samples an exponential random variate given the rate <code>rx</code>/<code>ry</code> with an error tolerance of 2<sup><code>-precision</code></sup>; for more information, see &quot;<a href="https://peteroupc.github.io/exporand.html"><strong>Partially-Sampled Random Numbers</strong></a>&quot;; see also Morina et al. (2022)[^48]; Canonne et al. (2020)[^49].  In this section:</p>

<ul>
<li><code>RNDINT(1)</code> generates an independent unbiased random bit.</li>
<li><code>ZeroOrOne(x, y)</code> returns 1 with probability <code>x</code>/<code>y</code>, and 0 otherwise.  For example, ZeroOrOne could generate a uniform random integer in the interval [0, <code>y</code>) and output either 1 if that integer is less than x, or 0 otherwise.</li>
<li>The <a href="https://peteroupc.github.io/pseudocode.html"><strong>pseudocode conventions</strong></a> apply to this section.</li>
</ul>

<hr>

<pre>METHOD ZeroOrOneExpMinus(x, y)
  if y==0 or y&lt;0 or x&lt;0: return error
  if x==0: return 1 // exp(0) = 1
  if x &gt; y
    x = rem(x, y)
    if x&gt;0 and ZeroOrOneExpMinus(x, y) == 0: return 0
    for i in 0...floor(x/y): if ZeroOrOneExpMinus(1,1) == 0: return 0
    return 1
  end
  r = 1
  oy = y
  while true
    if ZeroOrOne(x, y) == 0: return r
    r=1-r; y = y + oy
  end
END METHOD

METHOD ExpoExact(rx, ry, precision)
   ret=0
   for i in 1..precision
    // This loop adds to ret with probability 1/(exp(2^-prec)+1).
    // References: Alg. 6 of Morina et al. 2022; Canonne et al. 2020.
    denom=pow(2,i)*ry
    while true
       if RNDINT(1)==0: break
       if ZeroOrOneExpMinus(rx, denom) == 1:
         ret=ret+MakeRatio(1,pow(2,i))
    end
   end
   while ZeroOrOneExpMinus(rx,ry)==1: ret=ret+1
   return ret
END METHOD
</pre>

<blockquote>
<p><strong>Note:</strong> After <code>ExpoExact</code> is used to generate a random variate, an application can append additional binary digits (such as <code>RNDINT(1)</code>) to the end of that number while remaining accurate to the precision given in <code>precision</code> (see also Karney 2016)[^50].</p>
</blockquote>

<p><a id=A_sampler_for_distributions_with_nonincreasing_or_nondecreasing_weights></a></p>

<h2>A sampler for distributions with nonincreasing or nondecreasing weights</h2>

<p>An algorithm for sampling an integer in the interval [<em>a</em>, <em>b</em>) with probability proportional to weights listed in <em>nonincreasing</em> order (example: [10, 3, 2, 1, 1] when <em>a</em> = 0 and <em>b</em> = 5) can be implemented as follows (Chewi et al. 2021)[^51].  It has a logarithmic time complexity in terms of setup and sampling.</p>

<ul>
<li>Setup:  Let <em>w</em>[<em>i</em>] be the weight for integer <em>i</em> (with <em>i</em> starting at <em>a</em>).

<ol>
<li>(Envelope weights.) Build a list <em>q</em> as follows: The first item is <em>w</em>[<em>a</em>], then set <em>j</em> to 1, then while <em>j</em> &lt; <em>b</em>&minus;<em>a</em>, append <em>w</em>[<em>a</em> + <em>j</em>] and multiply <em>j</em> by 2.  The list <em>q</em>&#39;s items should be rational numbers that equal the true values, if possible, or overestimate them if not.</li>
<li>(Envelope chunk weights.) Build a list <em>r</em> as follows: The first item is <em>q</em>[0], then set <em>j</em> to 1 and <em>m</em> to 1, then while <em>j</em> &lt; <em>b</em>&minus;<em>a</em>, append <em>q</em>[<em>m</em>]*min((<em>b</em>&minus;<em>a</em>) &minus; <em>j</em>, <em>j</em>) and multiply <em>j</em> by 2 and add 1 to <em>m</em>.</li>
<li>(Start and end points of each chunk.) Build a list <em>D</em> as follows: The first item is the list [<em>a</em>, <em>a</em>+1], then set <em>j</em> to 1, then while <em>j</em> &lt; <em>n</em>, append the list [<em>j</em>, <em>j</em> + min((<em>b</em>&minus;<em>a</em>) &minus; <em>j</em>, <em>j</em>)] and multiply <em>j</em> by 2.</li>
</ol></li>
<li>Sampling:

<ol>
<li>Choose an integer in [0, <em>s</em>) with probability proportional to the weights in <em>r</em>, where <em>s</em> is the number of items in <em>r</em>.  Call the chosen integer <em>k</em>.</li>
<li>Set <em>x</em> to an integer chosen uniformly at random in the half-open interval [<em>D</em>[<em>k</em>][0], <em>D</em>[<em>k</em>][1]).</li>
<li>With probability <em>w</em>[<em>x</em>] / <em>q</em>[<em>k</em>], return <em>x</em>.  Otherwise, go to step 1.</li>
</ol></li>
</ul>

<p>For <em>nondecreasing</em> rather than nonincreasing weights, the algorithm is as follows instead:</p>

<ul>
<li>Setup:  Let <em>w</em>[<em>i</em>] be the weight for integer <em>i</em> (with <em>i</em> starting at <em>a</em>).

<ol>
<li>(Envelope weights.) Build a list <em>q</em> as follows: The first item is <em>w</em>[<em>b</em>&minus;1], then set <em>j</em> to 1, then while <em>j</em> &lt; (<em>b</em>&minus;<em>a</em>), append <em>w</em>[<em>b</em>&minus;1&minus;<em>j</em>] and multiply <em>j</em> by 2.  The list <em>q</em>&#39;s items should be rational numbers that equal the true values, if possible, or overestimate them if not.</li>
<li>(Envelope chunk weights.) Build a list <em>r</em> as given in step 2 of the previous algorithm&#39;s setup.</li>
<li>(Start and end points of each chunk.) Build a list <em>D</em> as follows: The first item is the list [<em>b</em>&minus;1, <em>b</em>], then set <em>j</em> to 1, then while <em>j</em> &lt; (<em>b</em>&minus;<em>a</em>), append the list [(<em>b</em>&minus;<em>j</em>) &minus; min((<em>b</em>&minus;<em>a</em>) &minus; <em>j</em>, <em>j</em>), <em>b</em>&minus;<em>j</em>] and multiply <em>j</em> by 2.</li>
</ol></li>
<li>The sampling is the same as for the previous algorithm.</li>
</ul>

<blockquote>
<p><strong>Note:</strong> The weights can be base-<em>&beta;</em> logarithms, especially since logarithms preserve order, but in this case the algorithm requires changes.  In the setup step 2, replace &quot;<em>q</em>[<em>m</em>]*min((<em>b</em>&minus;<em>a</em>)&quot; with &quot;<em>q</em>[<em>m</em>]+ln(min((<em>b</em>&minus;<em>a</em>))/ln(<em>&beta;</em>)&quot; (which is generally inexact unless <em>&beta;</em> is 2); in sampling step 1, use an algorithm that takes base-<em>&beta;</em> logarithms as weights; and replace sampling step 3 with &quot;Generate an exponential random variate with rate ln(<em>&beta;</em>).  If that variate is greater than <em>q</em>[<em>k</em>] minus <em>w</em>[<em>x</em>], return <em>x</em>.  Otherwise, go to step 1.&quot;  These modifications can introduce numerical errors unless care is taken, such as by using partially-sampled random numbers (PSRNs).</p>
</blockquote>

<p><a id=A_sampler_for_unimodal_distributions_of_weights></a></p>

<h2>A sampler for unimodal distributions of weights</h2>

<p>The following is an algorithm for sampling an integer in the interval [<em>a</em>, <em>b</em>) with probability proportional to a <em>unimodal distribution</em> of weights (that is, nondecreasing on the left and nonincreasing on the right) (Chewi et al. 2021)[^51].  It assumes the mode (the point with the highest weight) is known.  An example is [1, 3, 9, 4, 4] when <em>a</em> = 0 and <em>b</em> = 5, and the <em>mode</em> is 2, which corresponds to the weight 9.  It has a logarithmic time complexity in terms of setup and sampling.</p>

<ul>
<li>Setup:

<ol>
<li>Find the point with the highest weight, such as via binary search.  Call this point <em>mode</em>.</li>
<li>Run the setup for <em>nondecreasing</em> weights on the interval [<em>a</em>, <em>mode</em>), then run the setup for <em>nonincreasing</em> weights on the interval [<em>mode</em>, <em>b</em>).  Both setups are described in the previous section.  Then, concatenate the two <em>q</em> lists into one, the two <em>r</em> lists into one, and the two <em>D</em> lists into one.</li>
</ol></li>
<li>The sampling is the same as for the algorithms in the previous section.</li>
</ul>

<p><a id=Log_Uniform_Distribution></a></p>

<h2>Log-Uniform Distribution</h2>

<p>Samples from the so-called &quot;log uniform distribution&quot; as used by the Abseil programming library.  This algorithm takes a maximum <em>mx</em> and a logarithmic base <em>b</em>, and chooses an integer in [0, <em>mx</em>] such that two values are equally likely to be chosen if their base-<em>b</em> logarithms are equal in their integer parts (which roughly means that lower numbers are exponentially more likely to occur).  Although this algorithm works, in principle, for any <em>b</em> &gt; 0, Abseil supports only integer bases <em>b</em>.</p>

<ol>
<li>Let <em>L</em> be ceil(ln(<em>mx</em>+1)/ln(<em>b</em>)). Choose a uniform random integer in the closed interval [0, <em>l</em>], call it <em>u</em>.</li>
<li>If <em>u</em> is 0, return 0.</li>
<li>Set <em>st</em> to min(<em>mx</em>, ceil(<em>b</em><sup><em>u</em>&minus;1</sup>)).</li>
<li>Set <em>en</em> to min(<em>mx</em>, ceil(<em>b</em><sup><em>u</em></sup>) &minus; 1).</li>
<li>Choose a uniform random integer in the closed interval [<em>st</em>, <em>en</em>], and return it.</li>
</ol>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<p>[^1]: K. Bringmann, F. Kuhn, et al., “Internal DLA: Efficient Simulation of a Physical Growth Model.” In: <em>Proc. 41st International Colloquium on Automata, Languages, and Programming (ICALP&#39;14)</em>, 2014.</p>

<p>[^2]: choose(<em>n</em>, <em>k</em>) = (1*2*3*...*<em>n</em>)/((1*...*<em>k</em>)*(1*...*(<em>n</em>&minus;<em>k</em>))) =  <em>n</em>!/(<em>k</em>! * (<em>n</em> &minus; <em>k</em>)!) is a <em>binomial coefficient</em>, or the number of ways to choose <em>k</em> out of <em>n</em> labeled items.  It can be calculated, for example, by calculating <em>i</em>/(<em>n</em>&minus;<em>i</em>+1) for each integer <em>i</em> in the interval [<em>n</em>&minus;<em>k</em>+1, <em>n</em>], then multiplying the results (Yannis Manolopoulos. 2002. &quot;<a href="https://doi.org/10.1145/820127.820168"><strong>Binomial coefficient computation: recursion or iteration?</strong></a>&quot;, SIGCSE Bull. 34, 4 (December 2002), 65–67).  Note that for every <em>m</em>&gt;0, choose(<em>m</em>, 0) = choose(<em>m</em>, <em>m</em>) = 1 and choose(<em>m</em>, 1) = choose(<em>m</em>, <em>m</em>&minus;1) = <em>m</em>; also, in this document, choose(<em>n</em>, <em>k</em>) is 0 when <em>k</em> is less than 0 or greater than <em>n</em>.</p>

<p>[^3]: Devroye, L., <a href="http://luc.devroye.org/rnbookindex.html"><strong><em>Non-Uniform Random Variate Generation</em></strong></a>, 1986.</p>

<p>[^4]: R. Schumacher, &quot;<a href="https://arxiv.org/abs/1602.00336v1"><strong>Rapidly Convergent Summation Formulas involving Stirling Series</strong></a>&quot;, arXiv:1602.00336v1 [math.NT], 2016.</p>

<p>[^5]: Farach-Colton, M. and Tsai, M.T., 2015. Exact sublinear binomial sampling. <em>Algorithmica</em> 73(4), pp. 637-651.</p>

<p>[^6]: Bringmann, K., and Friedrich, T., 2013, July. Exact and efficient generation of geometric random variates and random graphs, in <em>International Colloquium on Automata, Languages, and Programming</em> (pp. 267-278).</p>

<p>[^7]: Ahmad, Z. et al. &quot;Recent Developments in Distribution Theory: A Brief Survey and Some New Generalized Classes of distributions.&quot; Pakistan Journal of Statistics and Operation Research 15 (2019): 87-110.</p>

<p>[^8]: Jones, M. C. &quot;On families of distributions with shape parameters.&quot; International Statistical Review 83, no. 2 (2015): 175-192.</p>

<p>[^9]: Eugene, N., Lee, C., Famoye, F., &quot;Beta-normal distribution and its applications&quot;, <em>Commun. Stat. Theory Methods</em> 31, 2002.</p>

<p>[^10]: Barreto-Souza, Wagner and Alexandre B. Simas. &quot;The exp-G family of probability distributions.&quot; <em>Brazilian Journal of Probability and Statistics</em> 27, 2013.  Also in arXiv:1003.1727v1 [stat.ME], 2010.</p>

<p>[^11]: Mahdavi, Abbas, and Debasis Kundu. &quot;A new method for generating distributions with an application to exponential distribution.&quot; <em>Communications in Statistics -- Theory and Methods</em> 46, no. 13 (2017): 6543-6557.</p>

<p>[^12]: M. C. Jones. Letter to the Editor concerning “A new method for generating distributions with an application to exponential distribution” and “Alpha power Weibull distribution: Properties and applications”, <em>Communications in Statistics - Theory and Methods</em> 47 (2018).</p>

<p>[^13]: Mudholkar, G. S., Srivastava, D. K., &quot;Exponentiated Weibull family for analyzing bathtub failure-rate data&quot;, <em>IEEE Transactions on Reliability</em> 42(2), 299-302, 1993.</p>

<p>[^14]: Shaw, W.T., Buckley, I.R.C., &quot;The alchemy of probability distributions: Beyond Gram-Charlier expansions, and a skew-kurtotic-normal distribution from a rank transmutation map&quot;, 2007.</p>

<p>[^15]: Granzotto, D.C.T., Louzada, F., et al., &quot;Cubic rank transmuted distributions: inferential issues and applications&quot;, <em>Journal of Statistical Computation and Simulation</em>, 2017.</p>

<p>[^16]: Alzaatreh, A., Famoye, F., Lee, C., &quot;A new method for generating families of continuous distributions&quot;, <em>Metron</em> 71:63–79 (2013).</p>

<p>[^17]: Aljarrah, M.A., Lee, C. and Famoye, F., &quot;On generating T-X family of distributions using quantile functions&quot;, Journal of Statistical Distributions and Applications,1(2), 2014.</p>

<p>[^18]: Gleaton, J.U., Lynch, J. D., &quot;Properties of generalized log-logistic families of lifetime distributions&quot;, <em>Journal of Probability and Statistical Science</em> 4(1), 2006.</p>

<p>[^19]: Hosseini, B., Afshari, M., &quot;The Generalized Odd Gamma-G Family of Distributions:  Properties and Application&quot;, <em>Austrian Journal of Statistics</em> vol. 47, Feb. 2018.</p>

<p>[^20]: N.H. Al Noor and N.K. Assi, &quot;Rayleigh-Rayleigh Distribution: Properties and Applications&quot;, <em>Journal of Physics: Conference Series</em> 1591, 012038 (2020).  The underlying Rayleigh distribution uses a parameter <em>&theta;</em> (or <em>&lambda;</em>), which is different from <em>Mathematica</em>&#39;s parameterization with <em>&sigma;</em> = sqrt(1/<em>&theta;</em><sup>2</sup>) = sqrt(1/<em>&lambda;</em><sup>2</sup>).  The first Rayleigh distribution uses <em>&theta;</em> and the second, <em>&lambda;</em>.</p>

<p>[^21]: Boshi, M.A.A., et al., &quot;Generalized Gamma – Generalized Gompertz Distribution&quot;, <em>Journal of Physics: Conference Series</em> 1591, 012043 (2020).</p>

<p>[^22]: Tahir, M.H., Cordeiro, G.M., &quot;Compounding of distributions: a survey and new generalized classes&quot;, <em>Journal of Statistical Distributions and Applications</em> 3(13), 2016.</p>

<p>[^23]: Pérez-Casany, M., Valero, J., and Ginebra, J. (2016). Random-Stopped Extreme distributions. International Conference on Statistical Distributions and Applications.</p>

<p>[^24]: Johnson, N. L., Kemp, A. W., and Kotz, S. (2005). Univariate discrete distributions.</p>

<p>[^25]: Duarte-López, A., Pérez-Casany, M. and Valero, J., 2021. Randomly stopped extreme Zipf extensions. Extremes, pp.1-34.</p>

<p>[^26]: This is simplified from the paper because <em>Y</em> can take on only values greater than 0 so that the probability of getting 0 is 0.</p>

<p>[^27]: Akdoğan, Y., Kus, C., et al., &quot;Geometric-Zero Truncated Poisson Distribution: Properties and Applications&quot;, <em>Gazi University Journal of Science</em> 32(4), 2019.</p>

<p>[^28]: Kuş, C., &quot;A new lifetime distribution&quot;, <em>Computational Statistics &amp; Data Analysis</em> 51 (2007).</p>

<p>[^29]: Cancho, Vicente G., Franscisco Louzada-Neto, and Gladys DC Barriga. &quot;The Poisson-exponential lifetime distribution.&quot; Computational Statistics &amp; Data Analysis 55, no. 1 (2011): 677-686.</p>

<p>[^30]: Jodrá, P., &quot;A note on the right truncated Weibull distribution and the minimum of power function distributions&quot;, 2020.</p>

<p>[^31]: Keller, A.Z., Kamath A.R., &quot;Reliability analysis of CNC machine tools&quot;, <em>Reliability Engineering</em> 3 (1982).</p>

<p>[^32]: Rao, C.R., &quot;Weighted distributions arising out of methods of ascertainment&quot;, 1985.</p>

<p>[^33]: Ospina, R., Ferrari, S.L.P., &quot;Inflated Beta Distributions&quot;, 2010.</p>

<p>[^34]: Chakraborty, S., Bhattacharjee, S., &quot;<a href="https://arxiv.org/abs/2103.08916"><strong>Modeling proportion of success in high school leaving examination- A comparative study of Inflated Unit Lindley and Inflated Beta distribution</strong></a>&quot;, arXiv:2103.08916 [stat.ME], 2021.</p>

<p>[^35]: Mullahy, J., &quot;Specification and testing of some modified count data models&quot;, 1986.</p>

<p>[^36]: Grassia, A., &quot;On a family of distributions with argument between 0 and 1 obtained by transformation of the gamma and derived compound distributions&quot;, <em>Australian Journal of Statistics</em>, 1977.</p>

<p>[^37]: Elgohari, Hanaa, and Haitham Yousof. &quot;New Extension of Weibull Distribution: Copula, Mathematical Properties and Data Modeling.&quot; Stat., Optim. Inf. Comput., Vol.8, December 2020.</p>

<p>[^38]: Marshall, A.W. and Olkin, I., 1997. A new method for adding a parameter to a family of distributions with application to the exponential and Weibull families. Biometrika, 84(3), pp.641-652.</p>

<p>[^39]: Rady,  E.H.A.,  Hassanein,  W.A.,  Elhaddad,  T.A., &quot;The power Lomax distribution with an application to bladder cancer data&quot;, (2016).</p>

<p>[^40]: Castellares, F., Lemonte, A.J., Moreno, G., &quot;On the two-parameter Bell-Touchard discrete distribution&quot;, <em>Communications in Statistics
    - Theory and Methods</em> 4, (2020).</p>

<p>[^41]: The similar Bell&ndash;Touchard process is the sum of the first <em>N</em> variates from an infinite sequence of zero-truncated Poisson(<em>a</em>) random variates, where <em>N</em> is the number of events of a Poisson process with rate <em>b</em>*exp(<em>a</em>)&minus;<em>b</em> (Freud, T., Rodriguez, P.M., &quot;<a href="https://arxiv.org/abs/2203.16737v2"><strong>The Bell-Touchard counting process</strong></a>&quot;, arXiv:2203.16737v2 [math.PR], 2022).</p>

<p>[^42]: Batsidis, A., Lemonte, A.J., &quot;On Goodness-of-Fit Tests for the Neyman Type A Distribution&quot;, REVSTAT-Statistical Journal (accepted Nov. 2021).</p>

<p>[^43]: Devroye, L., Gravel, C., &quot;<a href="https://arxiv.org/abs/1502.02539v6"><strong>Random variate generation using only finitely many unbiased, independently and identically distributed random bits</strong></a>&quot;, arXiv:1502.02539v6  [cs.IT], 2020.</p>

<p>[^44]: Knuth, Donald E. and Andrew Chi-Chih Yao. &quot;The complexity of nonuniform random number generation&quot;, in <em>Algorithms and Complexity: New Directions and Recent Results</em>, 1976.</p>

<p>[^45]: A Lipschitz continuous function, with constant <em>L</em>, is a continuous function such that <em>f</em>(<em>x</em>) and <em>f</em>(<em>y</em>) are no more than <em>L</em>*<em>&epsilon;</em> apart whenever <em>x</em> and <em>y</em> are points in the domain that are no more than <em>&epsilon;</em> apart.  Roughly speaking, the function has a defined slope at all points or &quot;almost everywhere&quot;, and that slope is bounded wherever it&#39;s defined.</p>

<p>[^46]: Ker-I Ko makes heavy use of the inverse modulus of continuity in his complexity theory, for example, &quot;Computational complexity of roots of real functions.&quot; In <em>30th Annual Symposium on Foundations of Computer Science</em>, pp. 204-209. IEEE Computer Society, 1989.</p>

<p>[^47]: Here is a sketch of the proof: Because the quantile function <em>Q</em>(<em>x</em>) is continuous on a closed interval, it&#39;s uniformly continuous there.  For this reason, there is a positive function <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>) such that <em>Q</em>(<em>x</em>) is less than <em>&epsilon;</em>-away from <em>Q</em>(<em>y</em>) whenever <em>x</em> is less than <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>)-away from <em>y</em>, for every <em>&epsilon;</em>&gt;0 and for any <em>x</em> and <em>y</em> in that interval.  The inverse modulus of continuity is one such function, which is formed by inverting a modulus of continuity admitted by <em>Q</em>, as long as that modulus is continuous and monotone increasing on that interval to make that modulus invertible.  Finally, max(0, ceil(&minus;ln(<em>z</em>)/ln(<em>&beta;</em>))) is an upper bound on the number of base-<em>&beta;</em> fractional digits needed to store 1/<em>z</em> with an error of at most <em>&epsilon;</em>.</p>

<p>[^48]: Giulio Morina. Krzysztof Łatuszyński. Piotr Nayar. Alex Wendland. &quot;From the Bernoulli factory to a dice enterprise via perfect sampling of Markov chains.&quot; Ann. Appl. Probab. 32 (1) 327 - 359, February 2022. <a href="https://doi.org/10.1214/21-AAP1679"><strong>https://doi.org/10.1214/21-AAP1679</strong></a></p>

<p>[^49]: Canonne, C., Kamath, G., Steinke, T., &quot;<a href="https://arxiv.org/abs/2004.00010"><strong>The Discrete Gaussian for Differential Privacy</strong></a>&quot;, arXiv:2004.00010 [cs.DS], 2020.</p>

<p>[^50]: Karney, C.F.F., 2016. Sampling exactly from the normal distribution. ACM Transactions on Mathematical Software (TOMS), 42(1), pp.1-14. Also: &quot;<a href="https://arxiv.org/abs/1303.6257v2"><strong>Sampling exactly from the normal distribution</strong></a>&quot;, arXiv:1303.6257v2  [physics.comp-ph], 2014.</p>

<p>[^51]: Chewi, S., Gerber, P., et al., &quot;<a href="https://arxiv.org/abs/2105.14166"><strong>Rejection sampling from shape-constrained distributions in sublinear time</strong></a>&quot;, arXiv:2105.14166, 2021</p>

<p>[^52]: Al-Khazaleh, A.M.H., Alzoubi, L., &quot;<a href="https://doi.org/10.28919/jmcs/6082"><strong>New compound probability distribution using biweight kernel function and exponential distribution</strong></a>&quot;, J. Math. Comput. Sci. 11 (2021), No. 5, 5878-5896.  Note that the paper&#39;s two graphs are erroneous.</p>

<p><a id=License></a></p>

<h2>License</h2>

<p>Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under <a href="https://creativecommons.org/publicdomain/zero/1.0/"><strong>Creative Commons Zero</strong></a>.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>

<div class="noprint">
<p>
<a href="//twitter.com/intent/tweet">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
if("share" in navigator){
 document.getElementById("sharer").href="javascript:void(null)";
 document.getElementById("sharer").innerHTML="Share This Page";
 navigator.share({title:document.title,url:document.location.href}).then(
   function(){});
} else {
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
}
</script>
</body></html>
