<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>Miscellaneous Observations on Randomization</title><meta name="citation_title" content="Miscellaneous Observations on Randomization"><meta name="citation_pdf_url" content="https://peteroupc.github.io/randmisc.pdf"><meta name="citation_url" content="https://peteroupc.github.io/randmisc.html"><meta name="citation_date" content="2021/05/18"><meta name="citation_online_date" content="2021/05/18"><meta name="og:title" content="Miscellaneous Observations on Randomization"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/randmisc.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="Miscellaneous Observations on Randomization"><meta name="author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css"></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>Miscellaneous Observations on Randomization</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><a id=Contents></a></p>

<h2>Contents</h2>

<ul>
<li><a href="#Contents"><strong>Contents</strong></a></li>
<li><a href="#On_a_Binomial_Sampler"><strong>On a Binomial Sampler</strong></a></li>
<li><a href="#On_a_Geometric_Sampler"><strong>On a Geometric Sampler</strong></a></li>
<li><a href="#Sampling_Unbounded_Monotone_Density_Functions"><strong>Sampling Unbounded Monotone Density Functions</strong></a></li>
<li><a href="#Certain_Families_of_Distributions"><strong>Certain Families of Distributions</strong></a></li>
<li><a href="#Certain_Distributions"><strong>Certain Distributions</strong></a></li>
<li><a href="#Batching_Random_Samples_via_Randomness_Extraction"><strong>Batching Random Samples via Randomness Extraction</strong></a></li>
<li><a href="#Randomization_via_Quantiles"><strong>Randomization via Quantiles</strong></a></li>
<li><a href="#ExpoExact"><strong>ExpoExact</strong></a></li>
<li><a href="#Notes"><strong>Notes</strong></a></li>
<li><a href="#License"><strong>License</strong></a></li>
</ul>

<p><a id=On_a_Binomial_Sampler></a></p>

<h2>On a Binomial Sampler</h2>

<p>Take the following sampler of a binomial(<em>n</em>, 1/2) distribution (where <em>n</em> is even), which is equivalent to the one that appeared in (Bringmann et al. 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>, and adapted to be more programmer-friendly.</p>

<ol>
<li>If <em>n</em> is less than 4, generate <em>n</em> unbiased random bits (zeros or ones) and return their sum.  Otherwise, if <em>n</em> is odd, set <em>ret</em> to the result of this algorithm with <em>n</em> = <em>n</em> &minus; 1, then add an unbiased random bit&#39;s value to <em>ret</em>, then return <em>ret</em>.</li>
<li>Set <em>m</em> to floor(sqrt(<em>n</em>)) + 1.</li>
<li>(First, sample from an envelope of the binomial curve.) Generate unbiased random bits until a zero is generated this way.  Set <em>k</em> to the number of ones generated this way.</li>
<li>Set <em>s</em> to an integer in [0, <em>m</em>) chosen uniformly at random, then set <em>i</em> to <em>k</em>*<em>m</em> + <em>s</em>.</li>
<li>Generate an unbiased random bit.  If that bit is 0, set <em>ret</em> to (<em>n</em>/2)+<em>i</em>.  Otherwise, set <em>ret</em> to (<em>n</em>/2)&minus;<em>i</em>&minus;1.</li>
<li>(Second, accept or reject <em>ret</em>.) If <em>ret</em> &lt; 0 or <em>ret</em> &gt; <em>n</em>, go to step 3.</li>
<li>With probability choose(<em>n</em>, <em>ret</em>)*<em>m</em>*2<sup>(<em>k</em>&minus;<em>n</em>)+2</sup>, return <em>ret</em>.  Otherwise, go to step 3. (Here, choose(<em>n</em>, <em>k</em>) is a <em>binomial coefficient</em>, or the number of ways to choose <em>k</em> out of <em>n</em> labeled items.<sup><a href="#Note2"><strong>(2)</strong></a></sup>)</li>
</ol>

<p>This algorithm has an acceptance rate of 1/16 regardless of the value of <em>n</em>.  However, step 7 will generally require a growing amount of storage and time to exactly calculate the given probability as <em>n</em> gets larger, notably due to the inherent factorial in the binomial coefficient.  The Bringmann paper suggests approximating this factorial via Spouge&#39;s approximation; however, it seems hard to do so without using floating-point arithmetic, which the paper ultimately resorts to. Alternatively, the logarithm of that probability can be calculated that is much more economical in terms of storage than the full exact probability.  Then, an exponential random variate can be generated, negated, and compared with that logarithm to determine whether the step succeeds.</p>

<p>More specifically, step 7 can be changed as follows:</p>

<ul>
<li>(7.) Let <em>p</em> be loggamma(<em>n</em>+1)&minus;loggamma(<em>ret</em>+1)&minus;loggamma((<em>n</em>&minus;<em>ret</em>)+1)+ln(<em>m</em>)+ln(2)*((<em>k</em>&minus;<em>n</em>)+2) (where loggamma(<em>x</em>) is the logarithm of the gamma function).</li>
<li>(7a.) Generate an exponential random variate with rate 1 (which is the negative natural logarithm of a uniform(0,1) random variate).  Set <em>&eta;</em> to 0 minus that number.</li>
<li>(7b.) If <em>h</em> is greater than <em>p</em>, go to step 3.  Otherwise, return <em>ret</em>. (This step can be replaced by calculating lower and upper bounds that converge to <em>p</em>.  In that case, go to step 3 if <em>h</em> is greater than the upper bound, or return <em>ret</em> if <em>h</em> is less than the lower bound, or compute better bounds and repeat this step otherwise.  See also chapter 4 of (Devroye 1986)<sup><a href="#Note3"><strong>(3)</strong></a></sup>.)</li>
</ul>

<p>My implementation of loggamma and the natural logarithm (<a href="https://peteroupc.github.io/interval.py"><strong>interval.py</strong></a>) relies on rational interval arithmetic (Daumas et al. 2007)<sup><a href="#Note4"><strong>(4)</strong></a></sup> and a fast converging version of Stirling&#39;s formula for the factorial&#39;s natural logarithm (Schumacher 2016)<sup><a href="#Note5"><strong>(5)</strong></a></sup>.</p>

<p>Also, according to the Bringmann paper, <em>m</em> can be set such that <em>m</em> is in the interval [sqrt(<em>n</em>), sqrt(<em>n</em>)+3], so I implement step 1 by starting with <em>u</em> = 2<sup>floor((1+<em>&beta;</em>(<em>n</em>))/2)</sup>, then calculating <em>v</em> = floor((<em>u</em>+floor(<em>n</em>/<em>u</em>))/2), <em>w</em> = <em>u</em>, <em>u</em> = <em>v</em>  until <em>v</em> &gt;= <em>w</em>, then setting <em>m</em> to <em>w</em> + 1.  Here, <em>&beta;</em>(<em>n</em>) = ceil(ln(<em>n</em>+1)/ln(2)), or alternatively the minimum number of bits needed to store <em>n</em> (with <em>&beta;</em>(0) = 0).</p>

<blockquote>
<p><strong>Notes:</strong></p>

<ul>
<li>A binomial(<em>n</em>, 1/2) random variate, where <em>n</em> is odd, can be generated by adding an unbiased random bit&#39;s value (either zero or one with equal probability) to a binomial(<em>n</em>&minus;1, 1/2) random variate.</li>
<li>As pointed out by Farach-Colton and Tsai (2015)<sup><a href="#Note6"><strong>(6)</strong></a></sup>, a binomial(<em>n</em>, <em>p</em>) random variate, where <em>p</em> is in the interval (0, 1), can be generated using binomial(<em>n</em>, 1/2) numbers using a procedure equivalent to the following:

<ol>
<li>Set <em>k</em> to 0 and <em>ret</em> to 0.</li>
<li>If the binary digit at position <em>k</em> after the point in <em>p</em>&#39;s binary expansion (that is, 0.bbbb... where each b is a zero or one) is 1, add a binomial(<em>n</em>, 1/2) random variate to <em>ret</em> and subtract the same variate from <em>n</em>; otherwise, set <em>n</em> to a binomial(<em>n</em>, 1/2) random variate.</li>
<li>If <em>n</em> is greater than 0, add 1 to <em>k</em> and go to step 2; otherwise, return <em>ret</em>. (Positions start at 0 where 0 is the most significant digit after the point, 1 is the next, etc.)</li>
</ol></li>
</ul>
</blockquote>

<p><a id=On_a_Geometric_Sampler></a></p>

<h2>On a Geometric Sampler</h2>

<p>The following algorithm is equivalent to the geometric(<em>px</em>/<em>py</em>) sampler that appeared in (Bringmann and Friedrich 2013)<sup><a href="#Note7"><strong>(7)</strong></a></sup>, but adapted to be more programmer-friendly.  As used in that paper, a geometric(<em>p</em>) random variate expresses the number of failing trials before the first success, where each trial is independent and has success probability <em>p</em>. (Note that the terminology &quot;geometric random variate&quot; has conflicting meanings in academic works.  Note also that the algorithm uses the rational number <em>px</em>/<em>py</em>, not an arbitrary real number <em>p</em>; some of the notes in this section indicate how to adapt the algorithm to an arbitrary <em>p</em>.)</p>

<ol>
<li>Set <em>pn</em> to <em>px</em>, <em>k</em> to 0, and <em>d</em> to 0.</li>
<li>While <em>pn</em>*2 &lt;= <em>py</em>, add 1 to <em>k</em> and multiply <em>pn</em> by 2.  (Equivalent to finding the largest <em>k</em> &gt;= 0 such that <em>p</em>*2<sup><em>k</em></sup> &lt;= 1.  For the case when <em>p</em> need not be rational, enough of its binary expansion can be calculated to carry out this step accurately, but in this case any <em>k</em> such that <em>p</em> is greater than 1/(2<sup><em>k</em>+2</sup>) and less than or equal to 1/(2<sup><em>k</em></sup>) will suffice, as the Bringmann paper points out.)</li>
<li>With probability (1&minus;<em>px</em>/<em>py</em>)<sup>2<sup><em>k</em></sup></sup>, add 1 to <em>d</em> and repeat this step. (To simulate this probability, the first sub-algorithm below can be used.)</li>
<li>Generate a uniform random integer in [0, 2<sup><em>k</em></sup>), call it <em>m</em>, then with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>m</em></sup>, return <em>d</em>*2<sup><em>k</em></sup>+<em>m</em>. Otherwise, repeat this step. (The Bringmann paper, though, suggests to simulate this probability by sampling only as many bits of <em>m</em> as needed to do so, rather than just generating <em>m</em> in one go, then using the first sub-algorithm on <em>m</em>.  However, the implementation, given as the second sub-algorithm below, is much more complicated and is not crucial for correctness.)</li>
</ol>

<p>The first sub-algorithm returns 1 with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>n</em></sup>, assuming that <em>n</em>*<em>px</em>/<em>py</em> &lt;= 1.  It implements the approach from the Bringmann paper by rewriting the probability using the binomial theorem. (For the case when <em>p</em> need not be rational, the probability (1&minus;<em>p</em>)<sup><em>n</em></sup> can be simulated using <em>Bernoulli factory</em> algorithms, or by calculating its digit expansion or series expansion and using the appropriate algorithm for <a href="https://peteroupc.github.io/bernoulli.html#Algorithms_for_Irrational_Constants"><strong>simulating irrational constants</strong></a>. Run that algorithm <em>n</em> times or until it outputs 1, whichever comes first.  This sub-algorithm returns 1 if all the runs return 0, or 1 otherwise.)</p>

<ol>
<li>Set <em>pnum</em>, <em>pden</em>, and <em>j</em>  to 1, then set <em>r</em> to 0, then set <em>qnum</em> to <em>px</em>, and <em>qden</em> to <em>py</em>, then set <em>i</em> to 2.</li>
<li>If <em>j</em> is greater than <em>n</em>, go to step 5.</li>
<li>If <em>j</em> is even, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> + <em>pden</em>*<em>qnum</em>*choose(<em>n</em>,<em>j</em>). Otherwise, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> &minus; <em>pden</em>*<em>qnum</em>*choose(<em>n</em>,<em>j</em>).</li>
<li>Multiply <em>pden</em> by <em>qden</em>, then multiply <em>qnum</em> by <em>px</em>, then multiply <em>qden</em> by <em>py</em>, then add 1 to <em>j</em>.</li>
<li>If <em>j</em> is less than or equal to 2 and less than or equal to <em>n</em>, go to step 2.</li>
<li>Multiply <em>r</em> by 2, then add an unbiased random bit&#39;s value (either 0 or 1 with equal probability) to <em>r</em>.</li>
<li>If <em>r</em> &lt;= floor((<em>pnum</em>*<em>i</em>)/<em>pden</em>) &minus; 2, return 1. If <em>r</em> &gt;= floor((<em>pnum</em>*<em>i</em>)/<em>pden</em>) + 1, return 0.  If neither is the case, multiply <em>i</em> by 2 and go to step 2.</li>
</ol>

<p>The second sub-algorithm returns an integer <em>m</em> in [0, 2<sup><em>k</em></sup>) with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>m</em></sup>, or &minus;1 with the opposite probability.  It assumes that 2<sup><em>k</em></sup>*<em>px</em>/<em>py</em> &lt;= 1.</p>

<ol>
<li>Set <em>r</em> and <em>m</em> to 0.</li>
<li>Set <em>b</em> to 0, then while <em>b</em> is less than <em>k</em>:

<ol>
<li>(Sum <em>b</em>+2 summands of the binomial equivalent of the desired probability.  First, append an additional bit to <em>m</em>, from most to least significant.) Generate an unbiased random bit (either 0 or 1 with equal probability).  If that bit is 1, add 2<sup><em>k</em>&minus;<em>b</em></sup> to <em>m</em>.</li>
<li>(Now build up the binomial probability.) Set <em>pnum</em>, <em>pden</em>, and <em>j</em>  to 1, then set <em>qnum</em> to <em>px</em>, and <em>qden</em> to <em>py</em>.</li>
<li>If <em>j</em> is greater than <em>m</em> or greater than <em>b</em> + 2, go to the sixth substep.</li>
<li>If <em>j</em> is even, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> + <em>pden</em>*<em>qnum</em>*choose(<em>m</em>,<em>j</em>). Otherwise, set <em>pnum</em> to <em>pnum</em>*<em>qden</em> &minus; <em>pden</em>*<em>qnum</em>*choose(<em>m</em>,<em>j</em>).</li>
<li>Multiply <em>pden</em> by <em>qden</em>, then multiply <em>qnum</em> by <em>px</em>, then multiply <em>qden</em> by <em>py</em>, then add 1 to <em>j</em>, then go to the third substep.</li>
<li>(Now check the probability.) Multiply <em>r</em> by 2, then add an unbiased random bit&#39;s value (either 0 or 1 with equal probability) to <em>r</em>.</li>
<li>If <em>r</em> &lt;= floor((<em>pnum</em>*2<sup><em>b</em></sup>)/<em>pden</em>) &minus; 2, add a uniform random integer in [0, 2<sup><em>k</em>*<em>b</em></sup>) to <em>m</em> and return <em>m</em> (and, if requested, the number <em>k</em>&minus;<em>b</em>&minus;1). If <em>r</em> &gt;= floor((<em>pnum</em>*2<sup><em>b</em></sup>)/<em>pden</em>) + 1, return &minus;1 (and, if requested, an arbitrary value).  If neither is the case, add 1 to <em>b</em>.</li>
</ol></li>
<li>Add an unbiased random bit to <em>m</em>. (At this point, <em>m</em> is fully sampled.)</li>
<li>Run the first sub-algorithm with <em>n</em> = <em>m</em>, except in step 1 of that sub-algorithm, set <em>r</em> to the value of <em>r</em> built up by this algorithm, rather than 0, and set <em>i</em> to 2<sup><em>k</em></sup>, rather than 2.  If that sub-algorithm returns 1, return <em>m</em> (and, if requested, the number &minus;1).  Otherwise, return &minus;1 (and, if requested, an arbitrary value).</li>
</ol>

<p>As used in the Bringmann paper, a bounded geometric(<em>p</em>, <em>n</em>) random variate is a geometric(<em>p</em>) random variate or <em>n</em> (an integer greater than 0), whichever is less.  The following algorithm is equivalent to the algorithm given in that paper, but adapted to be more programmer-friendly.</p>

<ol>
<li>Set <em>pn</em> to <em>px</em>, <em>k</em> to 0, <em>d</em> to 0, and <em>m2</em> to the smallest power of 2 that is greater than <em>n</em> (or equivalently, 2<sup><em>bits</em></sup> where <em>bits</em> is the minimum number of bits needed to store <em>n</em>).</li>
<li>While <em>pn</em>*2 &lt;= <em>py</em>, add 1 to <em>k</em> and multiply <em>pn</em> by 2.</li>
<li>With probability (1&minus;<em>px</em>/<em>py</em>)<sup>2<sup><em>k</em></sup></sup>, add 1 to <em>d</em> and then either return <em>n</em> if <em>d</em>*2<sup><em>k</em></sup> is greater than or equal to <em>m2</em>, or repeat this step if less. (To simulate this probability, the first sub-algorithm above can be used.)</li>
<li>Generate a uniform random integer in [0, 2<sup><em>k</em></sup>), call it <em>m</em>, then with probability (1&minus;<em>px</em>/<em>py</em>)<sup><em>m</em></sup>, return min(<em>n</em>, <em>d</em>*2<sup><em>k</em></sup>+<em>m</em>). In the Bringmann paper, this step is implemented in a manner equivalent to the following (this alternative implementation, though, is not crucial for correctness):

<ol>
<li>Run the second sub-algorithm above, except return two values, rather than one, in the situations given in the sub-algorithm.  Call these two values <em>m</em> and <em>mbit</em>.</li>
<li>If <em>m</em> &lt; 0, go to the first substep.</li>
<li>If <em>mbit</em> &gt;= 0, add 2<sup><em>mbit</em></sup> times an unbiased random bit to <em>m</em> and subtract 1 from <em>mbit</em>.  If that bit is 1 or <em>mbit</em> &lt; 0, go to the next substep; otherwise, repeat this substep.</li>
<li>Return <em>n</em> if <em>d</em>*2<sup><em>k</em></sup> is greater than or equal to <em>m2</em>.</li>
<li>Add a uniform random integer in [0, 2<sup><em>mbit</em>+1</sup>) to <em>m</em>, then return min(<em>n</em>, <em>d</em>*2<sup><em>k</em></sup>+<em>m</em>).</li>
</ol></li>
</ol>

<p><a id=Sampling_Unbounded_Monotone_Density_Functions></a></p>

<h2>Sampling Unbounded Monotone Density Functions</h2>

<p>This section shows a preprocessing algorithm to generate a random variate in [0, 1] from a distribution whose probability density function (PDF)&mdash;</p>

<ul>
<li>is continuous in the interval [0, 1],</li>
<li>is monotonically decreasing in [0, 1], and</li>
<li>has an unbounded peak at 0.</li>
</ul>

<p>The trick here is to sample the peak in such a way that the result is either forced to be 0 or forced to belong to the bounded part of the PDF.  This algorithm does not require the area under the curve of the PDF in [0, 1] to be 1; in other words, this algorithm works even if the PDF is known up to a normalizing constant.  The algorithm is as follows.</p>

<ol>
<li>Set <em>i</em> to 1.</li>
<li>Calculate the cumulative probability of the interval [0, 2<sup>&minus;<em>i</em></sup>] and that of [0, 2<sup>&minus;(<em>i</em> &minus; 1)</sup>], call them <em>p</em> and <em>t</em>, respectively.</li>
<li>With probability <em>p</em>/<em>t</em>, add 1 to <em>i</em> and go to step 2. (Alternatively, if <em>i</em> is equal to or higher than the desired number of fractional bits in the result, return 0 instead of adding 1 and going to step 2.)</li>
<li>At this point, the PDF at [2<sup>&minus;<em>i</em></sup>, 2<sup>&minus;(<em>i</em> &minus; 1)</sup>) is bounded from above, so sample a random variate in this interval using any appropriate algorithm, including rejection sampling.  Because the PDF is monotonically decreasing, the peak of the PDF at this interval is located at 2<sup>&minus;<em>i</em></sup>, so that rejection sampling becomes trivial.</li>
</ol>

<p>It is relatively straightforward to adapt this algorithm for monotonically increasing PDFs with the unbounded peak at 1, or to PDFs with a different domain than [0, 1].</p>

<p>This algorithm is similar to the &quot;inversion&ndash;rejection&quot; algorithm mentioned in section 4.4 of chapter 7 of Devroye&#39;s <em>Non-Uniform Random Variate Generation</em> (1986)<sup><a href="#Note3"><strong>(3)</strong></a></sup>.  I was unaware of that algorithm at the time I started writing the text that became this section (Jul. 25, 2020).  The difference here is that it assumes the whole distribution (including its PDF and cumulative distribution function) can take on any value in the interval [0, 1] and only those values (that is, its <em>support</em> is [0, 1]), while the algorithm presented in this article doesn&#39;t make that assumption (e.g., the interval [0, 1] can cover only part of the PDF&#39;s support).</p>

<p>By the way, this algorithm arose while trying to devise an algorithm that can generate an integer power of a uniform random variate, with arbitrary precision, without actually calculating that power (a naïve calculation that is merely an approximation and usually introduces bias); for more information, see my other article on <a href="https://peteroupc.github.io/exporand.html"><strong>partially-sampled random numbers</strong></a>.  Even so, the algorithm I have come up with in this note may be of independent interest.</p>

<p>In the case of powers of a uniform [0, 1] random variate <em>X</em>, namely <em>X</em><sup><em>n</em></sup>, the ratio <em>p</em>/<em>t</em> in this algorithm has a very simple form, namely (1/2)<sup>1/<em>n</em></sup>, which is possible to simulate using a so-called <em>Bernoulli factory</em> algorithm without actually having to calculate this ratio.  Note that this formula is the same regardless of <em>i</em>.  This is found by taking the PDF f(<em>x</em>) = <em>x</em><sup>1/<em>n</em></sup>/(<em>x</em> * <em>n</em>)</sup> and finding the appropriate <em>p</em>/<em>t</em> ratios by integrating <em>f</em> over the two intervals mentioned in step 2 of the algorithm.</p>

<p><a id=Certain_Families_of_Distributions></a></p>

<h2>Certain Families of Distributions</h2>

<p>This section is a note on certain families of univariate (one-variable) probability distributions, with
emphasis on sampling random variates from them.  Some of these families are described in Ahmad et al. (2019)<sup><a href="#Note8"><strong>(8)</strong></a></sup>.</p>

<p>The following definitions are used:</p>

<ul>
<li>A distribution&#39;s <em>quantile function</em> (also known as <em>inverse cumulative distribution function</em> or <em>inverse CDF</em>) is a nondecreasing function that maps uniform random variates in the closed interval [0, 1] to numbers that follow the distribution.</li>
<li>A distribution&#39;s <em>support</em> is the set of values the distribution can take on.  For example, the beta distribution&#39;s support is the interval [0, 1], and the normal distribution&#39;s support is the entire real line.</li>
</ul>

<p>In general, families of the form &quot;X-G&quot; (such as &quot;beta-G&quot; (Eugene et al., 2002)<sup><a href="#Note9"><strong>(9)</strong></a></sup>) use two distributions, X and G, where X is a continuous distribution whose support is the interval [0, 1] and G is a distribution with an easy-to-compute quantile function.  The following algorithm samples a random variate following a distribution from this kind of family:</p>

<ol>
<li>Generate a random variate that follows the distribution X. (Or generate a uniform <a href="https://peteroupc.github.io/exporand.html"><strong>partially-sampled random number (PSRN)</strong></a> that follows the distribution X.)  Call the number <em>x</em>.</li>
<li>Calculate the quantile for G of <em>x</em>, and return that quantile. (If <em>x</em> is a uniform PSRN, see the note at the end of this section.)</li>
</ol>

<p>Certain special cases of the &quot;X-G&quot; families, such as the following, use a specially designed distribution for X:</p>

<ul>
<li>The <em>alpha power</em> or <em>alpha power transformed</em> family (Mahdavi and Kundu 2017)<sup><a href="#Note10"><strong>(10)</strong></a></sup>. The family uses a shape parameter <em>&alpha;</em> &gt; 0; step 1 is modified to read: &quot;Generate a uniform(0, 1) random variate <em>U</em>, then set <em>x</em> to ln((<em>&alpha;</em>&minus;1)*<em>U</em> + 1)/ln(<em>&alpha;</em>) if <em>&alpha;</em> != 1, and <em>U</em> otherwise.&quot;</li>
<li>The <em>exponentiated</em> family (Mudholkar and Srivastava 1993)<sup><a href="#Note11"><strong>(11)</strong></a></sup>. The family uses a shape parameter <em>a</em> &gt; 1; step 1 is modified to read: &quot;Generate a uniform(0, 1) random variate <em>u</em>, then set <em>x</em> to <em>u</em><sup>1/<em>a</em></sup>.&quot;</li>
<li>The <em>transmuted-G</em> family (described, for example, by Tahir and Cordeiro (2016)<sup><a href="#Note12"><strong>(12)</strong></a></sup>). The family uses a shape parameter <em>&eta;</em> in the interval [&minus;1, 1]; step 1 is modified to read: &quot;Generate a piecewise linear random variate in [0, 1] with weight 1&minus;<em>&eta;</em> at 0 and weight 1+<em>&eta;</em> at 1, call the number <em>x</em>. (It can be generated as follows, see also (Devroye 1986, p. 71-72)<sup><a href="#Note3"><strong>(3)</strong></a></sup>: With probability min(1&minus;<em>&eta;</em>, 1+<em>&eta;</em>), generate <em>x</em>, a uniform(0, 1) random variate. Otherwise, generate two uniform(0, 1) random variates, set <em>x</em> to the higher of the two, then if <em>&eta;</em> is less than 0, set <em>x</em> to 1&minus;<em>x</em>.)&quot;.</li>
</ul>

<p>In fact, the &quot;X-G&quot; families are a special case of the so-called &quot;transformed&ndash;transformer&quot; family of distributions introduced by Alzaatreh et al. (2013)<sup><a href="#Note13"><strong>(13)</strong></a></sup> that uses two distributions, X and G, where X (the &quot;transformed&quot;) is an arbitrary continuous distribution, G (the &quot;transformer&quot;) is a distribution with an easy-to-compute quantile function, and <em>W</em> is a nondecreasing function that maps a number in [0, 1] to a number with the same support as X and meets certain other conditions.  The following algorithm samples a random variate from this kind of family:</p>

<ol>
<li>Generate a random variate that follows the distribution X. (Or generate a uniform PSRN that follows X.) Call the number <em>x</em>.</li>
<li>Calculate the quantile for G of <em>W</em><sup>&minus;1</sup>(<em>x</em>) (where <em>W</em><sup>&minus;1</sup>(.) is the inverse of <em>W</em>), and return that quantile. (If <em>x</em> is a uniform PSRN, see the note at the end of this section.)</li>
</ol>

<p>The following are special cases of the &quot;transformed&ndash;transformer&quot; family:</p>

<ul>
<li>The &quot;T-R{<em>Y</em>}&quot; family (Aljarrah et al., 2014)<sup><a href="#Note14"><strong>(14)</strong></a></sup>, in which <em>T</em> is an arbitrary continuous distribution (X in the algorithm above), <em>R</em> is a distribution with an easy-to-compute quantile function (G in the algorithm above), and <em>W</em> is the quantile function for the distribution <em>Y</em>, whose support must be included in the support of <em>T</em> (so that <em>W</em><sup>&minus;1</sup>(<em>x</em>) is the CDF for <em>Y</em>).</li>
<li>Several versions of <em>W</em> have been proposed for the case when distribution X&#39;s support is [0, &infin;), such as the Rayleigh and gamma distributions.  They include:

<ul>
<li><em>W</em>(<em>x</em>) = &minus;ln(1&minus;<em>x</em>) (<em>W</em><sup>&minus;1</sup>(<em>x</em>) = 1&minus;exp(&minus;<em>x</em>)).  Suggested in the original paper by Alzaatreh et al.</li>
<li><em>W</em>(<em>x</em>) = <em>x</em>/(1&minus;<em>x</em>) (<em>W</em><sup>&minus;1</sup>(<em>x</em>) = <em>x</em>/(1+<em>x</em>)).  Suggested in the original paper by Alzaatreh et al.  This choice forms the so-called &quot;odd X G&quot; family, and one example is the &quot;odd log-logistic G&quot; family (Gleaton and Lynch 2006)<sup><a href="#Note15"><strong>(15)</strong></a></sup>.</li>
</ul></li>
</ul>

<p>Many special cases of the &quot;transformed&ndash;transformer&quot; family have been proposed in many papers, and usually their names suggest the distributions that make up this family.  Some members of the &quot;odd X G&quot; family have names that begin with the word &quot;generalized&quot;, and in most such cases this corresponds to <em>W</em><sup>&minus;1</sup>(<em>x</em>) = (<em>x</em>/(1+<em>x</em>))<sup>1/<em>a</em></sup>, where <em>a</em> &gt; 0 is a shape parameter; examples include the &quot;generalized odd gamma-G&quot; family (Hosseini et al. 2018)<sup><a href="#Note16"><strong>(16)</strong></a></sup>.</p>

<p>A family very similar to the &quot;transformed&ndash;transformer&quot; family uses a <em>decreasing</em> <em>W</em>.  When distribution X&#39;s support is [0, &infin;), one such <em>W</em> that has been proposed is <em>W</em>(<em>x</em>) = &minus;ln(<em>x</em>) (<em>W</em><sup>&minus;1</sup>(<em>x</em>) = exp(&minus;<em>x</em>); examples include the &quot;Rayleigh-G&quot; family or &quot;Rayleigh&ndash;Rayleigh&quot; distribution (Al Noor and Assi 2020)<sup><a href="#Note17"><strong>(17)</strong></a></sup>, as well as the &quot;generalized gamma-G&quot; family, where &quot;generalized gamma&quot; refers to the Stacy distribution (Boshi et al. 2020)<sup><a href="#Note18"><strong>(18)</strong></a></sup>).</p>

<p>A <em>compound distribution</em> is simply the minimum of <em>N</em> random variates distributed as <em>X</em>, where <em>N</em> &gt;= 1 is an integer distributed as the discrete distribution <em>Y</em> (Tahir and Cordeiro 2016)<sup><a href="#Note12"><strong>(12)</strong></a></sup>.  For example, the &quot;beta-G-geometric&quot; family represents the minimum of <em>N</em> beta-G random variates, where <em>N</em> is a random variate expressing 1 plus the number of failures before the first success, with each success having the same probability.</p>

<p>A <em>complementary compound distribution</em> is the maximum of <em>N</em> random variates distributed as <em>X</em>, where <em>N</em> &gt;= 1 is an integer distributed as the discrete distribution <em>Y</em>.  An example is the &quot;geometric zero-truncated Poisson distribution&quot;, where <em>X</em> is the distribution of 1 plus the number of failures before the first success, with each success having the same probability, and <em>Y</em> is the zero-truncated Poisson distribution (Akdoğan et al., 2020)<sup><a href="#Note19"><strong>(19)</strong></a></sup>.</p>

<p>An <em>inverse X distribution</em> (or <em>inverted X distribution</em>) is generally the distribution of the reciprocal of a random variate distributed as <em>X</em>.  For example, an <em>inverse exponential</em> random variate (Keller and Kamath 1982)<sup><a href="#Note20"><strong>(20)</strong></a></sup> is the reciprocal of an exponential(1) random variate (and so is distributed as &minus;1/ln(<em>U</em>) where <em>U</em> is a uniform(0, 1) random variate) and may be scaled by a parameter <em>&theta;</em> &gt; 0.</p>

<p>A <em>weight-biased X</em> or <em>weighted X distribution</em> uses a distribution X and a weight function <em>w</em>(<em>x</em>) whose values lie in [0, 1] everywhere in X&#39;s support.  The following algorithm samples from a weighted distribution (see also (Devroye 1986, p. 47)<sup><a href="#Note3"><strong>(3)</strong></a></sup>):</p>

<ol>
<li>Generate a random variate that follows the distribution X. (Or generate a uniform PSRN that follows X.) Call the number <em>x</em>.</li>
<li>With probability <em>w</em>(<em>x</em>), return <em>x</em>.  Otherwise, go to step 1.</li>
</ol>

<p>To generate an <em>inflated X</em> random variate with parameters <em>c</em> and <em>&alpha;</em>, generate&mdash;</p>

<ul>
<li><em>c</em> with probability <em>&alpha;</em>, and</li>
<li>a random variate distributed as X otherwise.</li>
</ul>

<p>For example, a <em>zero-inflated beta</em> random variate is 0 with probability <em>&alpha;</em> and a beta random variate otherwise (the parameter <em>c</em> is 0) (Ospina and Ferrari 2010)<sup><a href="#Note21"><strong>(21)</strong></a></sup>  A zero-and-one inflated X distribution is 0 or 1 with probability <em>&alpha;</em> and distributed as X otherwise.  For example, to generate a <em>zero-and-one-inflated unit Lindley</em> random variate (with parameters <em>&alpha;</em>, <em>&theta;</em>, and <em>p</em>) (Chakraborty and Bhattacharjee 2021)<sup><a href="#Note22"><strong>(22)</strong></a></sup>:</p>

<ol>
<li>With probability <em>&alpha;</em>, return a number that is 0 with probability <em>p</em> and 1 otherwise.</li>
<li>Generate a unit Lindley(<em>&theta;</em>) random variate, that is, generate <em>x</em>/(1+<em>x</em>) where <em>x</em> is a <a href="https://peteroupc.github.io/morealg.html#Lindley_Distribution_and_Lindley_Like_Mixtures"><strong>Lindley(<em>&theta;</em>) random variate</strong></a>.</li>
</ol>

<blockquote>
<p><strong>Note</strong>: For more on quantile generation, see &quot;Randomization via Quantiles&quot; later on this page.</p>
</blockquote>

<p><a id=Certain_Distributions></a></p>

<h2>Certain Distributions</h2>

<p>In the table below, <em>U</em> is a uniform(0, 1) random variate.</p>

<table><thead>
<tr>
<th>This distribution:</th>
<th>Is distributed as:</th>
<th>And uses these parameters:</th>
</tr>
</thead><tbody>
<tr>
<td>Power function(<em>a</em>, <em>c</em>).</td>
<td><em>c</em>*<em>U</em><sup>1/<em>a</em></sup>.</td>
<td><em>a</em> &gt; 0, <em>c</em> &gt; 0.</td>
</tr>
<tr>
<td>Right-truncated Weibull(<em>a</em>, <em>b</em>, <em>c</em>) (Jodrá 2020)<sup><a href="#Note23"><strong>(23)</strong></a></sup>.</td>
<td>Minimum of <em>N</em> power function(<em>b</em>, <em>c</em>) random variates, where <em>N</em> is zero-truncated Poisson(<em>a</em>*<em>c</em><sup><em>b</em></sup>).</td>
<td><em>a</em>, <em>b</em>, <em>c</em> &gt; 0.</td>
</tr>
<tr>
<td>Lehmann Weibull(<em>a1</em>, <em>a2</em>, <em>&beta;</em>) (Elgohari and Yousof 2020)<sup><a href="#Note24"><strong>(24)</strong></a></sup>.</td>
<td>(ln(1/<em>U</em>)/<em>&beta;</em>)<sup>1/<em>a1</em></sup>/<em>a2</em> or <em>E</em><sup>1/<em>a1</em></sup>/<em>a2</em></td>
<td><em>a1</em>, <em>a2</em>, <em>&beta;</em> &gt; 0. <em>E</em> is exponential(<em>&beta;</em>).</td>
</tr>
<tr>
<td>Marshall&ndash;Olkin(<em>&alpha;</em>).</td>
<td>(1&minus;<em>U</em>)/(<em>U</em>*(<em>&alpha;</em>&minus;1) + 1).</td>
<td><em>&alpha;</em> in [0, 1].</td>
</tr>
<tr>
<td>Lomax(<em>&alpha;</em>).</td>
<td>(&minus;1/(<em>U</em>&minus;1))<sup>1/<em>&alpha;</em></sup>&minus;1.</td>
<td><em>&alpha;</em> &gt; 0.</td>
</tr>
<tr>
<td>Power Lomax(<em>&alpha;</em>, <em>&beta;</em>) (Rady et al. 2016)<sup><a href="#Note25"><strong>(25)</strong></a></sup>.</td>
<td><em>L</em><sup>1/<em>&beta;</em></sup></td>
<td><em>&beta;</em> &gt; 0; <em>L</em> is Lomax(<em>&alpha;</em>).</td>
</tr>
</tbody></table>

<p><a id=Batching_Random_Samples_via_Randomness_Extraction></a></p>

<h2>Batching Random Samples via Randomness Extraction</h2>

<p>Devroye and Gravel (2020)<sup><a href="#Note26"><strong>(26)</strong></a></sup> suggest the following randomness extractor to reduce the number of random bits needed to produce a batch of samples by a sampling algorithm.  The extractor works based on the probability that the algorithm consumes <em>X</em> random bits to produce a specific output <em>Y</em> (or <em>P</em>(<em>X</em> | <em>Y</em>) for short):</p>

<ol>
<li>Start with the interval [0, 1].</li>
<li>For each pair (<em>X</em>, <em>Y</em>) in the batch, the interval shrinks from below by <em>P</em>(<em>X</em>&minus;1 | <em>Y</em>) and from above by <em>P</em>(<em>X</em> | <em>Y</em>). (For example, if [0.2, 0.8] (range 0.6) shrinks from below by 0.1 and from above by 0.8, the new interval is [0.2+0.1*0.6, 0.2+0.8*0.6] = [0.26, 0.68].  For correctness, though, the interval is not allowed to shrink to a single point, since otherwise step 3 would run forever.)</li>
<li>Extract the bits, starting from the binary point, that the final interval&#39;s lower and upper bound have in common (or 0 bits if the upper bound is 1). (For example, if the final interval is [0.101010..., 0.101110...] in binary, the bits 1, 0, 1 are extracted, since the common bits starting from the point are 101.)</li>
</ol>

<p>After a sampling method produces an output <em>Y</em>, both <em>X</em> (the number of random bits the sampler consumed) and <em>Y</em> (the output) are added to the batch and fed to the extractor, and new bits extracted this way are added to a queue for the sampling method to use to produce future outputs. (Notice that the number of bits extracted by the algorithm above grows as the batch grows, so only the new bits extracted this way are added to the queue this way.)</p>

<p>The issue of finding <em>P</em>(<em>X</em> | <em>Y</em>) is now discussed.  Generally, if the sampling method implements a random walk on a binary tree that is driven by unbiased random bits and has leaves labeled with one outcome each (Knuth and Yao 1976)<sup><a href="#Note27"><strong>(27)</strong></a></sup>, <em>P</em>(<em>X</em> | <em>Y</em>) is found as follows (and Claude Gravel clarified to me that this is the intention of the extractor algorithm): Take a weighted count of all leaves labeled <em>Y</em> up to depth <em>X</em> (where the weight for depth <em>z</em> is 1/2<sup><em>z</em></sup>), then divide it by a weighted count of all leaves labeled <em>Y</em> at all depths (for instance, if the tree has two leaves labeled <em>Y</em> at <em>z</em>=2, three at <em>z</em>=3, and three at <em>z</em>=4, and <em>X</em> is 3, then <em>P</em>(<em>X</em> | <em>Y</em>) is (2/2<sup>2</sup>+3/2<sup>3</sup>) / (2/2<sup>2</sup>+3/2<sup>3</sup>+3/2<sup>4</sup>)).  In the special case where the tree has at most 1 leaf labeled <em>Y</em> at every depth, this is implemented by finding <em>P</em>(<em>Y</em>), or the probability to output <em>Y</em>, then chopping <em>P</em>(<em>Y</em>) up to the <em>X</em><sup>th</sup> binary digit after the point and dividing by the original <em>P</em>(<em>Y</em>) (for instance, if <em>X</em> is 4 and P(<em>Y</em>) is 0.101011..., then <em>P</em>(<em>X</em> | <em>Y</em>) is 0.1010 / 0.101011...).</p>

<p>Unfortunately, <em>P</em>(<em>X</em> | <em>Y</em>) is not easy to calculate when the number of values <em>Y</em> can take on is large or even unbounded.  In this case, I can suggest the following ad hoc algorithm, which uses a randomness extractor that takes <em>bits</em> as input, such as the von Neumann, Peres, or Zhou&ndash;Bruck extractor (see &quot;<a href="https://peteroupc.github.io/randextract.html"><strong>Notes on Randomness Extraction</strong></a>&quot;).  The algorithm counts the number of bits it consumes (<em>X</em>) to produce an output, then feeds <em>X</em> to the extractor as follows.</p>

<ol>
<li>Let <em>z</em> be abs(<em>X</em>&minus;<em>lastX</em>), where <em>lastX</em> is either the last value of <em>X</em> fed to this extractor for this batch or 0 if there is no such value.</li>
<li>If <em>z</em> is greater than 0, feed the bits of <em>z</em> from most significant to least significant to a queue of extractor inputs.</li>
<li>Now, when the sampler consumes a random bit, it checks the input queue.  As long as 64 bits or more are in the input queue, the sampler dequeues 64 bits from it, runs the extractor on those bits, and adds the extracted bits to an output queue. (The number 64 can instead be any even number greater than 2.)  Then, if the output queue is not empty, the sampler dequeues a bit from that queue and uses that bit; otherwise it generates an unbiased random bit as usual.</li>
</ol>

<p><a id=Randomization_via_Quantiles></a></p>

<h2>Randomization via Quantiles</h2>

<p>This note is about generating random variates from a continuous distribution via inverse transform sampling (or via quantiles), using uniform <a href="https://peteroupc.github.io/exporand.html"><strong>partially-sampled random numbers (PSRNs)</strong></a>.  See &quot;Certain Families of Distributions&quot; for a definition of quantile functions.  A <em>uniform PSRN</em> is ultimately a number that lies in an interval [<em>a</em>, <em>b</em>]; it contains a sign, an integer part, and a fractional part made up of base-<em>&beta;</em> digits.</p>

<p>Let G be a distribution for which the quantile is wanted, and let <em>f</em>(.) be a function applied to <em>a</em> or <em>b</em> before calculating the quantile.</p>

<p>When a random variate <em>x</em> is a uniform PSRN in the closed interval [0, 1], then the following algorithm transforms that number to a random variate for the distribution associated with the quantile function, with a desired error tolerance of <em>&epsilon;</em> with probability 1 (see (Devroye and Gravel 2020)<sup><a href="#Note26"><strong>(26)</strong></a></sup>):</p>

<ol>
<li>Generate additional digits of <em>x</em> uniformly at random&mdash;thus shortening the interval [<em>a</em>, <em>b</em>]&mdash;until a lower bound of the quantile of <em>f</em>(<em>a</em>) and an upper bound of the quantile of <em>f</em>(<em>b</em>) differ by no more than 2*<em>&epsilon;</em>.  Call the two bounds <em>low</em> and <em>high</em>, respectively.</li>
<li>Return <em>low</em>+(<em>high</em>&minus;<em>low</em>)/2.</li>
</ol>

<p>If <em>f</em>(<em>t</em>) = <em>t</em> and the quantile function is <em>Lipschitz continuous</em>, which roughly means that it&#39;s a continuous function whose slope doesn&#39;t tend to a vertical slope anywhere, then the following algorithm generates a quantile with error tolerance <em>&epsilon;</em>:</p>

<ol>
<li>Let <em>d</em> be ceil((ln(max(1,<em>L</em>)) &minus; ln(<em>&epsilon;</em>)) / ln(<em>&beta;</em>)), where <em>L</em> is an upper bound of the quantile function&#39;s maximum slope (also known as the <em>Lipschitz constant</em>). For each digit among the first <em>d</em> digits in <em>x</em>&#39;s fractional part, if that digit is unsampled, set it to a digit chosen uniformly at random.</li>
<li>The PSRN <em>x</em> now lies in the interval [<em>a</em>, <em>b</em>].  Calculate lower and upper bounds of the quantiles of <em>a</em> and <em>b</em>, respectively, that are within <em>&epsilon;</em>/2 of the true quantiles, call the bounds <em>low</em> and <em>high</em>, respectively.</li>
<li>Return <em>low</em>+(<em>high</em>&minus;<em>low</em>)/2.</li>
</ol>

<p>This algorithm chooses a random interval of size equal to <em>&beta;</em><sup><em>d</em></sup>, and because the quantile function is Lipschitz continuous, the values at the interval&#39;s bounds are guaranteed to vary by no more than 2*<em>&epsilon;</em> (actually <em>&epsilon;</em>, but the calculation in step 2 adds an additional error of at most <em>&epsilon;</em>), which is needed to meet the tolerance <em>&epsilon;</em> (see also Devroye and Gravel 2020<sup><a href="#Note26"><strong>(26)</strong></a></sup>).  A Lipschitz continuous quantile function usually means that the distribution takes on only values in a bounded interval.</p>

<blockquote>
<p><strong>Note:</strong> I suspect that if the quantile function is continuous, has a minimum and maximum, and admits a modulus of continuity <em>&omega;</em>(<em>h</em>) that is monotone increasing, then <em>d</em> in step 1 above can be calculated as ceil(&minus;ln(<em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>))/ln(<em>&beta;</em>)), where <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>) is the inverse of the modulus of continuity.  (Loosely speaking, a modulus of continuity <em>&omega;</em>(<em>h</em>) gives the quantile function&#39;s maximum range in a window of size <em>h</em>, and the inverse modulus <em>&omega;</em><sup>&minus;1</sup>(<em>&epsilon;</em>) finds a window small enough that the quantile function differs by no more than <em>&epsilon;</em> in the window.<sup><a href="#Note28"><strong>(28)</strong></a></sup>)</p>
</blockquote>

<p>Both algorithms have a disadvantage: the desired error tolerance has to be made known to the algorithm in advance.  To generate a quantile to any error tolerance (even if the tolerance is not known in advance), a rejection sampling approach is needed.  For this to work:</p>

<ul>
<li>distribution G&#39;s probability density function, or a function proportional to it, must be known, and</li>
<li>that function must be continuous almost everywhere and bounded from above (see also (Devroye and Gravel 2020)<sup><a href="#Note26"><strong>(26)</strong></a></sup>).</li>
</ul>

<p>Here is a sketch of how this rejection sampler might work:</p>

<ol>
<li>Let <em>x</em> be as described in either algorithm given earlier in this section (so that <em>a</em> and <em>b</em> are <em>x</em>&#39;s upper and lower bounds).  Calculate lower and upper bounds of the quantiles of <em>f</em>(<em>a</em>) and <em>f</em>(<em>b</em>) (the bounds are [<em>alow</em>, <em>ahigh</em>] and [<em>blow</em>, <em>bhigh</em>] respectively).</li>
<li>Sample a uniform PSRN, <em>y</em>, using an arbitrary-precision rejection sampler such as Oberhoff&#39;s method (described in an <a href="https://peteroupc.github.io/exporand.html#Oberhoff_s_Exact_Rejection_Sampling_Method"><strong>appendix to the PSRN article</strong></a>), on the distribution G limited to the interval [<em>alow</em>, <em>bhigh</em>].</li>
<li>Accept <em>y</em> (and return it) if it clearly lies in [<em>ahigh</em>, <em>blow</em>].  Reject <em>y</em> (and go to the previous step) if it clearly lies outside [<em>alow</em>, <em>bhigh</em>].  If <em>y</em> clearly lies in [<em>alow</em>, <em>ahigh</em>] or in [<em>blow, _bhigh</em>], generate more digits of <em>x</em>, uniformly at random, and go to the first step.</li>
<li>If <em>y</em> doesn&#39;t clearly fall in any of the cases in the previous step, generate more digits of <em>y</em>, uniformly at random, and go to the previous step.</li>
</ol>

<p><a id=ExpoExact></a></p>

<h2>ExpoExact</h2>

<p>This algorithm <code>ExpoExact</code>, samples an exponential random variate given the rate <code>rx</code>/<code>ry</code> with an error tolerance of 2<sup><code>-precision</code></sup>; for more information, see &quot;<a href="https://peteroupc.github.io/exporand.html"><strong>Partially-Sampled Random Numbers</strong></a>&quot;; see also Morina et al. (2019)<sup><a href="#Note29"><strong>(29)</strong></a></sup>; Canonne et al. (2020)<sup><a href="#Note30"><strong>(30)</strong></a></sup>.  In this section, <code>RNDINT(1)</code> generates an independent unbiased random bit.</p>

<pre>METHOD ZeroOrOneExpMinus(x, y)
  if y &lt;= 0 or x&lt;0: return error
  if x==0: return 1 // exp(0) = 1
  if x &gt; y
    x = rem(x, y)
    if x&gt;0 and ZeroOrOneExpMinus(x, y) == 0: return 0
    for i in 0...floor(x/y): if ZeroOrOneExpMinus(1,1) == 0: return 0
    return 1
  end
  r = 1
  oy = y
  while true
    if ZeroOrOne(x, y) == 0: return r
    r=1-r; y = y + oy
  end
END METHOD

METHOD ExpoExact(rx, ry, precision)
   ret=0
   for i in 1..precision
    // This loop adds to ret with probability 1/(exp(2^-prec)+1).
    // References: Alg. 6 of Morina et al. 2019; Canonne et al. 2020.
    denom=pow(2,i)*ry
    while true
       if RNDINT(1)==0: break
       if ZeroOrOneExpMinus(rx, denom) == 1:
         ret=ret+MakeRatio(1,pow(2,i))
    end
   end
   while ZeroOrOneExpMinus(rx,ry)==1: ret=ret+1
   return ret
END METHOD
</pre>

<blockquote>
<p><strong>Note:</strong> After <code>ExpoExact</code> is used to generate a random variate, an application can append additional binary digits (such as <code>RNDINT(1)</code>) to the end of that number while remaining accurate to the given precision (Karney 2014)<sup><a href="#Note31"><strong>(31)</strong></a></sup></p>
</blockquote>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<ul>
<li><small><sup id=Note1>(1)</sup> K. Bringmann, F. Kuhn, et al., “Internal DLA: Efficient Simulation of a Physical Growth Model.” In: <em>Proc. 41st International Colloquium on Automata, Languages, and Programming (ICALP&#39;14)</em>, 2014.</small></li>
<li><small><sup id=Note2>(2)</sup> choose(<em>n</em>, <em>k</em>) = <em>n</em>!/(<em>k</em>! * (<em>n</em> &minus; <em>k</em>)!) is a <em>binomial coefficient</em>, or the number of ways to choose <em>k</em> out of <em>n</em> labeled items.  It can be calculated, for example, by calculating <em>i</em>/(<em>n</em>&minus;<em>i</em>+1) for each integer <em>i</em> in the interval [<em>n</em>&minus;<em>k</em>+1, <em>n</em>], then multiplying the results (Yannis Manolopoulos. 2002. &quot;<a href="https://doi.org/10.1145/820127.820168"><strong>Binomial coefficient computation: recursion or iteration?</strong></a>&quot;, SIGCSE Bull. 34, 4 (December 2002), 65–67).  Note that for every <em>m</em>&gt;0, choose(<em>m</em>, 0) = choose(<em>m</em>, <em>m</em>) = 1 and choose(<em>m</em>, 1) = choose(<em>m</em>, <em>m</em>&minus;1) = <em>m</em>; also, in this document, choose(<em>n</em>, <em>k</em>) is 0 when <em>k</em> is less than 0 or greater than <em>n</em>.</small></li>
<li><small><sup id=Note3>(3)</sup> Devroye, L., <a href="http://luc.devroye.org/rnbookindex.html"><strong><em>Non-Uniform Random Variate Generation</em></strong></a>, 1986.</small></li>
<li><small><sup id=Note4>(4)</sup> Daumas, M., Lester, D., Muñoz, C., &quot;<a href="https://arxiv.org/abs/0708.3721"><strong>Verified Real Number Calculations: A Library for Interval Arithmetic</strong></a>&quot;, arXiv:0708.3721 [cs.MS], 2007.</small></li>
<li><small><sup id=Note5>(5)</sup> R. Schumacher, &quot;<a href="https://arxiv.org/abs/1602.00336v1"><strong>Rapidly Convergent Summation Formulas involving Stirling Series</strong></a>&quot;, arXiv:1602.00336v1 [math.NT], 2016.</small></li>
<li><small><sup id=Note6>(6)</sup> Farach-Colton, M. and Tsai, M.T., 2015. Exact sublinear binomial sampling. <em>Algorithmica</em> 73(4), pp. 637-651.</small></li>
<li><small><sup id=Note7>(7)</sup> Bringmann, K., and Friedrich, T., 2013, July. Exact and efficient generation of geometric random variates and random graphs, in <em>International Colloquium on Automata, Languages, and Programming</em> (pp. 267-278).</small></li>
<li><small><sup id=Note8>(8)</sup> Ahmad, Z. et al. &quot;Recent Developments in Distribution Theory: A Brief Survey and Some New Generalized Classes of distributions.&quot; Pakistan Journal of Statistics and Operation Research 15 (2019): 87-110.</small></li>
<li><small><sup id=Note9>(9)</sup> Eugene, N., Lee, C., Famoye, F., &quot;Beta-normal distribution and its applications&quot;, <em>Commun. Stat. Theory Methods</em> 31, 2002.</small></li>
<li><small><sup id=Note10>(10)</sup> Mahdavi, Abbas, and Debasis Kundu. &quot;A new method for generating distributions with an application to exponential distribution.&quot; <em>Communications in Statistics -- Theory and Methods</em> 46, no. 13 (2017): 6543-6557.</small></li>
<li><small><sup id=Note11>(11)</sup> Mudholkar, G. S., Srivastava, D. K., &quot;Exponentiated Weibull family for analyzing bathtub failure-rate data&quot;, <em>IEEE Transactions on Reliability</em> 42(2), 299-302, 1993.</small></li>
<li><small><sup id=Note12>(12)</sup> Tahir, M.H., Cordeiro, G.M., &quot;Compounding of distributions: a survey and new generalized classes&quot;, <em>Journal of Statistical Distributions and Applications</em> 3(13), 2016.</small></li>
<li><small><sup id=Note13>(13)</sup> Alzaatreh, A., Famoye, F., Lee, C., &quot;A new method for generating families of continuous distributions&quot;, <em>Metron</em> 71:63–79 (2013).</small></li>
<li><small><sup id=Note14>(14)</sup> Aljarrah, M.A., Lee, C. and Famoye, F., &quot;On generating T-X family of distributions using quantile functions&quot;, Journal of Statistical Distributions and Applications,1(2), 2014.</small></li>
<li><small><sup id=Note15>(15)</sup> Gleaton, J.U., Lynch, J. D., &quot;Properties of generalized log-logistic families of lifetime distributions&quot;, <em>Journal of Probability and Statistical Science</em> 4(1), 2006.</small></li>
<li><small><sup id=Note16>(16)</sup> Hosseini, B., Afshari, M., &quot;The Generalized Odd Gamma-G Family of Distributions:  Properties and Application&quot;, <em>Austrian Journal of Statistics</em> vol. 47, Feb. 2018.</small></li>
<li><small><sup id=Note17>(17)</sup> N.H. Al Noor and N.K. Assi, &quot;Rayleigh-Rayleigh Distribution: Properties and Applications&quot;, <em>Journal of Physics: Conference Series</em> 1591, 012038 (2020).  The underlying Rayleigh distribution uses a parameter <em>&theta;</em> (or <em>&lambda;</em>), which is different from <em>Mathematica</em>&#39;s parameterization with <em>&sigma;</em> = sqrt(1/<em>&theta;</em><sup>2</sup>) = sqrt(1/<em>&lambda;</em><sup>2</sup>).  The first Rayleigh distribution uses <em>&theta;</em> and the second, <em>&lambda;</em>.</small></li>
<li><small><sup id=Note18>(18)</sup> Boshi, M.A.A., et al., &quot;Generalized Gamma – Generalized Gompertz Distribution&quot;, <em>Journal of Physics: Conference Series</em> 1591, 012043 (2020).</small></li>
<li><small><sup id=Note19>(19)</sup> Akdoğan, Y., Kus, C., et al., &quot;Geometric-Zero Truncated Poisson Distribution: Properties and Applications&quot;, <em>Gazi University Journal of Science</em> 32(4), 2019.</small></li>
<li><small><sup id=Note20>(20)</sup> Keller, A.Z., Kamath A.R., &quot;Reliability analysis of CNC machine tools&quot;, <em>Reliability Engineering</em> 3 (1982).</small></li>
<li><small><sup id=Note21>(21)</sup> Ospina, R., Ferrari, S.L.P., &quot;Inflated Beta Distributions&quot;, 2010.</small></li>
<li><small><sup id=Note22>(22)</sup> Chakraborty, S., Bhattacharjee, S., &quot;<a href="https://arxiv.org/abs/2103.08916"><strong>Modeling proportion of success in high school leaving examination- A comparative study of Inflated Unit Lindley and Inflated Beta distribution</strong></a>&quot;, arXiv:2103.08916 [stat.ME], 2021.</small></li>
<li><small><sup id=Note23>(23)</sup> Jodrá, P., &quot;A note on the right truncated Weibull distribution and the minimum of power function distributions&quot;, 2020.</small></li>
<li><small><sup id=Note24>(24)</sup> Elgohari, Hanaa, and Haitham Yousof. &quot;New Extension of Weibull Distribution: Copula, Mathematical Properties and Data Modeling.&quot; Stat., Optim. Inf. Comput., Vol.8, December 2020.</small></li>
<li><small><sup id=Note25>(25)</sup> Rady,  E.H.A.,  Hassanein,  W.A.,  Elhaddad,  T.A., &quot;The power Lomax distribution with an application to bladder cancer data&quot;, (2016).</small></li>
<li><small><sup id=Note26>(26)</sup> Devroye, L., Gravel, C., &quot;<a href="https://arxiv.org/abs/1502.02539v6"><strong>Random variate generation using only finitely many unbiased, independently and identically distributed random bits</strong></a>&quot;, arXiv:1502.02539v6  [cs.IT], 2020.</small></li>
<li><small><sup id=Note27>(27)</sup> Knuth, Donald E. and Andrew Chi-Chih Yao. &quot;The complexity of nonuniform random number generation&quot;, in <em>Algorithms and Complexity: New Directions and Recent Results</em>, 1976.</small></li>
<li><small><sup id=Note28>(28)</sup> Ker-I Ko makes heavy use of the inverse modulus of continuity in his complexity theory, e.g., &quot;Computational complexity of roots of real functions.&quot; In <em>30th Annual Symposium on Foundations of Computer Science</em>, pp. 204-209. IEEE Computer Society, 1989.</small></li>
<li><small><sup id=Note29>(29)</sup> Morina, G., Łatuszyński, K., et al., &quot;<a href="https://arxiv.org/abs/1912.09229v1"><strong>From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains</strong></a>&quot;, arXiv:1912.09229v1 [math.PR], 2019.</small></li>
<li><small><sup id=Note30>(30)</sup> Canonne, C., Kamath, G., Steinke, T., &quot;<a href="https://arxiv.org/abs/2004.00010"><strong>The Discrete Gaussian for Differential Privacy</strong></a>&quot;, arXiv:2004.00010 [cs.DS], 2020.</small></li>
<li><small><sup id=Note31>(31)</sup> Karney, C.F.F., &quot;<a href="https://arxiv.org/abs/1303.6257v2"><strong>Sampling exactly from the normal distribution</strong></a>&quot;, arXiv:1303.6257v2  [physics.comp-ph], 2014.</small></li>
</ul>

<p><a id=License></a></p>

<h2>License</h2>

<p>Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under <a href="https://creativecommons.org/publicdomain/zero/1.0/"><strong>Creative Commons Zero</strong></a>.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>
<p>
If you like this software, you should consider donating to me, Peter O., at the link below:</p>
<p class="printonly"><b>peteroupc.github.io</b></p>
<div class="noprint">
<a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=56E5T4FH7KD2S">
<img src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif"
name="submit" border="2" alt="PayPal - The safer, easier way to pay online!"></a>
<p>
<a href="//twitter.com/share">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
if("share" in navigator){
 document.getElementById("sharer").href="javascript:void(null)";
 document.getElementById("sharer").innerHTML="Share This Page";
 navigator.share({title:document.title,url:document.location.href}).then(
   function(){});
} else {
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
}
</script>
</body></html>
