<!DOCTYPE html><html><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>Partially-Sampled Random Numbers for Accurate Sampling of the Beta, Exponential, and Other Continuous Distributions</title><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css"></head><body>  <div class="header">
<p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a> -
<a href="http://peteroupc.github.io/">Donate to Me</a></p></div>
<div class="mainarea" id="top">
<h1>Partially-Sampled Random Numbers for Accurate Sampling of the Beta, Exponential, and Other Continuous Distributions</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><em>Note: Formerly &quot;Partially Sampled Exponential Random Numbers&quot;, due to a merger with &quot;An Exact Beta Generator&quot;.</em></p>

<p><a id=Introduction></a></p>

<h2>Introduction</h2>

<p>This page introduces a Python implementation of <em>partially-sampled random numbers</em>.  Although structures for partially-sampled random numbers were largely described before this work, this document unifies the concepts for these kinds of numbers from prior works and shows how they can be used to sample the beta distribution (with both parameters 1 or greater), the exponential distribution (with an arbitrary rate parameter), and other continuous distributions&mdash;</p>

<ul>
<li>while avoiding floating-point arithmetic, and</li>
<li>to an arbitrary precision and with user-specified error bounds (and thus in an &quot;exact&quot; manner in the sense defined in (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>).</li>
</ul>

<p>For instance, these two points distinguish the beta sampler in this document from any other specially-designed beta sampler I am aware of.  As for the exponential distribution, there are papers that discuss generating exponential random numbers using random bits (Flajolet and Saheb 1982)<sup><a href="#Note2"><strong>(2)</strong></a></sup>, (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>, (Devroye and Gravel 2015)<sup><a href="#Note3"><strong>(3)</strong></a></sup>, (Thomas and Luk 2008)<sup><a href="#Note4"><strong>(4)</strong></a></sup>, but almost all of them that I am aware of don&#39;t deal with generating partially-sampled exponential random numbers using an arbitrary rate, not just 1.  (Habibizad Navin et al., 2010)<sup><a href="#Note5"><strong>(5)</strong></a></sup>, which came to my attention on the afternoon of July 20, after I wrote much of this article, is perhaps an exception; however the approach appears to involve pregenerated tables of digit probabilities.</p>

<p>The samplers discussed here also draw on work dealing with a construct called the <em>Bernoulli factory</em> (Keane and O&#39;Brien 1994)<sup><a href="#Note6"><strong>(6)</strong></a></sup> (Flajolet et al., 2010)<sup><a href="#Note7"><strong>(7)</strong></a></sup>, which can simulate an arbitrary probability by transforming biased coins to biased coins.  One important feature of Bernoulli factories is that they can simulate a given probability <em>exactly</em>, without having to calculate that probability manually, which is important if the probability can be an irrational number that no computer can compute exactly (such as <code>pow(p, 1/2)</code> or <code>exp(-2)</code>).</p>

<p>This page shows <a href="#Sampler_Code"><strong>Python code</strong></a> for these samplers.</p>

<p><a id=About_the_Beta_Distribution></a></p>

<h2>About the Beta Distribution</h2>

<p>The <a href="https://en.wikipedia.org/wiki/Beta_distribution"><strong>beta distribution</strong></a> is a bounded-domain probability distribution; its two parameters, <code>alpha</code> and <code>beta</code>, are both greater than 0 and describe the distribution&#39;s shape.  Depending on <code>alpha</code> and <code>beta</code>, the shape can be a smooth peak or a smooth valley.  The beta distribution can take on values in the interval [0, 1].  Any value in this interval (<code>x</code>) can occur with a probability proportional to&mdash;</p>

<pre>pow(x, alpha - 1) * pow(1 - x, beta - 1).               (1)
</pre>

<p>Although <code>alpha</code> and <code>beta</code> can each be greater than 0, the sampler presented in this document only works if both parameters are 1 or greater.</p>

<p><a id=About_the_Exponential_Distribution></a></p>

<h2>About the Exponential Distribution</h2>

<p>The <em>exponential distribution</em> takes a parameter &lambda;.  Informally speaking, a random number that follows an exponential distribution is the number of units of time between one event and the next, and &lambda; is the expected average number of events per unit of time.  Usually, &lambda; is equal to 1.</p>

<p>An exponential random number is commonly generated as follows: <code>-ln(1 - RNDU01()) / lamda</code>, where <code>RNDU01()</code> is a uniform random number in the interval [0, 1).  (This particular formula is not robust, though, for reasons that are outside the scope of this document, but see (Pedersen 2018)<sup><a href="#Note8"><strong>(8)</strong></a></sup>.)  This page presents an alternative way to sample exponential random numbers.</p>

<p><a id=About_Partially_Sampled_Random_Numbers></a></p>

<h2>About Partially-Sampled Random Numbers</h2>

<p>In this document, a <em>partially-sampled</em> random number is a data structure that allows a random number that exactly follows a continuous distribution to be sampled digit by digit and with arbitrary precision, without relying on floating-point arithmetic or calculations of irrational or transcendental numbers (other than digit extractions).  Informally, they represent incomplete real numbers whose contents are sampled only when necessary, but in a way that follows the distribution being sampled.</p>

<p>The most trivial example of a <em>partially-sampled</em> random number is that of the uniform distribution in [0, 1].  Such a random number can be implemented as a list of items, where each item is either a digit (such as zero or one for binary), or a placeholder value (which represents an unsampled digit), and represents a list of the digits after the radix point, from left to right, of a real number in the interval [0, 1], that is, the number&#39;s <em>digit expansion</em> (e.g., <em>binary expansion</em> in the case of binary digits).  This kind of number is referred to&mdash;</p>

<ul>
<li>as a <em>geometric bag</em> in (Flajolet et al., 2010)<sup><a href="#Note7"><strong>(7)</strong></a></sup> (but only in the binary case), and</li>
<li>as a <em>u-rand</em> in (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.</li>
</ul>

<p>Each additional digit is sampled simply by setting it to an independent unbiased random digit, an observation that dates from von Neumann (1951)<sup><a href="#Note9"><strong>(9)</strong></a></sup> in the binary case.</p>

<p>This document may use the terms <em>geometric bag</em> and <em>partially-sampled random number</em> interchangeably.  Note that the <em>u-rand</em> concept by Karney only contemplates sampling bits from left to right without any gaps, whereas the geometric bag concept is more general in this respect.</p>

<p>Partially-sampled numbers of other distributions can be implemented via rejection from the uniform distribution. For example:</p>

<ol>
<li>The beta and continuous Bernoulli distributions, as discussed later in this document.</li>
<li>The standard normal distribution, as shown in (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup> by running Karney&#39;s Algorithm N and filling unsampled digits uniformly at random.</li>
<li>For uniform distributions in [0, <em>n</em>) (not just [0, 1]), a partially-sampled version might be trivial by first ensuring that the first &quot;few&quot; digits are such that the resulting number will be less than <em>n</em>, via rejection sampling.</li>
</ol>

<p>For these distributions (and others that are continuous almost everywhere and bounded from above), Oberhoff (2018)<sup><a href="#Note10"><strong>(10)</strong></a></sup> proved that unsampled trailing bits of the partially-sampled number converge to the uniform distribution.</p>

<p>As an additional example, in this document a partially-sampled exponential random number (or <em>e-rand</em>, named similarly to Karney&#39;s &quot;u-rands&quot; for partially-sampled uniform random numbers (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>) samples each bit that, when combined with the existing bits, results in an exponentially-distributed random number of the given rate.  Also, because <code>-ln(1 - RNDU01())</code> is exponentially distributed, e-rands can also represent the natural logarithm of a partially-sampled uniform random number in (0, 1].  The difference here is that additional bits are sampled not as unbiased random bits, but rather as bits with a vanishing bias.</p>

<p>Partially-sampled numbers could also be implemented via rejection from the exponential distribution, although no concrete examples are presented here.</p>

<p>On the other hand, the concept of <em>prefix distributions</em> (Oberhoff 2018)<sup><a href="#Note10"><strong>(10)</strong></a></sup> comes close to partially-sampled random numbers, but numbers sampled this way are not partially-sampled random numbers in the sense used here.  This is because the method requires calculating minimums of probabilities (and, in practice, requires the use of floating-point arithmetic in most cases).  Moreover, the method samples from a discrete distribution whose progression depends on the value of previously sampled bits, not just on the position of those bits as with the uniform and exponential distributions (see also (Thomas and Luk 2008)<sup><a href="#Note4"><strong>(4)</strong></a></sup>).</p>

<p><a id=Comparisons></a></p>

<h3>Comparisons</h3>

<p>Two partially-sampled random numbers, each of a different distribution but storing digits of the same radix, can be exactly compared to each other using an algorithm similar to the following. The <strong>RandLess</strong> algorithm compares two partially-sampled random numbers, <strong>a</strong> and <strong>b</strong> (and samples additional bits from them as necessary) and returns <code>true</code> if <strong>a</strong> turns out to be less than <strong>b</strong>, or <code>false</code> otherwise (see also (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>)).</p>

<ol>
<li>If <strong>a</strong>&#39;s integer part wasn&#39;t sampled yet, sample <strong>a</strong>&#39;s integer part.  Do the same for <strong>b</strong>.</li>
<li>Return <code>true</code> if <strong>a</strong>&#39;s integer part is less than <strong>b</strong>&#39;s, or <code>false</code> if <strong>a</strong>&#39;s integer part is greater than <strong>b</strong>&#39;s.</li>
<li>Set <em>i</em> to 0.</li>
<li>If <strong>a</strong>&#39;s fractional part has <em>i</em> or fewer digits, sample digit <em>i</em> of <strong>a</strong> (positions start at 0 where 0 is the most significant digit after the point, 1 is the next, etc.), and append the result to that fractional part&#39;s digit expansion.  Do the same for <strong>b</strong>.</li>
<li>Return <code>true</code> if <strong>a</strong>&#39;s fractional part is less than <strong>b</strong>&#39;s, or <code>false</code> if <strong>a</strong>&#39;s fractional part is greater than <strong>b</strong>&#39;s.</li>
<li>Add 1 to <em>i</em> and go to step 4.</li>
</ol>

<p><a id=Arithmetic></a></p>

<h3>Arithmetic</h3>

<p>Arithmetic between two partially-sampled random numbers is not exactly trivial.  The naïve approach of adding, multiplying or dividing two partially-sampled random numbers <em>A</em> and <em>B</em> (see also (Brassard et al., 2019)<sup><a href="#Note11"><strong>(11)</strong></a></sup>) may result in a partially-sampled number <em>C</em> that is only an approximation (even if <em>C</em>&#39;s distribution is close to the ideal by a given number of digit places), in the sense that appending additional digits to <em>C</em> (so that <em>C</em> now has <em>d</em> fractional digits) may cause <em>C</em> to follow a distribution that is not close to the ideal distribution (that is, the distribution that <em>C</em> would have if <em>A</em> and <em>B</em> had more digits) by <em>d</em> fractional digit places.</p>

<p>On the other hand, partially-sampled-number arithmetic may be possible by relating the relative probabilities of each digit, in the result&#39;s digit expansion, to some kind of formula.  (This ignores trivial arithmetic operations, such as addition by half provided the radix is even, or negation &mdash; both operations mentioned in (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup> &mdash; or operations affecting the integer part only.)  For example, I can show empirically that when an exponential(1) random number is multiplied by a uniform [0, 1] random number, the following (approximate) probabilities of 1 occur in the following positions of the result&#39;s binary expansion:</p>

<table><thead>
<tr>
<th>Position after the point</th>
<th>Approx. prob. of 1</th>
</tr>
</thead><tbody>
<tr>
<td>1st (half)</td>
<td>0.227233</td>
</tr>
<tr>
<td>2nd (quarter)</td>
<td>0.321432</td>
</tr>
<tr>
<td>3rd</td>
<td>0.388065</td>
</tr>
<tr>
<td>4th</td>
<td>0.433957</td>
</tr>
<tr>
<td>5th</td>
<td>0.461612</td>
</tr>
</tbody></table>

<p>There is previous work that relates continuous distributions to digit probabilities in a similar manner (but only in base 10) (Habibizad Navin et al., 2007)<sup><a href="#Note12"><strong>(12)</strong></a></sup>, (Nezhad et al., 2013)<sup><a href="#Note13"><strong>(13)</strong></a></sup>.</p>

<p>Finally, arithmetic with partially-sampled numbers may be possible if the result of the arithmetic is distributed with a known density function (e.g., one found via Rohatgi&#39;s formula (Rohatgi 1976)<sup><a href="#Note14"><strong>(14)</strong></a></sup>), allowing for an algorithm that implements rejection from the uniform or exponential distribution.  However, that density function may have an unbounded peak, thus ruling out rejection sampling in practice.  For example, if <em>X</em> is a partially-sampled uniform random number, then <em>X</em><sup>1/3</sup> is distributed as <code>pow(X, 3) * 3 / X</code>, which has an unbounded peak at 0, ruling out rejection samplers for <em>X</em><sup>1/3</sup> in practice.</p>

<p><a id=Building_Blocks></a></p>

<h2>Building Blocks</h2>

<p>This document relies on several building blocks described in this section.</p>

<p>One of them is the &quot;geometric bag&quot; technique by Flajolet and others (2010)<sup><a href="#Note7"><strong>(7)</strong></a></sup>, which generates heads or tails with a probability that is built up digit by digit.   A <em>geometric bag</em> was defined earlier.</p>

<p>The algorithm <strong>SampleGeometricBag</strong> is a Bernoulli factory algorithm.  For base 2, the algorithm is described as follows (see (Flajolet et al., 2010)<sup><a href="#Note7"><strong>(7)</strong></a></sup>):</p>

<ol>
<li> Let N be a geometric(1/2) random number.  In this document, a geometric(<em>p</em>) random number is the number of failures before the first success, where a success occurs with probability <em>p</em>. For example, flip fair coins until tails is flipped, then let N be the number of heads flipped this way.</li>
<li> If the item at position N in the geometric bag (positions start at 0) is not set to a digit (e.g., 0 or 1 for base 2), set the item at that position to a digit chosen uniformly at random (e.g., either 0 or 1 for base 2), increasing the geometric bag&#39;s capacity as necessary.  (As a result of this step, there may be &quot;gaps&quot; in the geometric bag where no digit was sampled yet.)</li>
<li> Return the item at position N.</li>
</ol>

<p>For another base (radix), such as 10 for decimal, this can be implemented as <strong>RandLess</strong>, with <strong>a</strong> being an empty partially-sampled uniform random number and <strong>b</strong> being the geometric bag.  (Digit <em>i</em> of <strong>a</strong> or <strong>b</strong> is sampled by choosing one uniformly at random and setting the item at position <em>i</em> to that digit; positions start at 0.) Return 1 if the algorithm returns <code>true</code>, or 0 otherwise.</p>

<p><strong>SampleGeometricBagComplement</strong> is the same as the <strong>SampleGeometricBag</strong> algorithm, except the return value is 1 minus the original return value.  The result is that if <strong>SampleGeometricBag</strong> outputs 1 with probability <em>U</em>, <strong>SampleGeometricBagComplement</strong> outputs 1 with probability 1 &minus; <em>U</em>.</p>

<p><strong>FillGeometricBag</strong> takes a geometric bag and generates a number whose fractional part has <code>p</code> digits as follows:</p>

<ol>
<li>For each position in [0, <code>p</code>), if the item at that position is not a digit, set the item there to to a digit chosen uniformly at random (e.g., either 0 or 1 for binary), increasing the geometric bag&#39;s capacity as necessary. (See also (Oberhoff 2018, sec. 8)<sup><a href="#Note10"><strong>(10)</strong></a></sup>.)</li>
<li>Take the first <code>p</code> digits of the geometric bag and return &Sigma;<sub><em>i</em>=0, <code>p</code>&minus;1</sub> bag[<em>i</em>] * <em>b</em><sup>&minus;<em>i</em>&minus;1</sup>, where <em>b</em> is the radix.  (If it somehow happens that digits beyond <code>p</code> are set to 0 or 1, then the implementation could choose instead to fill all unsampled digits between the first and the last set digit and return the full number, optionally rounding it to a number whose fractional part has <code>p</code> digits, with a rounding mode of choice.)</li>
</ol>

<p><strong>PowerBernoulliFactory</strong> is a Bernoulli factory algorithm that transforms a coin that produces heads with probability <code>p</code> into a coin that produces heads with probability <code>pow(p, y)</code>.  The case where <code>y</code> is in (0, 1) is due to recent work by Mendo (2019)<sup><a href="#Note15"><strong>(15)</strong></a></sup>.  The algorithm takes a Bernoulli factory sub-algorithm (the coin that produces heads with probability <code>p</code>) as well as the parameter <em>y</em>, and is described as follows:</p>

<ol>
<li>If <em>y</em> is equal to 1, call the sub-algorithm and return the result.</li>
<li>If <em>y</em> is greater than 1, call the sub-algorithm <code>floor(y)</code> times and call <strong>PowerBernoulliFactory</strong> (once) with <em>y</em> = <em>y</em> &minus; floor(<em>y</em>).  Return 1 if all these calls return 1; otherwise, return 0.</li>
<li><em>y</em> is less than 1, so set <em>i</em> to 1.</li>
<li>Call the sub-algorithm; if it returns 1, return 1.</li>
<li>Return 0 with probability <em>y</em>/<em>i</em>.</li>
<li>Add 1 to <em>i</em> and go to step 4.</li>
</ol>

<p>The <strong>kthsmallest</strong> method generates the &#39;k&#39;th smallest &#39;bitcount&#39;-digit uniform random number out of &#39;n&#39; of them, is also relied on by this beta sampler.  It is used when both <code>a</code> and <code>b</code> are integers, based on the known property that a beta random variable in this case is the <code>a</code>th smallest uniform (0, 1) random number out of <code>a + b - 1</code> of them (Devroye 1986, p. 431)<sup><a href="#Note16"><strong>(16)</strong></a></sup></sup>.</p>

<p><strong>kthsmallest</strong>, however, doesn&#39;t simply generate &#39;n&#39; &#39;bitcount&#39;-digit numbers and then sort them.  Rather, it builds up their digit expansions digit by digit, via partially-sampled random numbers.    It uses the observation that (in the binary case) each uniform (0, 1) random number is equally likely to be less than half or greater than half; thus, the number of uniform numbers that are less than half vs. greater than half follows a binomial(n, 1/2) distribution (and of the numbers less than half, say, the less-than-one-quarter vs. greater-than-one-quarter numbers follows the same distribution, and so on).    Thanks to this observation, the algorithm can generate a sorted sample &quot;on the fly&quot;.  A similar observation applies to other bases than base 2 if we use the multinomial distribution instead of the binomial distribution.  I am not aware of any other article or paper that describes the <strong>kthsmallest</strong> algorithm given here.</p>

<p>The algorithm is as follows:</p>

<ol>
<li>Create <code>n</code> empty partially-sampled random numbers.</li>
<li>Set <code>index</code> to 1.</li>
<li>If <code>index &lt;= k</code> and <code>index + n &gt;= k</code>:

<ol>
<li>Generate <strong>v</strong>, a multinomial random vector with <em>b</em> probabilities equal to 1/<em>b</em>, where <em>b</em> is the radix (for the binary case, <em>b</em> = 2, so this is equivalent to generating <code>LC</code> = binomial(<code>n</code>, 0.5) and setting <strong>v</strong> to {<code>LC</code>, <code>n - LC</code>}).</li>
<li>Starting at <code>index</code>, append the digit 0 to the first <strong>v</strong>[0] partially-sampled numbers, a 1 digit to the next <strong>v</strong>[1] partially-sampled numbers, and so on to appending a <em>b</em> &minus; 1 digit to the last <strong>v</strong>[<em>b</em> &minus; 1] partially-sampled numbers (for the binary case, this means appending a 0 bit to the first <code>LC</code> u-rands and a 1 bit to the next <code>n - LC</code> u-rands).</li>
<li>For each integer <em>i</em> in [0, <em>b</em>): If <strong>v</strong>[<em>i</em>] &gt; 1, repeat step 3 and these substeps with <code>index</code> = <code>index</code> + &Sigma;<sub><em>j</em>=0, <em>i</em>&minus;1</sub> <strong>v</strong>[<em>j</em>] and <code>n</code> = <strong>v</strong>[<em>i</em>]. (For the binary case, this means: If <code>LC &gt; 1</code>, repeat step 3 and these substeps with the same <code>index</code> and <code>n = LC</code>; then, if <code>n - LC &gt; 1</code>, repeat step 3 and these substeps with <code>index = index + LC</code>, and <code>n = n - LC</code>).</li>
</ol></li>
<li>Take the <code>k</code>th partially-sampled random number (starting at 1) and fill it with uniform random digits as necessary to give its fractional part <code>bitcount</code> many digits (similarly to <strong>FillGeometricBag</strong> above). Return that number.  (An implementation may instead just return the partially-sampled random number without filling it this way first, but the beta sampler described later doesn&#39;t use this alternative.)</li>
</ol>

<p><strong>Sampling an e-rand</strong> (a partially-sampled exponential random number) makes use of two observations (based on the parameter &lambda; of the exponential distribution):</p>

<ul>
<li>While a coin flip with probability of heads of exp(-&lambda;) is heads, the exponential random number is increased by 1.</li>
<li>If a coin flip with probability of heads of 1/(1+exp(&lambda;/2<sup><em>k</em></sup>)) is heads, the exponential random number is increased by 2<sup>-<em>k</em></sup>, where <em>k</em> &gt; 0 is an integer.</li>
</ul>

<p>(Devroye and Gravel 2015)<sup><a href="#Note3"><strong>(3)</strong></a></sup> already made these observations in their Appendix, but only for &lambda; = 1.</p>

<p>To implement these probabilities using just random bits, the sampler uses two algorithms:</p>

<ol>
<li>One to simulate a probability of the form <code>exp(-x/y)</code> (<strong>ZeroOrOneExpMinus</strong>).</li>
<li>One to simulate a probability of the form <code>1/(1+exp(x/(y*pow(2, prec))))</code> (<strong>LogisticExp</strong>).</li>
</ol>

<p>These two algorithms enable e-rands with rational-valued &lambda; parameters and are described below.</p>

<p>The <strong>ZeroOrOneExpMinus</strong> algorithm takes integers <em>x</em> &gt;= 0 and <em>y</em> &gt; 0 and outputs 1 with probability <code>exp(-x/y)</code> or 0 otherwise. It originates from (Canonne et al. 2020)<sup><a href="#Note17"><strong>(17)</strong></a></sup>.</p>

<ol>
<li>Special case: If <em>x</em> is 0, return 1. (This is because the probability becomes <code>exp(0) = 1</code>.)</li>
<li>If <code>x &gt; y</code> (so <em>x</em>/<em>y</em> is greater than 1), call <strong>ZeroOrOneExpMinus</strong> <code>floor(x/y)</code> times with <em>x</em> = <em>y</em> = 1 and once with <em>x</em> = <em>x</em> - floor(<em>x</em>/<em>y</em>) * <em>y</em> and <em>y</em> = <em>y</em>.  Return 1 if all these calls return 1; otherwise, return 0.</li>
<li>Set <em>r</em> to 1 and <em>i</em> to 1.</li>
<li>Return <em>r</em> with probability (<em>y</em> * <em>i</em> &minus; <em>x</em>) / (<em>y</em> * <em>i</em>).</li>
<li>Set <em>r</em> to 1 - <em>r</em>, add 1 to <em>i</em>, and go to step 4.</li>
</ol>

<p>The <strong>LogisticExp</strong> algorithm is a special case of the <em>logistic Bernoulli factory</em> given in (Morina et al. 2019)<sup><a href="#Note18"><strong>(18)</strong></a></sup>.  It takes integers <em>x</em> &gt;= 0,  <em>y</em> &gt; 0, and <em>prec</em> &gt; 0 and outputs 1 with probability <code>1/(1+exp(x/(y*pow(2, prec))))</code> and 0 otherwise.</p>

<ol>
<li>Return 0 with probability 1/2.</li>
<li>Call <strong>ZeroOrOneExpMinus</strong> with <em>x</em> = <em>x</em> and <em>y</em> = <em>y</em>*2<sup><em>prec</em></sup>.  If the call returns 1, return 1.</li>
<li>Go to step 1.</li>
</ol>

<p><a id=Algorithms_for_the_Beta_and_Exponential_Distributions></a></p>

<h2>Algorithms for the Beta and Exponential Distributions</h2>

<p>&nbsp;</p>

<p><a id=Beta_Distribution></a></p>

<h3>Beta Distribution</h3>

<p>All the building blocks are now in place to describe a <em>new</em> algorithm to sample the beta distribution, described as follows.  It takes three parameters: <em>a</em> &gt;= 1 and <em>b</em> &gt;= 1 are the parameters to the beta distribution, and <em>p</em> &gt; 0 is a precision parameter.</p>

<ol>
<li>Special case: If <em>a</em> = 1 and <em>b</em> = 1, return a uniform number whose fractional part has <em>p</em> digits (for example, in the binary case, RandomBits(<em>p</em>) / 2<sup><em>p</em></sup> where <code>RandomBits(x)</code> returns an x-bit block of unbiased random bits).</li>
<li>Special case: If <em>a</em> and <em>b</em> are both integers, return the result of <strong>kthsmallest</strong> with parameters (<em>a</em> &minus; <em>b</em> + 1) and <em>a</em> in that order, and fill it as necessary to give the number an <em>p</em>-digit fractional part (similarly to <strong>FillGeometricBag</strong> above).</li>
<li>Create an empty list to serve as a &quot;geometric bag&quot;.</li>
<li>Remove all digits from the geometric bag.  This will result in an empty uniform random number, <em>U</em>, for the following steps, which will accept <em>U</em> with probability <em>U</em><sup>a&minus;1</sup>*(1&minus;<em>U</em>)<sup>b&minus;1</sup>) (the proportional probability for the beta distribution), as <em>U</em> is built up.</li>
<li>Call the <strong>PowerBernoulliFactory</strong> using the <strong>SampleGeometricBag</strong> algorithm and parameter <em>a</em> &minus; 1 (which will return 1 with probability <em>U</em><sup>a&minus;1</sup>).  If the result is 0, go to step 4.</li>
<li>Call the <strong>PowerBernoulliFactory</strong> using the <strong>SampleGeometricBagComplement</strong> algorithm and parameter <em>b</em> &minus; 1 (which will return 1 with probability (1&minus;<em>U</em>)<sup>b&minus;1</sup>).  If the result is 0, go to step 4. (Note that steps 5 and 6 don&#39;t depend on each other and can be done in either order without affecting correctness, and this is taken advantage of in the Python code below.)</li>
<li><em>U</em> was accepted, so return the result of <strong>FillGeometricBag</strong>.</li>
</ol>

<p><a id=Exponential_Distribution></a></p>

<h3>Exponential Distribution</h3>

<p>We also have the necessary building blocks to describe how to sample e-rands.  As implemented in the Python code, an e-rand consists of five numbers: the first is a multiple of 2<sup><em>x</em></sup>, the second is <em>x</em>, the third is the integer part (initially &minus;1 to indicate the integer part wasn&#39;t sampled yet), and the fourth and fifth are the &lambda; parameter&#39;s numerator and denominator, respectively.</p>

<p>To sample bit <em>k</em> after the binary point of an exponential random number with rate &lambda; (where <em>k</em> = 1 means the first digit after the point, <em>k</em> = 2 means the second, etc.), call the <strong>LogisticExp</strong> algorithm with <em>x</em> = &lambda;&#39;s numerator, <em>y</em> = &lambda;&#39;s denominator, and <em>prec</em> = <em>k</em>.</p>

<p>The <strong>ExpRandLess</strong> algorithm is a special case of the general <strong>RandLess</strong> algorithm given earlier.  It compares two e-rands <strong>a</strong> and <strong>b</strong> (and samples additional bits from them as necessary) and returns <code>true</code> if <strong>a</strong> turns out to be less than <strong>b</strong>, or <code>false</code> otherwise. (Note that <strong>a</strong> and <strong>b</strong> are allowed to have different &lambda; parameters.)</p>

<ol>
<li>If <strong>a</strong>&#39;s integer part wasn&#39;t sampled yet, call <strong>ZeroOrOneExpMinus</strong> with <em>x</em> = &lambda;&#39;s numerator and <em>y</em> = &lambda;&#39;s denominator, until the call returns 0, then set the integer part to the number of times 1 was returned this way.  Do the same for <strong>b</strong>.</li>
<li>Return <code>true</code> if <strong>a</strong>&#39;s integer part is less than <strong>b</strong>&#39;s, or <code>false</code> if <strong>a</strong>&#39;s integer part is greater than <strong>b</strong>&#39;s.</li>
<li>Set <em>i</em> to 0.</li>
<li>If <strong>a</strong>&#39;s fractional part has <em>i</em> or fewer bits, call <strong>LogisticExp</strong> with <em>x</em> = &lambda;&#39;s numerator, <em>y</em> = &lambda;&#39;s denominator, and <em>prec</em> = <em>i</em> + 1, and append the result to that fractional part&#39;s binary expansion.  Do the same for <strong>b</strong>.</li>
<li>Return <code>true</code> if <strong>a</strong>&#39;s fractional part is less than <strong>b</strong>&#39;s, or <code>false</code> if <strong>a</strong>&#39;s fractional part is greater than <strong>b</strong>&#39;s.</li>
<li>Add 1 to <em>i</em> and go to step 4.</li>
</ol>

<p>The <strong>ExpRandFill</strong> algorithm takes an e-rand <strong>a</strong> and generates a number whose fractional part has <code>p</code> bits as follows:</p>

<ol>
<li>If <strong>a</strong>&#39;s integer part wasn&#39;t sampled yet, sample it as given in step 1 of <strong>ExpRandLess</strong>.</li>
<li>If <strong>a</strong>&#39;s fractional part has greater than <code>p</code> bits, round <strong>a</strong> to a number whose fractional part has <code>p</code> bits, and return that number.  The rounding can be done, for example, by discarding all bits beyond <code>p</code> bits after the place to be rounded, or by rounding to the nearest 2<sup>-p</sup>, ties-to-up, as done in the sample Python code.</li>
<li>While <strong>a</strong>&#39;s fractional part has fewer than <code>p</code> bits, call <strong>LogisticExp</strong> with <em>x</em> = &lambda;&#39;s numerator, <em>y</em> = &lambda;&#39;s denominator, and <em>prec</em> = <em>i</em>, where <em>i</em> is 1 plus the number of bits in <strong>a</strong>&#39;s fractional part, and append the result to that fractional part&#39;s binary expansion.</li>
<li>Return the number represented by <strong>a</strong>.</li>
</ol>

<p><a id=Sampler_Code></a></p>

<h2>Sampler Code</h2>

<p>The following Python code implements the beta sampler just described.  It relies on a class I wrote called &quot;<a href="https://github.com/peteroupc/peteroupc.github.io/blob/master/bernoulli.py"><strong>bernoulli.py</strong></a>&quot;, which collects a number of Bernoulli factories, some of which are relied on by the code below.  This includes the building blocks mentioned earlier.  Note that the code uses floating-point arithmetic only to convert the result of the sampler to a convenient form, namely a floating-point number.</p>

<p>This code is far from fast, though, at least in Python.</p>

<pre>import math
import random
import bernoulli
from randomgen import RandomGen
from fractions import Fraction

def _toreal(ret, precision):
        # NOTE: Although we convert to a floating-point
        # number here, this is not strictly necessary and
        # is merely for convenience.
        return ret*1.0/(1&lt;&lt;precision)

def betadist(b, ax, ay, bx, by, precision=53):
        # Beta distribution for alpha&gt;=1 and beta&gt;=1
        bag=[]
        bpower=Fraction(bx, by)-1
        apower=Fraction(ax, ay)-1
        # Special case for a=b=1
        if bpower==0 and apower==0:
           return _toreal(random.randint(0, (1&lt;&lt;precision)-1), 1&lt;&lt;precision)
        # Special case if a and b are integers
        if int(bpower) == bpower and int(apower) == apower:
           a=int(Fraction(ax, ay))
           b=int(Fraction(bx, by))
           return _toreal(RandomGen().kthsmallest(a+b-1,a, \
                  precision), precision)
        # Create a &quot;geometric bag&quot; to hold a uniform random
        # number (U), described by Flajolet et al. 2010
        gb=lambda: b.geometric_bag(bag)
        # Complement of &quot;geometric bag&quot;
        gbcomp=lambda: b.geometric_bag(bag)^1
        bPowerBigger=(bpower &gt; apower)
        while True:
           # Create a uniform random number (U) bit-by-bit, and
           # accept it with probability U^(a-1)*(1-U)^(b-1), which
           # is the unnormalized PDF of the beta distribution
           bag.clear()
           r=1
           if bPowerBigger:
             # Produce 1 with probability (1-U)^(b-1)
             r=b.power(gbcomp, bpower)
             # Produce 1 with probability U^(a-1)
             if r==1: r=b.power(gb, apower)
           else:
             # Produce 1 with probability U^(a-1)
             r=b.power(gb, apower)
             # Produce 1 with probability (1-U)^(b-1)
             if r==1: r=b.power(gbcomp, bpower)
           if r == 1:
                 # Accepted, so fill up the &quot;bag&quot; and return the
                 # uniform number
                 ret=_fill_geometric_bag(b, bag, precision)
                 return ret

def _fill_geometric_bag(b, bag, precision):
        ret=0
        lb=min(len(bag), precision)
        for i in range(lb):
           if i&gt;=len(bag) or bag[i]==None:
              ret=(ret&lt;&lt;1)|b.randbit()
           else:
              ret=(ret&lt;&lt;1)|bag[i]
        if len(bag) &lt; precision:
           diff=precision-len(bag)
           ret=(ret &lt;&lt; diff)|random.randint(0,(1 &lt;&lt; diff)-1)
        # Now we have a number that is a multiple of
        # 2^-precision.
        return _toreal(ret, precision)
</pre>

<p>The following Python code implements the exponential sampler described earlier.  In the Python code below, note that <code>zero_or_one_exp_minus</code> uses <code>random.randint</code> which does not necessarily use only random bits; it could be replaced with a random-bit-only algorithm such as FastDiceRoller or Bernoulli, both of which were presented by Lumbroso (2013)<sup><a href="#Note19"><strong>(19)</strong></a></sup></p>

<pre>import random

def logisticexp(ln, ld, prec):
        &quot;&quot;&quot; Returns 1 with probability 1/(1+exp(ln/(ld*2^prec))). &quot;&quot;&quot;
        denom=ld*2**prec
        while True:
           if zero_or_one(1, 2)==0: return 0
           if zero_or_one_exp_minus(ln, denom) == 1: return 1

def exprandnew(lamdanum=1, lamdaden=1):
     &quot;&quot;&quot; Returns an object to serve as a partially-sampled
          exponential random number with the given
          rate &#39;lamdanum&#39;/&#39;lamdaden&#39;.  The object is a list of five numbers
          as given in the prose.  Default for &#39;lamdanum&#39;
          and &#39;lamdaden&#39; is 1.
          The number created by this method will be &quot;empty&quot;
          (no bits sampled yet).
          &quot;&quot;&quot;
     return [0, 0, -1, lamdanum, lamdaden]

def exprandfill(a, bits):
    &quot;&quot;&quot; Fills the unsampled bits of the given exponential random number
           &#39;a&#39; as necessary to make a number whose fractional part
           has &#39;bits&#39; many bits.  If the number&#39;s fractional part already has
           that many bits or more, the number is rounded using the round-to-nearest,
           ties to even rounding rule.  Returns the resulting number as a
           multiple of 2^&#39;bits&#39;. &quot;&quot;&quot;
    # Fill the integer if necessary.
    if a[2]==-1:
        a[2]=0
        while zero_or_one_exp_minus(a[3], a[4]) == 1:
            a[2]+=1
    if a[1] &gt; bits:
        # Shifting bits beyond the first excess bit.
        aa = a[0] &gt;&gt; (a[1] - bits - 1)
        # Check the excess bit; if odd, round up.
        ret=aa &gt;&gt; 1 if (aa &amp; 1) == 0 else (aa &gt;&gt; 1) + 1
        return ret|(a[2]&lt;&lt;bits)
    # Fill the fractional part if necessary.
    while a[1] &lt; bits:
       index = a[1]
       a[1]+=1
       a[0]=(a[0]&lt;&lt;1)|logisticexp(a[3], a[4], index+1)
    return a[0]|(a[2]&lt;&lt;bits)

def exprandless(a, b):
        &quot;&quot;&quot; Determines whether one partially-sampled exponential number
           is less than another; returns
           true if so and false otherwise.  During
           the comparison, additional bits will be sampled in both numbers
           if necessary for the comparison. &quot;&quot;&quot;
        # Check integer part of exponentials
        if a[2] == -1:
            a[2] = 0
            while zero_or_one_exp_minus(a[3], a[4]) == 1:
                a[2] += 1
        if b[2] == -1:
            b[2] = 0
            while zero_or_one_exp_minus(b[3], b[4]) == 1:
                b[2] += 1
        if a[2] &lt; b[2]:
            return True
        if a[2] &gt; b[2]:
            return False
        index = 0
        while True:
            # Fill with next bit in a&#39;s exponential number
            if a[1] &lt; index:
                raise ValueError
            if b[1] &lt; index:
                raise ValueError
            if a[1] &lt;= index:
                a[1] += 1
                a[0] = logisticexp(a[3], a[4], index + 1) | (a[0] &lt;&lt; 1)
            # Fill with next bit in b&#39;s exponential number
            if b[1] &lt;= index:
                b[1] += 1
                b[0] = logisticexp(b[3], b[4], index + 1) | (b[0] &lt;&lt; 1)
            aa = (a[0] &gt;&gt; (a[1] - 1 - index)) &amp; 1
            bb = (b[0] &gt;&gt; (b[1] - 1 - index)) &amp; 1
            if aa &lt; bb:
                return True
            if aa &gt; bb:
                return False
            index += 1

def zero_or_one(px, py):
        global bitcount
        &quot;&quot;&quot; Returns 1 at probability px/py, 0 otherwise.
            Uses Bernoulli algorithm from Lumbroso appendix 3,
            with one exception noted in this code. &quot;&quot;&quot;
        if py &lt;= 0:
            raise ValueError
        if px == py:
            return 1
        z = px
        while True:
            z = z * 2
            if z &gt;= py:
                bitcount+=1
                if random.randint(0,1) == 0:
                    return 1
                z = z - py
            # Exception: Condition added to help save bits
            elif z == 0: return 0
            else:
                bitcount+=1
                if random.randint(0,1) == 0:
                   return 0

def zero_or_one_exp_minus(x, y):
        &quot;&quot;&quot; Generates 1 with probability exp(-px/py); 0 otherwise.
               Reference: Canonne et al. 2020. &quot;&quot;&quot;
        if y &lt;= 0 or x &lt; 0:
            raise ValueError
        if x==0: return 1
        if x &gt; y:
            xf = int(x / y)  # Get integer part
            x = x % y  # Reduce to fraction
            if x &gt; 0 and zero_or_one_exp_minus(x, y) == 0:
                return 0
            for i in range(xf):
                if zero_or_one_exp_minus(1, 1) == 0:
                    return 0
            return 1
        r = 1
        ii = 1
        while True:
            if zero_or_one(x, y*ii) == 0:
                return r
            r=1-r
            ii += 1

# Example of use
def exprand(lam):
   return exprandfill(exprandnew(lam),53)*1.0/(1&lt;&lt;53)

</pre>

<p><a id=Beta_Sampler_Known_Issues></a></p>

<h3>Beta Sampler: Known Issues</h3>

<p>In the beta sampler, the bigger <code>alpha</code> or <code>beta</code> is, the smaller the area of acceptance becomes (and the more likely random numbers get rejected by this method, raising its run-time).  This is because <code>max(u^(alpha-1)*(1-u)^(beta-1))</code>, the peak of the density, approaches 0 as the parameters get bigger.  One idea to solve this issue is to expand the density so that the acceptance rate increases.  The following was tried:</p>

<ul>
<li>Estimate an upper bound for the peak of the density <code>peak</code>, given <code>alpha</code> and <code>beta</code>.</li>
<li>Calculate a largest factor <code>c</code> such that <code>peak * c = m &lt; 0.5</code>.</li>
<li>Use Huber&#39;s <code>linear_lowprob</code> Bernoulli factory (implemented in <em>bernoulli.py</em>) (Huber 2016)<sup><a href="#Note20"><strong>(20)</strong></a></sup>, taking the values found for <code>c</code> and <code>m</code>.  Testing shows that the choice of <code>m</code> is crucial for performance.</li>
</ul>

<p>But doing so apparently worsened the performance (in terms of random bits used) compared to the simple rejection approach.</p>

<p><a id=Exponential_Sampler_Extension></a></p>

<h3>Exponential Sampler: Extension</h3>

<p>The code above supports rational-valued &lambda; parameters.  It can be extended to support any real-valued &lambda; parameter greater than 0, as long as &lambda; can be rewritten as the sum of one or more components whose fractional parts can each be simulated by a Bernoulli factory algorithm that outputs heads with probability equal to that fractional part.<sup><a href="#Note21"><strong>(21)</strong></a></sup>.</p>

<p>More specifically:</p>

<ol>
<li>Decompose &lambda; into <em>n</em> &gt; 0 positive components that sum to &lambda;.  For example, if &lambda; = 3.5, it can be decomposed into only one component, 3.5 (whose fractional part is trivial to simulate), and if &lambda; = &pi;, it can be decomposed into four components that are all (&pi; / 4), which has a not-so-trivial simulation as a so-called <em>Machin machine</em> (as described by (Flajolet et al. 2010)<sup><a href="#Note7"><strong>(7)</strong></a></sup>).</li>
<li>For each component <em>LC</em>[<em>i</em>] found this way, let <em>LI</em>[<em>i</em>] be floor(<em>LC</em>[<em>i</em>]) and let <em>LF</em>[<em>i</em>] be <em>LC</em>[<em>i</em>] &minus; floor(<em>LC</em>[<em>i</em>]) (<em>LC</em>[<em>i</em>]&#39;s fractional part).</li>
</ol>

<p>The code above can then be modified as follows:</p>

<ul>
<li><p><code>exprandnew</code> is modified so that instead of taking <code>lamdanum</code> and <code>lamdaden</code>, it takes a list of the components described above.  Each component is stored as <em>LI</em>[<em>i</em>] and an algorithm that simulates <em>LF</em>[<em>i</em>].</p></li>
<li><p><code>zero_or_one_exp_minus(a, b)</code> is replaced with a Bernoulli factory, described below, that takes a component list as described above and outputs heads with probability exp(&minus;&lambda;).  It extends the <strong>ExpMinus Bernoulli factory</strong> as described, for example, in (Łatuszyński et al. 2011)<sup><a href="#Note22"><strong>(22)</strong></a></sup> or (Flajolet et al. 2010)<sup><a href="#Note7"><strong>(7)</strong></a></sup>. Here, the probability <code>exp(-x/y)</code> is rewritten as <code>exp(-LI[0]) * exp(-LF[0]) * ... * exp(-LI[n-1]) * exp(-LF[n-1])</code>.</p>

<ol>
<li>For each component <em>LC</em>[<em>i</em>], call <strong>ZeroOrOneExpMinus</strong> with <em>x</em> = <em>LI</em>[<em>i</em>] and <em>y</em> = 1, and call the <strong>ExpMinus Bernoulli factory</strong> with the Bernoulli generator that simulates <em>LF</em>[<em>i</em>].  Return 0 if any of these calls returns 0. (See also (Canonne et al. 2020)<sup><a href="#Note17"><strong>(17)</strong></a></sup>.)</li>
<li>Return 1.</li>
</ol></li>
<li><p><code>logisticexp(a, b, index+1)</code> is replaced with a modified <strong>LogisticExp</strong> algorithm described as follows.  Here, the probability <code>1/(1+exp(x/(y*pow(2, prec))))</code> is rewritten as 1/(1+exp(&Sigma;<sub><em>i</em></sub><em>LI</em>[<em>i</em>]/2<sup><em>prec</em></sup>) * &Pi;<sub><em>i</em></sub> exp(<em>LF</em>[<em>i</em>]/2<sup><em>prec</em></sup>) ).</p>

<ol>
<li>For each component <em>LC</em>[<em>i</em>], create a Bernoulli generator that uses the following algorithm: (a) With probability 1/(2<sup><em>prec</em></sup>), return 1 if the algorithm that simulates <em>LF</em>[<em>i</em>] returns 1; (b) Return 0.</li>
<li>Return 0 with probability 1/2.</li>
<li>Call <strong>ZeroOrOneExpMinus</strong> with <em>x</em> = &Sigma;<sub><em>i</em></sub> <em>LI</em>[<em>i</em>] and <em>y</em> = 2<sup><em>prec</em></sup>.  If this call returns 0, go to step 2.</li>
<li>For each component <em>LC</em>[<em>i</em>], call the <strong>ExpMinus Bernoulli factory</strong> with the Bernoulli generator for that component described in step 1.  Go to step 2 if any of these calls returns 0.</li>
<li>Return 1.</li>
</ol></li>
</ul>

<p><a id=Correctness_Testing></a></p>

<h2>Correctness Testing</h2>

<p><a id=Beta_Sampler></a></p>

<h3>Beta Sampler</h3>

<p>To test the correctness of the beta sampler presented in this document, the Kolmogorov&ndash;Smirnov test was applied with various values of <code>alpha</code> and <code>beta</code> and the default precision of 53, using SciPy&#39;s <code>kstest</code> method.  The code for the test is very simple: <code>kst = scipy.stats.kstest(ksample, lambda x: scipy.stats.beta.cdf(x, alpha, beta))</code>, where <code>ksample</code> is a sample of random numbers generated using the sampler above.  Note that SciPy uses a two-sided Kolmogorov&ndash;Smirnov test by default.</p>

<p>See the results of the <a href="https://peteroupc.github.io/betadistresults.html"><strong>correctness testing</strong></a>.   For each pair of parameters, five samples with 50,000 numbers per sample were taken, and results show the lowest and highest Kolmogorov&ndash;Smirnov statistics and p-values achieved for the five samples.  Note that a p-value extremely close to 0 or 1 strongly indicates that the samples do not come from the corresponding beta distribution.</p>

<p><a id=ExpRandFill></a></p>

<h3>ExpRandFill</h3>

<p>To test the correctness of the <code>exprandfill</code> method (which implements the <strong>ExpRandFill</strong> algorithm), the Kolmogorov&ndash;Smirnov test was applied with various values of &lambda; and the default precision of 53, using SciPy&#39;s <code>kstest</code> method.  The code for the test is very simple: <code>kst = scipy.stats.kstest(ksample, lambda x: scipy.stats.expon.cdf(x, scale=1/lamda))</code>, where <code>ksample</code> is a sample of random numbers generated using the <code>exprand</code> method above.  Note that SciPy uses a two-sided Kolmogorov&ndash;Smirnov test by default.</p>

<p>The table below shows the results of the correctness testing. For each parameter, five samples with 50,000 numbers per sample were taken, and results show the lowest and highest Kolmogorov&ndash;Smirnov statistics and p-values achieved for the five samples.  Note that a p-value extremely close to 0 or 1 strongly indicates that the samples do not come from the corresponding exponential distribution.</p>

<table><thead>
<tr>
<th>&lambda;</th>
<th>Statistic</th>
<th><em>p</em>-value</th>
</tr>
</thead><tbody>
<tr>
<td>1/10</td>
<td>0.00233-0.00435</td>
<td>0.29954-0.94867</td>
</tr>
<tr>
<td>1/4</td>
<td>0.00254-0.00738</td>
<td>0.00864-0.90282</td>
</tr>
<tr>
<td>1/2</td>
<td>0.00195-0.00521</td>
<td>0.13238-0.99139</td>
</tr>
<tr>
<td>2/3</td>
<td>0.00295-0.00457</td>
<td>0.24659-0.77715</td>
</tr>
<tr>
<td>3/4</td>
<td>0.00190-0.00636</td>
<td>0.03514-0.99381</td>
</tr>
<tr>
<td>9/10</td>
<td>0.00226-0.00474</td>
<td>0.21032-0.96029</td>
</tr>
<tr>
<td>1</td>
<td>0.00267-0.00601</td>
<td>0.05389-0.86676</td>
</tr>
<tr>
<td>2</td>
<td>0.00293-0.00684</td>
<td>0.01870-0.78310</td>
</tr>
<tr>
<td>3</td>
<td>0.00284-0.00675</td>
<td>0.02091-0.81589</td>
</tr>
<tr>
<td>5</td>
<td>0.00256-0.00546</td>
<td>0.10130-0.89935</td>
</tr>
<tr>
<td>10</td>
<td>0.00279-0.00528</td>
<td>0.12358-0.82974</td>
</tr>
</tbody></table>

<p><a id=ExpRandLess></a></p>

<h3>ExpRandLess</h3>

<p>To test the correctness of <code>exprandless</code>, a two-independent-sample T-test was applied to scores involving e-rands and scores involving the Python <code>random.expovariate</code> method.  Specifically, the score is calculated as the number of times one exponential number compares as less than another; for the same &lambda; this event should ideally be as likely as the event that it compares as greater.  The Python code that follows the table calculates this score for e-rands and <code>expovariate</code>.   Even here, the code for the test is very simple: <code>kst = scipy.stats.ttest_ind(exppyscores, exprandscores)</code>, where <code>exppyscores</code> and <code>exprandscores</code> are each lists of 20 results from <code>exppyscore</code> or <code>exprandscore</code>, respectively, and the results contained in <code>exppyscores</code> and <code>exprandscores</code> were generated independently of each other.</p>

<p>The table below shows the results of the correctness testing. For each pair of parameters, results show the lowest and highest T-test statistics and p-values achieved for the 20 results.  Note that a p-value extremely close to 0 or 1 strongly indicates that exponential random numbers are not compared as less or greater with the expected probability.</p>

<table><thead>
<tr>
<th>Left &lambda;</th>
<th>Right &lambda;</th>
<th>Statistic</th>
<th><em>p</em>-value</th>
</tr>
</thead><tbody>
<tr>
<td>1/10</td>
<td>1/10</td>
<td>-1.21015 &ndash; 0.93682</td>
<td>0.23369 &ndash; 0.75610</td>
</tr>
<tr>
<td>1/10</td>
<td>1/2</td>
<td>-1.25248 &ndash; 3.56291</td>
<td>0.00101 &ndash; 0.39963</td>
</tr>
<tr>
<td>1/10</td>
<td>1</td>
<td>-0.76586 &ndash; 1.07628</td>
<td>0.28859 &ndash; 0.94709</td>
</tr>
<tr>
<td>1/10</td>
<td>2</td>
<td>-1.80624 &ndash; 1.58347</td>
<td>0.07881 &ndash; 0.90802</td>
</tr>
<tr>
<td>1/10</td>
<td>5</td>
<td>-0.16197 &ndash; 1.78700</td>
<td>0.08192 &ndash; 0.87219</td>
</tr>
<tr>
<td>1/2</td>
<td>1/10</td>
<td>-1.46973 &ndash; 1.40308</td>
<td>0.14987 &ndash; 0.74549</td>
</tr>
<tr>
<td>1/2</td>
<td>1/2</td>
<td>-0.79555 &ndash; 1.21538</td>
<td>0.23172 &ndash; 0.93613</td>
</tr>
<tr>
<td>1/2</td>
<td>1</td>
<td>-0.90496 &ndash; 0.11113</td>
<td>0.37119 &ndash; 0.91210</td>
</tr>
<tr>
<td>1/2</td>
<td>2</td>
<td>-1.32157 &ndash; -0.07066</td>
<td>0.19421 &ndash; 0.94404</td>
</tr>
<tr>
<td>1/2</td>
<td>5</td>
<td>-0.55135 &ndash; 1.85604</td>
<td>0.07122 &ndash; 0.76994</td>
</tr>
<tr>
<td>1</td>
<td>1/10</td>
<td>-1.27023 &ndash; 0.73501</td>
<td>0.21173 &ndash; 0.87314</td>
</tr>
<tr>
<td>1</td>
<td>1/2</td>
<td>-2.33246 &ndash; 0.66827</td>
<td>0.02507 &ndash; 0.58741</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>-1.24446 &ndash; 0.84555</td>
<td>0.22095 &ndash; 0.90587</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>-1.13643 &ndash; 0.84148</td>
<td>0.26289 &ndash; 0.95717</td>
</tr>
<tr>
<td>1</td>
<td>5</td>
<td>-0.70037 &ndash; 1.46778</td>
<td>0.15039 &ndash; 0.86996</td>
</tr>
<tr>
<td>2</td>
<td>1/10</td>
<td>-0.77675 &ndash; 1.15350</td>
<td>0.25591 &ndash; 0.97870</td>
</tr>
<tr>
<td>2</td>
<td>1/2</td>
<td>-0.23122 &ndash; 1.20764</td>
<td>0.23465 &ndash; 0.91855</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>-0.92273 &ndash; -0.05904</td>
<td>0.36197 &ndash; 0.95323</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>-1.88150 &ndash; 0.64096</td>
<td>0.06758 &ndash; 0.73056</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>-0.08315 &ndash; 1.01951</td>
<td>0.31441 &ndash; 0.93417</td>
</tr>
<tr>
<td>5</td>
<td>1/10</td>
<td>-0.60921 &ndash; 1.54606</td>
<td>0.13038 &ndash; 0.91563</td>
</tr>
<tr>
<td>5</td>
<td>1/2</td>
<td>-1.30038 &ndash; 1.43602</td>
<td>0.15918 &ndash; 0.86349</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>-1.22803 &ndash; 1.35380</td>
<td>0.18380 &ndash; 0.64158</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>-1.83124 &ndash; 1.40222</td>
<td>0.07491 &ndash; 0.66075</td>
</tr>
<tr>
<td>5</td>
<td>5</td>
<td>-0.97110 &ndash; 2.00904</td>
<td>0.05168 &ndash; 0.74398</td>
</tr>
</tbody></table>

<pre>def exppyscore(ln,ld,ln2,ld2):
        return sum(1 if random.expovariate(ln*1.0/ld)&lt;random.expovariate(ln2*1.0/ld2) \
              else 0 for i in range(1000))

def exprandscore(ln,ld,ln2,ld2):
        return sum(1 if exprandless(exprandnew(ln,ld), exprandnew(ln2,ld2)) \
              else 0 for i in range(1000))
</pre>

<p><a id=Accurate_Simulation_of_Continuous_Distributions_on_0_1></a></p>

<h2>Accurate Simulation of Continuous Distributions on [0, 1]</h2>

<p>The beta sampler in this document shows one case of a general approach to simulating a wide class of continuous distributions supported on [0, 1], thanks to Bernoulli factories.  This general approach can sample a number that follows one of these distributions, using the algorithm below.  The algorithm allows any arbitrary radix <em>b</em> (such as 2 for binary).</p>

<ol>
<li>Create an &quot;empty&quot; partially-sampled uniform random number (or &quot;geometric bag&quot;).  Create a <strong>SampleGeometricBag</strong> Bernoulli factory that uses that geometric bag.</li>
<li><p>As the geometric bag builds up a uniform random number, accept the number with a probability that can be represented by a Bernoulli factory (that takes the <strong>SampleGeometricBag</strong> factory from step 1 as part of its input), or reject it otherwise.  Let <em>f</em>(<em>U</em>) be the probability function modeled by this Bernoulli factory, where <em>U</em> is the uniform random number built up by the geometric bag. <em>f</em> is a multiple of the density function for the underlying continuous distribution (as a result, this algorithm can be used even if the distribution&#39;s density function is only known up to a normalization constant).  As shown by Keane and O&#39;Brien <sup><a href="#Note6"><strong>(6)</strong></a></sup>, however, this step works if and only if <em>f</em>, in the interval [0, 1]&mdash;</p>

<ul>
<li>is continuous everywhere, and</li>
<li>either returns a constant value in [0, 1] everywhere, or returns a value in [0, 1] at each of the points 0 and 1 and a value in (0, 1) at each other point,</li>
</ul>

<p>and they give the example of 2<em>p</em> as a probability function that cannot be represented by a Bernoulli factory.  In the case of constants, the Bernoulli factory can represent them by a geometric bag&mdash;</p>

<ul>
<li>that is prefilled with the digit expansion of the constant in question, or</li>
<li>that uses a modified <strong>SampleGeometricBag</strong> algorithm in which the constant&#39;s digit expansion&#39;s digits are not sampled at random, but rather calculated &quot;on the fly&quot; and as necessary.</li>
</ul></li>
<li><p>If the geometric bag is accepted, either return the bag as is or fill the unsampled digits of the bag with uniform random digits as necessary to give the number an <em>n</em>-digit fractional part (similarly to <strong>FillGeometricBag</strong> above), where <em>n</em> is a precision parameter.</p></li>
</ol>

<p>The beta distribution&#39;s probability function at (1) fits the requirements of Keane and O&#39;Brien (for <code>alpha</code> and <code>beta</code> both greater than 1), thus it can be simulated by Bernoulli factories and is covered by this general approach.</p>

<p>This algorithm can be modified to produce random numbers in the interval [<em>m</em>, <em>m</em> + <em>b</em><sup><em>i</em></sup>] (where <em>b</em> is the radix and <em>i</em> and <em>m</em> are integers), rather than [0, 1], as follows:</p>

<ol>
<li>Apply the algorithm above, except a modified probability function <em>f&prime;</em>(<em>x</em>) = <em>f</em>(<em>x</em> * <em>b</em><sup><em>i</em></sup> + <em>m</em>) is used rather than <em>f</em>.</li>
<li>Multiply the resulting random number or geometric bag by <em>b</em><sup><em>i</em></sup>, then add <em>m</em> (this step is relatively trivial given that the geometric bag stores a base-<em>b</em> fractional part).</li>
<li>If the random number (rather than its geometric bag) will be returned, and the number&#39;s fractional part now has fewer than <em>n</em> digits due to step 2, re-fill the number as necessary to give the fractional part <em>n</em> digits.</li>
</ol>

<p>Note that here, the probability function <em>f&prime;</em> must meet the requirements of Keane and O&#39;Brien.  (For example, take the probability function <code>sqrt((x - 4) / 2)</code>, which isn&#39;t a Bernoulli factory function.  If we now seek to sample from the interval [4, 4+2<sup>1</sup>] = [4, 6], the <em>f</em> used in step 2 is now <code>sqrt(x)</code>, which <em>is</em> a Bernoulli factory function so that we can apply this algorithm.)</p>

<p>On the other hand, modifying this algorithm to produce random numbers in any other interval is non-trivial, since it often requires relating digit probabilities to some kind of formula (see &quot;About Partially-Sampled Random Numbers&quot;, above).</p>

<p><a id=An_Example_The_Continuous_Bernoulli_Distribution></a></p>

<h3>An Example: The Continuous Bernoulli Distribution</h3>

<p>The continuous Bernoulli distribution (Loaiza-Ganem and Cunningham 2019)<sup><a href="#Note23"><strong>(23)</strong></a></sup> was designed to considerably improve performance of variational autoencoders (a machine learning model) in modeling continuous data that takes values in the interval [0, 1], including &quot;almost-binary&quot; image data.</p>

<p>The continous Bernoulli distribution takes one parameter <code>lamda</code> (a number in [0, 1]), and takes on values in the interval [0, 1] with a probability proportional to&mdash;</p>

<pre>pow(lamda, x) * pow(1 - lamda, 1 - x).
</pre>

<p>Again, this function meets the requirements stated by Keane and O&#39;Brien, so it can be simulated via Bernoulli factories.  Thus, this distribution can be simulated in Python using a geometric bag (which represents <em>x</em> in the formula above) and a two-coin Bernoulli factory described below.</p>

<p>The <strong>two-coin power factory</strong> has the following algorithm.  It is based on the <strong>PowerBernoulliFactory</strong> given earlier (including the algorithm from (Mendo 2019)<sup><a href="#Note15"><strong>(15)</strong></a></sup>), but changed to accept a second Bernoulli factory sub-algorithm rather than a fixed value for the exponent. To the best of my knowledge, I am not aware of any other article or paper that presents this particular Bernoulli factory.</p>

<ol>
<li>Set <em>i</em> to 1.</li>
<li>Call the base sub-algorithm; if it returns 1, return 1.</li>
<li>Call the exponent sub-algorithm; if it returns 1, return 0 with probability 1/<em>i</em>.</li>
<li>Add 1 to <em>i</em> and go to step 1.</li>
</ol>

<p>The algorithm for sampling the continuous Bernoulli distribution follows.  It uses a <strong>lambda Bernoulli factory</strong> algorithm, which returns 1 with probability <code>lamda</code>.</p>

<ol>
<li>Create an empty list to serve as a &quot;geometric bag&quot;.</li>
<li>Create a <strong>complementary lambda Bernoulli factory</strong> that returns 1 minus the result of the <strong>lambda Bernoulli factory</strong>.</li>
<li>Remove all digits from the geometric bag.  This will result in an empty uniform random number, <em>U</em>, for the following steps, which will accept <em>U</em> with probability <code>lamda</code><sup><em>U</em></sup>*(1&minus;<code>lamda</code>)<sup>1&minus;<em>U</em></sup>) (the proportional probability for the beta distribution), as <em>U</em> is built up.</li>
<li>Call the <strong>two-coin power factory</strong> using the <strong>lambda Bernoulli factory</strong> as the base and <strong>SampleGeometricBag</strong> as the exponent (which will return 1 with probability <code>lamda</code><sup><em>U</em></sup>).  If the result is 0, go to step 3.</li>
<li>Call the <strong>two-coin power factory</strong> using the <strong>complementary lambda Bernoulli factory</strong> as the base and <strong>SampleGeometricBagComplement</strong> algorithm and parameter <em>b</em> &minus; 1 (which will return 1 with probability (1-<code>lamda</code>)<sup>1&minus;<em>U</em></sup>).  If the result is 0, go to step 3. (Note that steps 4 and 5 don&#39;t depend on each other and can be done in either order without affecting correctness.)</li>
<li><em>U</em> was accepted, so return the result of <strong>FillGeometricBag</strong>.</li>
</ol>

<p>The Python code that samples the continuous Bernoulli distribution follows.</p>

<pre>def _twofacpower(b, fbase, fexponent):
    &quot;&quot;&quot; Bernoulli factory B(p, q) =&gt; B(p^q).
           - fbase, fexponent: Functions that return 1 if heads and 0 if tails.
             The first is the base, the second is the exponent.
             &quot;&quot;&quot;
    i = 1
    while True:
        if fbase() == 1:
            return 1
        if fexponent() == 1 and \
            b.zero_or_one(1, i) == 1:
            return 0
        i = i + 1

def contbernoullidist(b, lamda, precision=53):
    # Continuous Bernoulli distribution
    bag=[]
    lamda=Fraction(lamda)
    gb=lambda: b.geometric_bag(bag)
    # Complement of &quot;geometric bag&quot;
    gbcomp=lambda: b.geometric_bag(bag)^1
    fcoin=b.coin(lamda)
    lamdab=lambda: fcoin()
    # Complement of &quot;lambda coin&quot;
    lamdabcomp=lambda: fcoin()^1
    acc=0
    while True:
       # Create a uniform random number (U) bit-by-bit, and
       # accept it with probability lamda^U*(1-lamda)^(1-U), which
       # is the unnormalized PDF of the beta distribution
       bag.clear()
       # Produce 1 with probability lamda^U
       r=_twofacpower(b, lamdab, gb)
       # Produce 1 with probability (1-lamda)^(1-U)
       if r==1: r=_twofacpower(b, lamdabcomp, gbcomp)
       if r == 1:
             # Accepted, so fill up the &quot;bag&quot; and return the
             # uniform number
             ret=_fill_geometric_bag(b, bag, precision)
             return ret
       acc+=1
</pre>

<p><a id=Complexity></a></p>

<h2>Complexity</h2>

<p>The <em>bit complexity</em> of an algorithm that generates random numbers is measured as the number of random bits that algorithm uses on average.</p>

<p><a id=General_Principles></a></p>

<h3>General Principles</h3>

<p>Existing work shows how to calculate the bit complexity for any distribution of random numbers:</p>

<ul>
<li>For a 1-dimensional continuous distribution, the bit complexity is bounded from below by <code>DE + prec - 1</code> random bits, where <code>DE</code> is the differential entropy for the distribution and <em>prec</em> is the number of bits in the random number&#39;s fractional part (Devroye and Gravel 2015)<sup><a href="#Note3"><strong>(3)</strong></a></sup>.</li>
<li>For a discrete distribution (a distribution of random integers with separate probabilities of occurring), the bit complexity is bounded from below by the binary entropies of all the probabilities involved, summed together (Knuth and Yao 1976)<sup><a href="#Note24"><strong>(24)</strong></a></sup>.  (For a given probability <em>p</em>, the binary entropy is <code>p*log2(1/p)</code>.)  An optimal algorithm will come within 2 bits of this lower bound on average.</li>
</ul>

<p>For example, in the case of the exponential distribution, <code>DE</code> is log2(exp(1)/&lambda;), so the minimum bit complexity for this distribution is log2(exp(1)/&lambda;) + <em>prec</em> &minus; 1, so that if <em>prec</em> = 20, this minimum is about 20.443 bits when &lambda; = 1, decreases when &lambda; goes up, and increases when &lambda; goes down.  In the case of any other continuous distribution, <code>DE</code> is the integral of <code>f(x) * log2(1/f(x))</code> over all valid values <code>x</code>, where <code>f</code> is the distribution&#39;s density function.</p>

<p>Although existing work shows lower bounds on the number of random bits an algorithm will need on average, most algorithms will generally not achieve these lower bounds in practice.</p>

<p>In general, if an algorithm calls other algorithms that generate random numbers, the total expected bit complexity is&mdash;</p>

<ul>
<li>the expected number of calls to each of those other algorithms, times</li>
<li>the bit complexity for each such call.</li>
</ul>

<p><a id=Complexity_of_Specific_Algorithms></a></p>

<h3>Complexity of Specific Algorithms</h3>

<p>The beta and exponential samplers given here will generally use many more bits on average than the lower bounds on bit complexity, especially since they generate a partially-sampled random number one digit at a time.</p>

<p>The <code>zero_or_one</code> method generally uses 2 random bits on average, due to its nature as a Bernoulli trial involving random bits, see also (Lumbroso 2013, Appendix B)<sup><a href="#Note19"><strong>(19)</strong></a></sup>.  However, it uses no random bits if both its parameters are the same.</p>

<p>For <strong>SampleGeometricBag</strong> with base 2, the bit complexity has two components.</p>

<ul>
<li>One component comes from sampling a geometric (1/2) random number, as follows:

<ul>
<li>Optimal lower bound: Since the binary entropy of the random number is 2, the optimal lower bound is 2 bits.</li>
<li>Optimal upper bound: 4 bits.</li>
</ul></li>
<li>The other component comes from filling the geometric bag with random bits.  The complexity here depends on the number of times <strong>SampleGeometricBag</strong> is called for the same bag, call it <code>n</code>.  Then the expected number of bits is the expected number of bit positions filled this way after <code>n</code> calls.</li>
</ul>

<p><strong>SampleGeometricBagComplement</strong> has the same bit complexity as <strong>SampleGeometricBag</strong>.</p>

<p><strong>FillGeometricBag</strong>&#39;s bit complexity is rather easy to find.  For base 2, it uses only one bit to sample each unfilled digit at positions less than <code>p</code>. (For bases other than 2, sampling <em>each</em> digit this way might not be optimal, since the digits are generated one at a time and random bits are not recycled over several digits.)  As a result, for an algorithm that uses both <strong>SampleGeometricBag</strong> and <strong>FillGeometricBag</strong> with <code>p</code> bits, these two contribute, on average, anywhere from <code>p + g * 2</code> to <code>p + g * 4</code> bits to the complexity, where <code>g</code> is the number of calls to <strong>SampleGeometricBag</strong>. (This complexity could be increased by 1 bit if <strong>FillGeometricBag</strong> is implemented with a rounding mechanism other than simple truncation.)</p>

<p>The complexity of <strong>ZeroOrOneExpMinus</strong> (which outputs 1 with probability exp(&minus;<em>x</em>/<em>y</em>)) was discussed in some detail by (Canonne et al. 2020)<sup><a href="#Note17"><strong>(17)</strong></a></sup>, but not in terms of its bit complexity.  The special case of &gamma; =<em>x</em>/<em>y</em> = 0 requires no bits.  If &gamma; is an integer greater than 1, then the bit complexity is the same as that of sampling a geometric(exp(&minus;1)) random number, but truncated to [0, &gamma;]. (In this document, the geometric(<code>n</code>) distribution has the density function <code>pow(x, n) * (1 - x)</code>.)</p>

<ul>
<li>Optimal lower bound: Has a complicated formula for general &gamma;, but approaches <code>log2(exp(1)-(exp(1)+1)*ln(exp(1)-1))</code> = 2.579730853... bits with increasing &gamma;.</li>
<li>Optimal upper bound: Optimal lower bound plus 2.</li>
<li>The actual implementation&#39;s average bit complexity is generally&mdash;

<ul>
<li>the expected number of calls to <strong>ZeroOrOneExpMinus</strong> (with &gamma; = 1), which is the expected value of the truncated geometric distribution described above, times</li>
<li>the bit complexity for each such call.</li>
</ul></li>
</ul>

<p>If &gamma; is 1 or less, the optimal bit complexity is determined as the complexity of sampling a random integer <em>k</em> with probability function&mdash;</p>

<ul>
<li>P(<em>k</em>) = &gamma;<sup><em>k</em></sup>/<em>k</em>! &minus; &gamma;<sup><em>k</em> + 1</sup>/(<em>k</em> + 1)!,</li>
</ul>

<p>and the optimal lower bound is found by taking the binary entropy of each probability (<code>P(k)/log2(1/P(k))</code>) and summing them all.</p>

<ul>
<li>Optimal lower bound: Again, this has a complicated formula (see the appendix for SymPy code), but it appears to be highest at about 1.85 bits, which is reached when &gamma; is about 0.848.</li>
<li>Optimal upper bound: Optimal lower bound plus 2.</li>
<li>The actual implementation&#39;s average bit complexity is generally&mdash;

<ul>
<li>the expected number of calls to <code>zero_or_one</code>, which was determined to be exp(&gamma;) in (Canonne et al. 2020)<sup><a href="#Note17"><strong>(17)</strong></a></sup>, times</li>
<li>the bit complexity for each such call (which is generally 2, but is lower in the case of &gamma; = 1, which involves <code>zero_or_one(1, 1)</code> that uses no random bits).</li>
</ul></li>
</ul>

<p>If &gamma; is a non-integer greater than 1, the bit complexity is the sum of the bit complexities for its integer part and for its fractional part.</p>

<p><a id=Application_to_Weighted_Reservoir_Sampling></a></p>

<h2>Application to Weighted Reservoir Sampling</h2>

<p><a href="https://peteroupc.github.io/randomfunc.html#Weighted_Choice_Without_Replacement_List_of_Unknown_Size"><strong>Weighted reservoir sampling</strong></a> (choosing an item at random from a list of unknown size) is often implemented by&mdash;</p>

<ul>
<li>assigning each item a <em>weight</em> (an integer 0 or greater) as it&#39;s encountered, call it <em>w</em>,</li>
<li>giving each item an exponential random number with &lambda; = <em>w</em>, call it a key, and</li>
<li>choosing the item with the smallest key</li>
</ul>

<p>(see also (Efraimidis 2015)<sup><a href="#Note25"><strong>(25)</strong></a></sup>). However, using fully-sampled exponential random numbers as keys (such as the naïve idiom <code>-ln(1-RNDU01())/w</code> in common floating-point arithmetic) can lead to inexact sampling, since the keys have a limited precision, it&#39;s possible for multiple items to have the same random key (which can make sampling those items depend on their order rather than on randomness), and the maximum weight is unknown.  Partially-sampled e-rands, as given in this document, eliminate the problem of inexact sampling.  This is notably because the <code>exprandless</code> method returns one of only two answers&mdash;either &quot;less&quot; or &quot;greater&quot;&mdash;and samples from both e-rands as necessary so that they will differ from each other by the end of the operation.  (This is not a problem because randomly generated real numbers are expected to differ from each other almost surely.) Another reason is that partially-sampled e-rands have potentially arbitrary precision.</p>

<p><a id=Open_Questions></a></p>

<h2>Open Questions</h2>

<p>There are some open questions on partially-sampled random numbers:</p>

<ol>
<li>Are there constructions for partially-sampled normal random numbers with a standard deviation other than 1 and/or a mean other than an integer?</li>
<li>Are there constructions for partially-sampled random numbers other than for cases given earlier in this document?</li>
<li>What are exact formulas for the digit probabilities when arithmetic is carried out between two partially-sampled random numbers (such as addition, multiplication, division, and powering)?</li>
</ol>

<p><a id=Acknowledgments></a></p>

<h2>Acknowledgments</h2>

<p>I acknowledge Claude Gravel who reviewed a previous version of this article.</p>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<p><small><sup id=Note1>(1)</sup> Karney, C.F.F., &quot;<a href="https://arxiv.org/abs/1303.6257v2"><strong>Sampling exactly from the normal distribution</strong></a>&quot;, arXiv:1303.6257v2  [physics.comp-ph], 2014.</small></p>

<p><small><sup id=Note2>(2)</sup> Philippe Flajolet, Nasser Saheb. The complexity of generating an exponentially distributed variate. [Research Report] RR-0159, INRIA. 1982. inria-00076400.</small></p>

<p><small><sup id=Note3>(3)</sup> Devroye, L., Gravel, C., &quot;<a href="https://arxiv.org/abs/1502.02539v5"><strong>Sampling with arbitrary precision</strong></a>&quot;, arXiv:1502.02539v5 [cs.IT], 2015.</small></p>

<p><small><sup id=Note4>(4)</sup> Thomas, D.B. and Luk, W., 2008, September. Sampling from the exponential distribution using independent bernoulli variates. In 2008 International Conference on Field Programmable Logic and Applications (pp. 239-244). IEEE.</small></p>

<p><small><sup id=Note5>(5)</sup> A. Habibizad Navin, R. Olfatkhah and M. K. Mirnia, &quot;A data-oriented model of exponential random variable,&quot; 2010 2nd International Conference on Advanced Computer Control, Shenyang, 2010, pp. 603-607, doi: 10.1109/ICACC.2010.5487128.</small></p>

<p><small><sup id=Note6>(6)</sup> Keane,  M.  S.,  and  O&#39;Brien,  G.  L., &quot;A Bernoulli factory&quot;, <em>ACM Transactions on Modeling and Computer Simulation</em> 4(2), 1994.</small></p>

<p><small><sup id=Note7>(7)</sup> Flajolet, P., Pelletier, M., Soria, M., &quot;<a href="https://arxiv.org/abs/0906.5560v2"><strong>On Buffon machines and numbers</strong></a>&quot;, arXiv:0906.5560v2  [math.PR], 2010.</small></p>

<p><small><sup id=Note8>(8)</sup> Pedersen, K., &quot;<a href="https://arxiv.org/abs/1704.07949v3"><strong>Reconditioning your quantile function</strong></a>&quot;, arXiv:1704.07949v3 [stat.CO], 2018.</small></p>

<p><small><sup id=Note9>(9)</sup> von Neumann, J., &quot;Various techniques used in connection with random digits&quot;, 1951.</small></p>

<p><small><sup id=Note10>(10)</sup> Oberhoff, Sebastian, &quot;<a href="https://dc.uwm.edu/etd/1888"><strong>Exact Sampling and Prefix Distributions</strong></a>&quot;, <em>Theses and Dissertations</em>, University of Wisconsin Milwaukee, 2018.</small></p>

<p><small><sup id=Note11>(11)</sup> Brassard, G., Devroye, L., Gravel, C., &quot;Remote Sampling with Applications to General Entanglement Simulation&quot;, <em>Entropy</em> 2019(21)(92), doi:10.3390/e21010092.</small></p>

<p><small><sup id=Note12>(12)</sup> A. Habibizad Navin, Fesharaki, M.N., Teshnelab, M. and Mirnia, M., 2007. &quot;Data oriented modeling of uniform random variable: Applied approach&quot;. <em>World Academy Science Engineering Technology</em>, 21, pp.382-385.</small></p>

<p><small><sup id=Note13>(13)</sup> Nezhad, R.F., Effatparvar, M., Rahimzadeh, M., 2013. &quot;Designing a Universal Data-Oriented Random Number Generator&quot;, <em>International Journal of Modern Education and Computer Science</em> 2013(2), pp. 19-24.</small></p>

<p><small><sup id=Note14>(14)</sup> Rohatgi, V.K., 1976. An Introduction to Probability Theory Mathematical Statistics.</small></p>

<p><small><sup id=Note15>(15)</sup> Mendo, Luis. &quot;An asymptotically optimal Bernoulli factory for certain functions that can be expressed as power series.&quot; Stochastic Processes and their Applications 129, no. 11 (2019): 4366-4384.</small></p>

<p><small><sup id=Note16>(16)</sup> Devroye, L., <a href="http://luc.devroye.org/rnbookindex.html"><strong><em>Non-Uniform Random Variate Generation</em></strong></a>, 1986.</small></p>

<p><small><sup id=Note17>(17)</sup> Canonne, C., Kamath, G., Steinke, T., &quot;<a href="https://arxiv.org/abs/2004.00010v2"><strong>The Discrete Gaussian for Differential Privacy</strong></a>&quot;, arXiv:2004.00010v2 [cs.DS], 2020.</small></p>

<p><small><sup id=Note18>(18)</sup> Morina, G., Łatuszyński, K., et al., &quot;From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains&quot;, 2019.</small></p>

<p><small><sup id=Note19>(19)</sup> Lumbroso, J., &quot;<a href="https://arxiv.org/abs/1304.1916"><strong>Optimal Discrete Uniform Generation from Coin Flips, and Applications</strong></a>&quot;, arXiv:1304.1916 [cs.DS].</small></p>

<p><small><sup id=Note20>(20)</sup> Huber, M., &quot;<a href="https://arxiv.org/abs/1507.00843v2"><strong>Optimal linear Bernoulli factories for small mean problems</strong></a>&quot;, arXiv:1507.00843v2 [math.PR], 2016</small></p>

<p><small><sup id=Note21>(21)</sup> In fact, thanks to the &quot;geometric bag&quot; technique of Flajolet et al. (2010), that fractional part can even be a uniform random number in [0, 1] whose contents are built up digit by digit.</small></p>

<p><small><sup id=Note22>(22)</sup> Łatuszyński, K., Kosmidis, I.,  Papaspiliopoulos, O., Roberts, G.O., &quot;Simulating events of unknown probabilities via reverse time martingales&quot;, 2011.</small></p>

<p><small><sup id=Note23>(23)</sup> Loaiza-Ganem, G., Cunningham, J.P., &quot;<a href="https://arxiv.org/abs/1907.06845v5"><strong>The continuous Bernoulli: fixing a pervasive error in variational autoencoders</strong></a>&quot;, arXiv:1907.06845v5  [stat.ML], 2019.</small></p>

<p><small><sup id=Note24>(24)</sup> Knuth, Donald E. and Andrew Chi-Chih Yao. &quot;The complexity of nonuniform random number generation&quot;, in <em>Algorithms and Complexity: New Directions and Recent Results</em>, 1976.</small></p>

<p><small><sup id=Note25>(25)</sup> Efraimidis, P. &quot;<a href="https://arxiv.org/abs/1012.0256v2"><strong>Weighted Random Sampling over Data Streams</strong></a>&quot;, arXiv:1012.0256v2 [cs.DS], 2015.</small></p>

<p><a id=Appendix></a></p>

<h2>Appendix</h2>

<p>The following Python code uses SymPy to plot the bit complexity lower bound for <strong>ZeroToOneExpMinus</strong> when &gamma; is 1 or less:</p>

<pre>def ent(p):
   return p*log(1/p,2)

def expminusformula():
   i=symbols(&#39;i&#39;,integer=True)
   x=symbols(&#39;x&#39;,real=True)
   # Approximation for k = [0, 6]; the result is little different
   # for k = [0, infinity]
   return summation(ent(x**i/factorial(i) - \
      x**(i+1)/factorial(i+1)), (i,0,6))

plot(expminusformula(), xlim=(0,1), ylim=(0,2))
</pre>

<p><a id=License></a></p>

<h2>License</h2>

<p>Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under <a href="https://creativecommons.org/publicdomain/zero/1.0/"><strong>Creative Commons Zero</strong></a>.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>
<p>
If you like this software, you should consider donating to me, Peter O., at the link below:</p>
<p class="printonly"><b>peteroupc.github.io</b></p>
<div class="noprint">
<a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=56E5T4FH7KD2S">
<img src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif"
name="submit" border="2" alt="PayPal - The safer, easier way to pay online!"></a>
<p>
<a href="//twitter.com/share">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
if("share" in navigator){
 document.getElementById("sharer").href="javascript:void(null)";
 document.getElementById("sharer").innerHTML="Share This Page";
 navigator.share({title:document.title,url:document.location.href}).then(
   function(){});
} else {
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
}
</script>
</body></html>
