<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>Randomized Estimation Algorithms</title><meta name="citation_title" content="Randomized Estimation Algorithms"><meta name="citation_pdf_url" content="https://peteroupc.github.io/estimation.pdf"><meta name="citation_url" content="https://peteroupc.github.io/estimation.html"><meta name="citation_date" content="2021/05/01"><meta name="citation_online_date" content="2021/05/01"><meta name="og:title" content="Randomized Estimation Algorithms"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/estimation.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="Randomized Estimation Algorithms"><meta name="author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css"></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>Randomized Estimation Algorithms</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><a id=Introduction></a></p>

<h2>Introduction</h2>

<p>This page presents general-purpose algorithms for estimating the mean value of a stream of random numbers, or estimating the mean value of a function of those numbers.  The estimates are either <em>unbiased</em> (they have no systematic bias from the true mean value), or they come close to the true value with a user-specified error tolerance.</p>

<p>The algorithms are described to make them easy to implement by programmers.</p>

<p><a id=Concepts></a></p>

<h2>Concepts</h2>

<p>The following concepts are used in this document.</p>

<p>Each algorithm takes a stream of random numbers.  These numbers follow a <em>probability distribution</em> or simply <em>distribution</em>, or a rule that says which kinds of numbers are more likely to occur than others.  A distribution has the following properties.</p>

<ul>
<li>The <em>expectation</em>, <em>expected value</em>, or <em>mean</em> is the average value of the distribution.  It is expressed as <strong>E</strong>[<em>X</em>], where <em>X</em> is a random number from the stream.  In other words, take random samples and then take their average.  The average will approach the expected value as <em>n</em> gets large.</li>
<li>An <em>n<sup>th</sup> moment</em> is the expected value of <em>X</em><sup><em>n</em></sup>.  In other words, take random samples, raise them to the power <em>n</em>, then take their average.  The average will approach the <em>n</em><sup>th</sup> moment as <em>n</em> gets large.</li>
<li>An <em>n<sup>th</sup> central moment (about the mean)</em> is the expected value of (<em>X</em><sup><em>n</em></sup> &minus; <em>&mu;</em>), where <em>&mu;</em> is the distribution&#39;s mean.  The 2nd central moment is called <em>variance</em>, and the 4th central moment <em>kurtosis</em>.</li>
<li>An <em>n<sup>th</sup> central absolute moment</em> (c.a.m.) is the expected value of abs(<em>X</em><sup><em>n</em></sup> &minus; <em>&mu;</em>), where <em>&mu;</em> is the distribution&#39;s mean.  This is the same as the central moment when <em>n</em> is even.</li>
</ul>

<p>Some distributions don&#39;t have an <em>n</em><sup>th</sup> moment for a particular <em>n</em>.  This usually means the <em>n</em><sup>th</sup> power of the random numbers varies so wildly that it can&#39;t be estimated accurately.  If a distribution has an <em>n</em><sup>th</sup> moment, it also has a <em>k</em><sup>th</sup> moment for any <em>k</em> in the interval [1, <em>n</em>).</p>

<p>For any estimation algorithm, the <em>relative error</em> is abs(<em>est</em>, <em>trueval</em>) &minus; 1, where <em>est</em> is the estimate and <em>trueval</em> is the true expected value.</p>

<p><a id=Estimators_with_User_Specified_Relative_Error></a></p>

<h2>Estimators with User-Specified Relative Error</h2>

<p>The following algorithm from Huber (2017)<sup><a href="#Note1"><strong>(1)</strong></a></sup> estimates the probability of 1 of a stream of random zeros and ones (that is, it estimates the mean of a stream of Bernoulli random numbers with unknown mean).  The algorithm&#39;s relative error is independent of that probability, however, and the algorithm produces <em>unbiased</em> estimates.  The algorithm assumes the stream of numbers can&#39;t take on the value 0 with probability 1.</p>

<p>The algorithm, also known as <em>Gamma Bernoulli Approximation Scheme</em>, has the following parameters:</p>

<ul>
<li><em>&epsilon;</em>, <em>&delta;</em>: Both parameters must be greater than 0, and <em>&epsilon;</em> must be 3/4 or less, and <em>&delta;</em> must be 1 or less.  With this algorithm, the relative error will be no greater than <em>&epsilon;</em> with probability 1 &minus; <em>&delta;</em> or greater.</li>
</ul>

<p>The algorithm follows:</p>

<ol>
<li>Calculate the minimum number of samples <em>k</em>.  There are two suggestions.  The simpler one is <em>k</em> = ceil(&minus;6*ln(2/<em>&delta;</em>)/(<em>&epsilon;</em><sup>2</sup>*(4*<em>&epsilon;</em>&minus;3))).  A more complicated one is the smallest integer <em>k</em> such that gammainc(<em>k</em>,(<em>k</em>&minus;1)/(1+<em>&epsilon;</em>)) + (1 &minus; gammainc(<em>k</em>,(<em>k</em>&minus;1)/(1&minus;<em>&epsilon;</em>))) &le; <em>&delta;</em>, where gammainc is the regularized lower incomplete gamma function.</li>
<li>Take samples from the stream until <em>k</em> 1&#39;s are taken this way.  Let <em>r</em> be the total number of samples taken this way.</li>
<li>Generate <em>g</em>, a gamma(<em>r</em>) random variate, then return (<em>k</em>&minus;1)/<em>g</em>.</li>
</ol>

<blockquote>
<p><strong>Note</strong>:</p>

<ol>
<li><p>As noted in Huber 2017, if we have a stream of random numbers that take on values in the interval [0, 1], but have unknown mean, we can transform each number by&mdash;</p>

<ol>
<li>generating a uniform(0, 1) random variate <em>u</em>, then</li>
<li>changing that number to 1 if <em>u</em> is less than that number, or 0 otherwise,</li>
</ol>

<p>and we can use the new stream of zeros and ones in the algorithm to get an unbiased estimate of the unknown mean.</p></li>
<li>As can be seen in Feng et al. (2016)<sup><a href="#Note2"><strong>(2)</strong></a></sup>, the following is equivalent to steps 2 and 3 of the original algorithm: &quot;Let G be 0. Do this <em>k</em> times: &#39;Flip a coin until it shows heads, let <em>r</em> be the number of flips (including the last), and add a gamma(<em>r</em>) random variate to G.&#39; The estimated probability of heads is then (<em>k</em>&minus;1)/G.&quot;, and the following is likewise equivalent if the stream of random numbers follows a (zero-truncated) &quot;geometric&quot; distribution with unknown mean: &quot;Let G be 0. Do this <em>k</em> times: &#39;Take a sample from the stream, call it <em>r</em>, and add a gamma(<em>r</em>) random variate to G.&#39; The estimated mean is then (<em>k</em>&minus;1)/G.&quot; (This is with the understanding that the geometric distribution is defined differently in different academic works.)  The geometric algorithm produces unbiased estimates just like the original algorithm.</li>
</ol>
</blockquote>

<p><a id=An_Algorithm_for_a_Stream_of_Bounded_Random_Numbers></a></p>

<h2>An Algorithm for a Stream of Bounded Random Numbers</h2>

<p>The following algorithm comes from Huber and Jones (2019)<sup><a href="#Note3"><strong>(3)</strong></a></sup>; see also Huber (2017)<sup><a href="#Note4"><strong>(4)</strong></a></sup>.  It estimates the expected value of a stream of random numbers taking on values in the closed interval [0, 1].  It assumes the stream of numbers can&#39;t take on the value 0 with probability 1.</p>

<p>The algorithm has the following parameters:</p>

<ul>
<li><em>&epsilon;</em>, <em>&delta;</em>: Both parameters must be greater than 0, and <em>&epsilon;</em> must be 1/8 or less, and <em>&delta;</em> must be 1 or less.  The relative error is abs(<em>est</em>, <em>trueval</em>) &minus; 1, where <em>est</em> is the estimate and <em>trueval</em> is the true expected value.  With this algorithm, the relative error will be no greater than <em>&epsilon;</em> with probability 1 &minus; <em>&delta;</em> or greater.</li>
</ul>

<p>The algorithm follows.</p>

<ol>
<li>Set <em>k</em> to ceil(2*ln(6/<em>&delta;</em>)/<em>&epsilon;</em><sup>2/3</sup>).</li>
<li>Set <em>b</em> to 0 and <em>n</em> to 0.</li>
<li>(Stage 1: Modified gamma Bernoulli approximation scheme.) While <em>b</em> is less than <em>k</em>:

<ol>
<li>Add 1 to <em>n</em>.</li>
<li>Take a sample from the stream, call it <em>s</em>.</li>
<li>Generate a uniform(0, 1) random number, call it <em>u</em>.</li>
<li>If <em>u</em> is less than <em>s</em>, add 1 to <em>b</em>.</li>
</ol></li>
<li>Set <em>gb</em> to <em>k</em> + 2, then divide <em>gb</em> by a gamma(<em>n</em>) random variate.</li>
<li>(Find the sample size for the next stage.) Set <em>c1</em> to 2*ln(3/<em>&delta;</em>).</li>
<li>Set <em>n</em> to a Poisson(<em>c1</em>/(<em>&epsilon;</em>*<em>gb</em>)) random variate.</li>
<li>Run the standard deviation sub-algorithm (given later) <em>n</em> times.  Set <em>A</em> to the number of 1&#39;s returned by that sub-algorithm this way.</li>
<li>Set <em>csquared</em> to (<em>A</em> / <em>c1</em> + 1 / 2 + sqrt(<em>A</em> / <em>c1</em> + 1 / 4)) * (1 + <em>&epsilon;</em><sup>1 / 3</sup>)<sup>2</sup>*<em>&epsilon;</em>/<em>gb</em>.</li>
<li>Set <em>n</em> to ceil((2*ln(6/<em>&delta;</em>)/<em>&epsilon;</em><sup>2</sup>)/(1&minus;<em>&epsilon;</em><sup>1/3</sup>)).</li>
<li>(Stage 2: Light-tailed sample average.)  Set <em>e0</em> to <em>&epsilon;</em><sup>1/3</sup>.</li>
<li>Set <em>mu0</em> to <em>gb</em>/(1&minus;<em>e0</em><sup>2</sup>).</li>
<li>Set <em>alpha</em> to <em>&epsilon;</em>/(<em>csquared</em>*<em>mu0</em>).</li>
<li>Set <em>w</em> to <em>n</em>*<em>mu0</em>.</li>
<li>Do the following <em>n</em> times:

<ol>
<li>Get a sample from the stream, call it <em>g</em>.  Set <em>s</em> to <em>alpha</em>*(<em>g</em>&minus;<em>mu0</em>).</li>
<li>If <em>s</em>&ge;0, add ln(1+<em>s</em>+<em>s</em>*<em>s</em>/2)/<em>alpha</em> to <em>w</em>.  Otherwise, subtract ln(1&minus;<em>s</em>+<em>s</em>*<em>s</em>/2)/<em>alpha</em> from <em>w</em>.</li>
</ol></li>
<li>Return <em>w</em>/<em>n</em>.</li>
</ol>

<p>The standard deviation sub-algorithm follows:</p>

<ol>
<li>Generate an unbiased random bit.  If that bit is 1 (which happens with probability 1/2), return 0.</li>
<li>Get two samples from the stream, call them <em>x</em> and <em>y</em>.</li>
<li>Generate a uniform(0, 1) random number, call it <em>u</em>.</li>
<li>If <em>u</em> is less than (<em>x</em>&minus;<em>y</em>)<sup>2</sup>, return 1.  Otherwise, return 0.</li>
</ol>

<blockquote>
<p><strong>Note:</strong> As noted in Huber and Jones, if the stream of random numbers takes on values in the interval [0, <em>m</em>], where <em>m</em> is a known number, we can divide the stream&#39;s numbers by <em>m</em> before using them in this algorithm, and the algorithm will still work.</p>
</blockquote>

<p><a id=An_Adaptive_Algorithm></a></p>

<h2>An Adaptive Algorithm</h2>

<p>The following algorithm comes from Kunsch et al. (2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup>.  It estimates the mean of a stream of random numbers, assuming their distribution has the following properties:</p>

<ul>
<li>It has a finite <em>q</em><sup>th</sup> c.a.m. and <em>p</em><sup>th</sup> c.a.m. (also called <em>q</em>-moment and <em>p</em>-moment, respectively, in this section).</li>
<li>The <em>q</em>-moment&#39;s <em>q</em><sup>th</sup> root is no more than <em>&kappa;</em> times the <em>p</em>-moment&#39;s <em>p</em><sup>th</sup> root, where <em>&kappa;</em> is 1 or greater. (Note that the <em>q</em>-moment&#39;s <em>q</em><sup>th</sup> root is also known as <em>standard deviation</em> if <em>q</em> = 2, and <em>mean deviation</em> if <em>q</em> = 1; similarly for <em>p</em>.)</li>
</ul>

<p>The algorithm works by first estimating the <em>p</em>-moment of the stream, then using the estimate to determine a sample size for the next step, which actually estimates the stream&#39;s mean.</p>

<p>The algorithm has the following parameters:</p>

<ul>
<li><em>&epsilon;</em>, <em>&delta;</em>: Both parameters must be greater than 0, and <em>&delta;</em> must be 1 or less.  The algorithm will return an estimate within <em>&epsilon;</em> of the true expected value with probability 1 &minus; <em>&delta;</em> or greater.  The algorithm is not guaranteed to maintain a finite mean squared error or expected error in its estimates.</li>
<li><em>p</em>: The degree of the <em>p</em>-moment that the algorithm will estimate to determine the mean.</li>
<li><em>q</em>: The degree of the <em>q</em>-moment.  <em>q</em> must be greater than <em>p</em>.</li>
<li><em>&kappa;</em>: May not be less than the <em>q</em>-moment&#39;s  <em>q</em><sup>th</sup> root divided by the <em>p</em>-moment&#39;s <em>p</em><sup>th</sup> root, and may not be less than 1.</li>
</ul>

<p>For example:</p>

<ul>
<li>With parameters <em>p</em> = 2, <em>q</em> = 4, <em>&epsilon;</em> = 1/10, <em>&delta;</em> = 1/16, <em>&kappa;</em> = 1.1, the algorithm assumes the random numbers&#39; distribution has a bounded 4th c.a.m. and that the 4th c.a.m.&#39;s 4th root is no more than 1.1 times the 2nd c.a.m.&#39;s square root (that is, the standard deviation), and will return an estimate that&#39;s within 1/10 of the true mean with probability greater than (1 &minus; 1/16) or greater, or 15/16 or greater.</li>
<li>With parameters <em>p</em> = 1, <em>q</em> = 2, <em>&epsilon;</em> = 1/10, <em>&delta;</em> = 1/16, <em>&kappa;</em> = 2, the algorithm assumes the random numbers&#39; distribution has a standard deviation (<em>q</em>=2) that is no more than 2 times its mean deviation (<em>p</em>=1), and will return an estimate that&#39;s within 1/10 of the true mean with probability greater than (1 &minus; 1/16) or greater, or 15/16 or greater.</li>
</ul>

<p>The algorithm can be implemented as follows.</p>

<ol>
<li>If <em>&kappa;</em> is 1:

<ol>
<li>Set <em>n</em> to ceil(ln(1/<em>&delta;</em>)/ln(2))+1.</li>
<li>Get <em>n</em> samples from the stream and return (<em>mn</em> + <em>mx</em>)/2, where <em>mx</em> is the highest sample and <em>mn</em> is the lowest.</li>
</ol></li>
<li>Set <em>k</em> to ceil((2*ln(1/<em>&delta;</em>))/ln(4/3)).  If <em>k</em> is even, add 1 to <em>k</em>.</li>
<li>Set <em>kp</em> to <em>k</em>.</li>
<li>Set <em>&kappa;</em> to <em>&kappa;</em><sup>(<em>p</em>*<em>q</em>/(<em>q</em>&minus;<em>p</em>))</sup>.</li>
<li>If <em>q</em> is 2 or less:

<ul>
<li>Set <em>m</em> to ceil(3*<em>&kappa;</em>*48<sup>1/(<em>q</em>&minus;1)</sup>); set <em>s</em> to 1+1/(<em>q</em>&minus;1); set <em>&eta;</em> to 16<sup>1/(<em>q</em>&minus;1)</sup>*<em>&kappa;</em>/<em>&epsilon;</em><sup><em>s</em></sup>.</li>
</ul></li>
<li>If <em>q</em> is greater than 2:

<ul>
<li>Set <em>m</em> to ceil(144*<em>&kappa;</em>); set <em>s</em> to 2; set <em>&eta;</em> to 16*<em>&kappa;</em>/<em>&epsilon;</em><sup><em>s</em></sup>.</li>
</ul></li>
<li>(Stage 1: Estimate <em>p</em>-moment to determine number of samples for stage 2.)  Create <em>k</em> many blocks.  For each block:

<ol>
<li>Get <em>m</em> samples from the stream.</li>
<li>Add the samples and divide by <em>m</em> to get this block&#39;s sample mean, <em>mean</em>.</li>
<li>Calculate the <em>p</em>-moment estimate for this block, which is: (<em>&sum;</em><sub><em>i</em> = 0, ..., <em>k</em>&minus;1</sub> (<em>block</em>[<em>i</em>] &minus; <em>mean</em>)<sup><em>p</em></sup>)/<em>m</em>, where <em>block</em>[<em>i</em>] is the sample at position <em>i</em> of the block (positions start at 0).</li>
</ol></li>
<li>(Find the median of the <em>p</em>-moment estimates.)  Sort the <em>p</em>-moment estimates from step 7 in ascending order, and set <em>median</em> to the value in the middle of the sorted list (at position floor(<em>k</em>/2) with positions starting at 0); this works because <em>k</em> is odd.</li>
<li>(Calculate sample size for the next stage.)  Set <em>mp</em> to max(1, ceil(<em>&eta;</em> * <em>median</em><sup><em>s</em></sup>)).</li>
<li>(Stage 2: Estimate of the sample mean.) Create <em>kp</em> many blocks.  For each block:

<ol>
<li>Get <em>mp</em> samples from the stream.</li>
<li>Add the samples and divide by <em>mp</em> to get this block&#39;s sample mean.</li>
</ol></li>
<li>(Find the median of the sample means.  This is definitely an unbiased estimate of the mean when <em>kp</em> is 1 or 2, but unfortunately, it isn&#39;t one for any <em>kp</em> &gt; 2.)  Sort the sample means from step 10 in ascending order, and return the value in the middle of the sorted list (at position floor(<em>kp</em>/2) with positions starting at 0); this works because <em>kp</em> is odd.</li>
</ol>

<blockquote>
<p><strong>Note:</strong> If the stream of random numbers meets the condition for this algorithm for a given <em>q</em>, <em>p</em>, and <em>&kappa;</em>, then it still meets that condition when those numbers are multiplied by a constant or a constant is added to them.</p>
</blockquote>

<p><a id=Randomized_Integration></a></p>

<h2>Randomized Integration</h2>

<p>Monte Carlo integration is a randomized way to estimate the integral of a function.  The adaptive algorithm in this article can be used to estimate an integral of a function <em>f</em>(<em>Z</em>), where <em>Z</em> is an n-dimensional vector chosen at random in the sampling domain.  The estimate will come within <em>&epsilon;</em> of the true integral with probability 1 &minus; <em>&delta;</em> or greater, as long as the following conditions are met:</p>

<ul>
<li>The <em>q</em><sup>th</sup> c.a.m. for <em>f</em>(<em>Z</em>) is finite.  That is, <strong>E</strong>[abs(<em>f</em>(<em>Z</em>)&minus;<strong>E</strong>[<em>f</em>(<em>Z</em>)])<sup><em>q</em></sup>] is finite.</li>
<li>The <em>q</em><sup>th</sup> c.a.m.&#39;s <em>q</em><sup>th</sup> root is no more than <em>&kappa;</em> times the <em>p</em><sup>th</sup> c.a.m.&#39;s <em>p</em><sup>th</sup> root, where <em>&kappa;</em> is 1 or greater.</li>
</ul>

<p>Unfortunately, these conditions may be hard to verify in practice, especially when <em>f</em>(<em>Z</em>) is not known.  (In fact, <strong>E</strong>[<em>f</em>(<em>Z</em>)], as seen above, is the unknown integral that we seek to estimate.)</p>

<p>For this purpose, each number in the stream of random numbers is generated as follows (see also Kunsch et al.):</p>

<ol>
<li>Set <em>Z</em> to an <em>n</em>-dimensional vector (list of <em>n</em> numbers) chosen at random in the sampling domain, independently of any other choice.  Usually, <em>Z</em> is chosen <em>uniformly</em> at random this way.</li>
<li>Calculate <em>f</em>(<em>Z</em>), and set the next number in the stream to that value.</li>
</ol>

<p>The following example (coded in Python for the SymPy computer algebra library) shows how to find parameter <em>&kappa;</em> for estimating the integral of min(<em>Z1</em>, <em>Z2</em>) where <em>Z1</em> and <em>Z2</em> are each uniformly chosen at random in the interval [0, 1].  It assumes <em>p</em> = 2 and <em>q</em> = 4. (This is a trivial example because we can calculate the integral directly &mdash; 1/3 &mdash; but it shows how to proceed for more complicated cases.)</p>

<pre># Distribution of Z1 and Z2
u1=Uniform(&#39;U1&#39;,0,1)
u2=Uniform(&#39;U2&#39;,0,1)
# Function to estimate
func = Min(u1,u2)
emean=E(func)
p = S(2) # Degree of p-moment
q = S(4) # Degree of q-moment
# Calculate value for kappa
kappa = E(Abs(func-emean)**q)**(1/q) / E(Abs(func-emean)**p)**(1/p)
pprint(Max(1,kappa))
</pre>

<p><a id=Requests_and_Open_Questions></a></p>

<h2>Requests and Open Questions</h2>

<p>There is one request on randomized estimation:</p>

<p>Assume we have an <em>oracle</em>, or &quot;black box&quot;, that produces independent random numbers <em>X</em> with a known minimum and maximum, but an unknown mean (average).  Given this oracle and possibly a second source of randomness (such as unbiased random bits), are there results that give an algorithm that approximates <em>f</em>(<strong>E</strong>[<em>X</em>]) with a user-specified error bound, ideally using as few samples from the oracle as possible?  Here, <em>f</em> is a known function belonging to a given class of functions, and <strong>E</strong>[<em>X</em>] is the true mean of the oracle&#39;s numbers.</p>

<p>The algorithm should&mdash;</p>

<ul>
<li>ensure the expected (absolute) error and/or mean squared error is within a user-specified error tolerance (<em>&epsilon;</em>), or if that is not possible,</li>
<li>return an estimate that is within a user-specified tolerance (<em>&epsilon;</em>) on the absolute error or relative error with probability greater than 1 minus <em>&delta;</em>, where <em>&delta;</em> is user-specified.</li>
</ul>

<p>The classes of functions <em>f</em> that are of interest are:</p>

<ul>
<li>C1: Continuous functions that map [0, 1] to [0, 1] and are polynomially bounded (meaning that <em>f</em>(<em>x</em>) and 1&minus;<em>f</em>(<em>x</em>) are both bounded from below by min(<em>x</em><sup><em>n</em></sup>, (1&minus;<em>x</em>)<sup><em>n</em></sup>) for some integer <em>n</em>) (Keane and O&#39;Brien 1994)<sup><a href="#Note6"><strong>(6)</strong></a></sup>.</li>
<li>C2: Continuous functions that map [0, 1] to [0, 1] and are not necessarily polynomially bounded.</li>
<li>C3: Piecewise continuous functions that map [0, 1] to [0, 1].</li>
</ul>

<p>Note the following:</p>

<ul>
<li>The value the algorithm should approximate is <em>f</em>(<strong>E</strong>[<em>X</em>]), not <strong>E</strong>[<em>f</em>(<em>X</em>)], which is different in general.  But algorithms for <strong>E</strong>[<em>f</em>(<em>X</em>)] are welcome in case it&#39;s more difficult.</li>
<li>The Gamma Bernoulli Approximation scheme as well as the Kunsch algorithm both estimate the mean of <em>X</em> (<strong>E</strong>[<em>X</em>]), rather than a function of that mean (<em>f</em>(<strong>E</strong>[<em>X</em>])), as this request asks.</li>
<li>Some algorithms produce unbiased estimates of <em>f</em>(<strong>E</strong>[<em>X</em>]), but not with a user-specified error, as this request asks (Jacob and Thiery 2015)<sup><a href="#Note7"><strong>(7)</strong></a></sup>.</li>
<li>Algorithms like the one being asked for here are especially useful because they can help build so-called &quot;<a href="https://peteroupc.github.io/bernsupp.html#Approximate_Bernoulli_Factories"><strong>approximate Bernoulli factories</strong></a>&quot;, or algorithms that approximately sample the probability <em>f</em>(<em>&lambda;</em>) given a coin with probability of heads of <em>&lambda;</em>.   In this case, <em>X</em> should follow the Bernoulli distribution.</li>
<li>It is suspected that the algorithm&#39;s performance will depend on the &quot;smoothness&quot; of <em>f</em>(<em>X</em>) (see also (Holtz et al. 2011)<sup><a href="#Note8"><strong>(8)</strong></a></sup>).</li>
</ul>

<p>Also, are there results that estimate <em>f</em>(<strong>E</strong>[<em>X</em>]) with a user-specified error bound even if <em>X</em> follows a certain kind of unbounded distribution (such as a geometric, exponential, or Poisson distribution, or another distribution with exponential tails)?</p>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<ul>
<li><small><sup id=Note1>(1)</sup> Huber, M., 2017. A Bernoulli mean estimate with known relative error distribution. Random Structures &amp; Algorithms, 50(2), pp.173-182. (preprint in arXiv:1309.5413v2  [math.ST], 2015).</small></li>
<li><small><sup id=Note2>(2)</sup> Feng, J. et al. “Monte Carlo with User-Specified Relative Error.” (2016).</small></li>
<li><small><sup id=Note3>(3)</sup> Huber, Mark, and Bo Jones. &quot;Faster estimates of the mean of bounded random variables.&quot; Mathematics and Computers in Simulation 161 (2019): 93-101.</small></li>
<li><small><sup id=Note4>(4)</sup> Huber, Mark, &quot;<a href="https://arxiv.org/abs/1706.01478"><strong>An optimal(<em>&epsilon;</em>, <em>&delta;</em>)-approximation scheme for the mean of random variables with bounded relative variance</strong></a>&quot;, arXiv:1706.01478, 2017.</small></li>
<li><small><sup id=Note5>(5)</sup> Kunsch, Robert J., Erich Novak, and Daniel Rudolf. &quot;Solvable integration problems and optimal sample size selection.&quot; Journal of Complexity 53 (2019): 40-67.  Also in <a href="https://arxiv.org/pdf/1805.08637.pdf"><strong>https://arxiv.org/pdf/1805.08637.pdf</strong></a> .</small></li>
<li><small><sup id=Note6>(6)</sup> Keane, M. S., and O&#39;Brien, G. L., &quot;A Bernoulli factory&quot;, ACM Transactions on Modeling and Computer Simulation 4(2), 1994.</small></li>
<li><small><sup id=Note7>(7)</sup> Pierre E. Jacob.  Alexandre H. Thiery. &quot;On nonnegative unbiased estimators.&quot; Ann. Statist. 43 (2) 769 - 784, April 2015. <a href="https://doi.org/10.1214/15-AOS1311"><strong>https://doi.org/10.1214/15-AOS1311</strong></a></small></li>
<li><small><sup id=Note8>(8)</sup> Holtz, O., Nazarov, F., Peres, Y., &quot;New Coins from Old, Smoothly&quot;, Constructive Approximation 33 (2011).</small></li>
</ul>

<p><a id=License></a></p>

<h2>License</h2>

<p>Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under <a href="https://creativecommons.org/publicdomain/zero/1.0/"><strong>Creative Commons Zero</strong></a>.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>
<p>
If you like this software, you should consider donating to me, Peter O., at the link below:</p>
<p class="printonly"><b>peteroupc.github.io</b></p>
<div class="noprint">
<a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=56E5T4FH7KD2S">
<img src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif"
name="submit" border="2" alt="PayPal - The safer, easier way to pay online!"></a>
<p>
<a href="//twitter.com/share">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
if("share" in navigator){
 document.getElementById("sharer").href="javascript:void(null)";
 document.getElementById("sharer").innerHTML="Share This Page";
 navigator.share({title:document.title,url:document.location.href}).then(
   function(){});
} else {
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
}
</script>
</body></html>
