<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>More Random Sampling Methods</title><meta name="citation_title" content="More Random Sampling Methods"><meta name="citation_pdf_url" content="https://peteroupc.github.io/randomnotes.pdf"><meta name="citation_url" content="https://peteroupc.github.io/randomnotes.html"><meta name="citation_date" content="2021/10/30"><meta name="citation_online_date" content="2021/10/30"><meta name="og:title" content="More Random Sampling Methods"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/randomnotes.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="More Random Sampling Methods"><meta name="author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css"></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>More Random Sampling Methods</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><a id=Contents></a></p>

<h2>Contents</h2>

<ul>
<li><a href="#Contents"><strong>Contents</strong></a>

<ul>
<li><a href="#Specific_Distributions"><strong>Specific Distributions</strong></a>

<ul>
<li><a href="#Normal_Gaussian_Distribution"><strong>Normal (Gaussian) Distribution</strong></a></li>
<li><a href="#Gamma_Distribution"><strong>Gamma Distribution</strong></a></li>
<li><a href="#Beta_Distribution"><strong>Beta Distribution</strong></a></li>
<li><a href="#Noncentral_Hypergeometric_Distributions"><strong>Noncentral Hypergeometric Distributions</strong></a></li>
<li><a href="#von_Mises_Distribution"><strong>von Mises Distribution</strong></a></li>
<li><a href="#Stable_Distribution"><strong>Stable Distribution</strong></a></li>
<li><a href="#Multivariate_Normal_Multinormal_Distribution"><strong>Multivariate Normal (Multinormal) Distribution</strong></a></li>
<li><a href="#Gaussian_and_Other_Copulas"><strong>Gaussian and Other Copulas</strong></a></li>
</ul></li>
</ul></li>
<li><a href="#Notes"><strong>Notes</strong></a></li>
<li><a href="#Appendix"><strong>Appendix</strong></a>

<ul>
<li><a href="#Implementation_of_erf"><strong>Implementation of <code>erf</code></strong></a></li>
<li><a href="#Exact_Error_Bounded_and_Approximate_Algorithms"><strong>Exact, Error-Bounded, and Approximate Algorithms</strong></a></li>
</ul></li>
<li><a href="#License"><strong>License</strong></a></li>
</ul>

<p><a id=Specific_Distributions></a></p>

<h3>Specific Distributions</h3>

<p><strong>Requires random real numbers.</strong>  This section shows algorithms to sample several popular non-uniform distributions.  The algorithms are exact unless otherwise noted, and applications should choose algorithms with either no error (including rounding error) or a user-settable error bound.  See the <a href="#Exact_Error_Bounded_and_Approximate_Algorithms"><strong>appendix</strong></a> for more information.</p>

<p><a id=Normal_Gaussian_Distribution></a></p>

<h4>Normal (Gaussian) Distribution</h4>

<p>The <a href="https://en.wikipedia.org/wiki/Normal_distribution"><strong><em>normal distribution</em></strong></a> (also called the Gaussian distribution) takes the following two parameters:</p>

<ul>
<li><code>mu</code> (&mu;) is the mean (average), or where the peak of the distribution&#39;s &quot;bell curve&quot; is.</li>
<li><code>sigma</code> (&sigma;), the standard deviation, affects how wide the &quot;bell curve&quot; appears. The
probability that a number sampled from the normal distribution will be within one standard deviation from the mean is about 68.3%; within two standard deviations (2 times <code>sigma</code>), about 95.4%; and within three standard deviations, about 99.7%.  (Some publications give &sigma;<sup>2</sup>, or variance, rather than standard deviation, as the second parameter.  In this case, the standard deviation is the variance&#39;s square root.)</li>
</ul>

<p>There are a number of methods for sampling the normal distribution. An application can combine some or all of these.</p>

<ol>
<li>The ratio-of-uniforms method (given as <code>NormalRatioOfUniforms</code> below).</li>
<li>In the <em>Box&ndash;Muller transformation</em>, <code>mu + radius * cos(angle)</code> and <code>mu + radius * sin(angle)</code>, where <code>angle = RNDRANGEMaxExc(0, 2 * pi)</code> and <code>radius = sqrt(Expo(0.5)) * sigma</code>, are two independent values sampled from the normal distribution.  The polar method (given as <code>NormalPolar</code> below) likewise produces two independent values sampled from that distribution at a time.</li>
<li>Karney&#39;s algorithm to sample from the normal distribution, in a manner that minimizes approximation error and without using floating-point numbers (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.</li>
</ol>

<p>For surveys of Gaussian samplers, see (Thomas et al. 2007)<sup><a href="#Note2"><strong>(2)</strong></a></sup>, and (Malik and Hemani 2016)<sup><a href="#Note3"><strong>(3)</strong></a></sup>.</p>

<pre>METHOD NormalRatioOfUniforms(mu, sigma)
    while true
        a=RNDRANGEMinExc(0,1)
        b=RNDRANGE(0,sqrt(2.0/exp(1.0)))
        if b*b &lt;= -a * a * 4 * ln(a)
          return (RNDINT(1) * 2 - 1) *
            (b * sigma / a) + mu
        end
    end
END METHOD

METHOD NormalPolar(mu, sigma)
  while true
    a = RNDRANGEMinExc(0,1)
    b = RNDRANGEMinExc(0,1)
    if RNDINT(1) == 0: a = 0 - a
    if RNDINT(1) == 0: b = 0 - b
    c = a * a + b * b
    if c != 0 and c &lt;= 1
       c = sqrt(-ln(c) * 2 / c)
       return [a * sigma * c + mu, b * sigma * c + mu]
    end
  end
END METHOD
</pre>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li>The <em>standard normal distribution</em> is implemented as <code>Normal(0, 1)</code>.</li>
<li>Methods implementing a variant of the normal distribution, the <em>discrete Gaussian distribution</em>, generate <em>integers</em> that closely follow the normal distribution.  Examples include the one in (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup>, an improved version in (Du et al. 2021)<sup><a href="#Note4"><strong>(4)</strong></a></sup>, as well as so-called &quot;constant-time&quot; methods such as (Micciancio and Walter 2017)<sup><a href="#Note5"><strong>(5)</strong></a></sup> that are used above all in <em>lattice-based cryptography</em>.</li>
<li>The following are some approximations to the normal distribution that papers have suggested:

<ul>
<li>The sum of twelve <code>RNDRANGEMaxExc(0, sigma)</code> numbers, subtracted by 6 * <code>sigma</code>, to generate an approximate normal variate with mean 0 and standard deviation <code>sigma</code>. (Kabal 2000/2019)<sup><a href="#Note6"><strong>(6)</strong></a></sup> &quot;warps&quot; this sum in the following way (before adding the mean <code>mu</code>) to approximate the normal distribution better: <code>ssq = sum * sum; sum = ((((0.0000001141*ssq - 0.0000005102) * ssq + 0.00007474) * ssq + 0.0039439) * ssq + 0.98746) * sum</code>. See also <a href="https://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution"><strong>&quot;Irwin&ndash;Hall distribution&quot;</strong></a>, namely the sum of <code>n</code> many <code>RNDRANGE(0, 1)</code> numbers, on Wikipedia.  D. Thomas (2014)<sup><a href="#Note7"><strong>(7)</strong></a></sup>, describes a more general approximation called CLT<sub>k</sub>, which combines <code>k</code> numbers in [0, 1] sampled from the uniform distribution as follows: <code>RNDRANGE(0, 1) - RNDRANGE(0, 1) + RNDRANGE(0, 1) - ...</code>.</li>
<li>Numerical <a href="#Inverse_Transform_Sampling"><strong>inversions</strong></a> of the normal distribution&#39;s cumulative distribution function (CDF), including those by Wichura, by Acklam, and by Luu (Luu 2016)<sup><a href="#Note8"><strong>(8)</strong></a></sup>.  See also <a href="https://www.johndcook.com/blog/normal_cdf_inverse/"><strong>&quot;A literate program to compute the inverse of the normal CDF&quot;</strong></a>.  Notice that the normal distribution&#39;s inverse CDF has no closed form.</li>
</ul></li>
<li>A pair of <em>q-Gaussian</em> random variates with parameter <code>q</code> less than 3 can be generated using the Box&ndash;Muller transformation, except <code>radius</code> is <code>radius=sqrt(-2*(pow(u,1-qp)-1)/(1-qp))</code> (where <code>qp=(1+q)/(3-q)</code> and <code>u=RNDRANGE(0, 1)</code>), and the two variates are not statistically independent (Thistleton et al. 2007)<sup><a href="#Note9"><strong>(9)</strong></a></sup>.</li>
<li>A well-known result says that adding <code>n</code> many <code>Normal(0, 1)</code> variates, and dividing by <code>sqrt(n)</code>, results in a new <code>Normal(0, 1)</code> variate.</li>
</ol>
</blockquote>

<p><a id=Gamma_Distribution></a></p>

<h4>Gamma Distribution</h4>

<p>The following method samples a number from a <em>gamma distribution</em> and is based on Marsaglia and Tsang&#39;s method from 2000<sup><a href="#Note10"><strong>(10)</strong></a></sup> and (Liu et al. 2015)<sup><a href="#Note11"><strong>(11)</strong></a></sup>.  Usually, the number expresses either&mdash;</p>

<ul>
<li>the lifetime (in days, hours, or other fixed units) of a random component with an average lifetime of <code>meanLifetime</code>, or</li>
<li>a random amount of time (in days, hours, or other fixed units) that passes until as many events as <code>meanLifetime</code> happen.</li>
</ul>

<p>Here, <code>meanLifetime</code> must be an integer or noninteger greater than 0, and <code>scale</code> is a scaling parameter that is greater than 0, but usually 1 (the random gamma number is multiplied by <code>scale</code>).</p>

<pre>METHOD GammaDist(meanLifetime, scale)
    // Needs to be greater than 0
    if meanLifetime &lt;= 0 or scale &lt;= 0: return error
    // Exponential distribution special case if
    // `meanLifetime` is 1 (see also (Devroye 1986), p. 405)
    if meanLifetime == 1: return Expo(1.0 / scale)
    if meanLifetime &lt; 0.3 // Liu, Martin, Syring 2015
       lamda = (1.0/meanLifetime) - 1
       w = meanLifetime / (1-meanLifetime) * exp(1)
       r = 1.0/(1+w)
       while true
            z = 0
            x = RNDRANGE(0, 1)
            if x &lt;= r: z = -ln(x/r)
            else: z = -Expo(lamda)
            ret = exp(-z/meanLifetime)
            eta = 0
            if z&gt;=0: eta=exp(-z)
            else: eta=w*lamda*exp(lamda*z)
            if RNDRANGE(0, eta) &lt; exp(-ret-z): return ret * scale
       end
    end
    d = meanLifetime
    v = 0
    if meanLifetime &lt; 1: d = d + 1
    d = d - (1.0 / 3) // NOTE: 1.0 / 3 must be a fractional number
    c = 1.0 / sqrt(9 * d)
    while true
        x = 0
        while true
           x = Normal(0, 1)
           v = c * x + 1;
           v = v * v * v
           if v &gt; 0: break
        end
        u = RNDRANGEMinExc(0,1)
        x2 = x * x
        if u &lt; 1 - (0.0331 * x2 * x2): break
        if ln(u) &lt; (0.5 * x2) + (d * (1 - v + ln(v))): break
    end
    ret = d * v
    if meanLifetime &lt; 1
       ret = ret * pow(RNDRANGE(0, 1), 1.0 / meanLifetime)
    end
    return ret * scale
END METHOD
</pre>

<blockquote>
<p><strong>Note:</strong> The following is a useful identity for the gamma distribution: <code>GammaDist(a) = BetaDist(a, b - a) * GammaDist(b)</code> (Stuart 1962)<sup><a href="#Note12"><strong>(12)</strong></a></sup>.</p>
</blockquote>

<p><a id=Beta_Distribution></a></p>

<h4>Beta Distribution</h4>

<p>The beta distribution is a bounded-domain probability distribution; its two parameters, <code>a</code> and <code>b</code>, are both greater than 0 and describe the distribution&#39;s shape.  Depending on <code>a</code> and <code>b</code>, the shape can be a smooth peak or a smooth valley.</p>

<p>The following method samples a number from a <em>beta distribution</em>, in the interval [0, 1).</p>

<pre>METHOD BetaDist(a, b)
  if b==1 and a==1: return RNDRANGE(0, 1)
  // Min-of-uniform
  if a==1: return 1.0-pow(RNDRANGE(0, 1),1.0/b)
  // Max-of-uniform.  Use only if a is small to
  // avoid accuracy problems, as pointed out
  // by Devroye 1986, p. 675.
  if b==1 and a &lt; 10: return pow(RNDRANGE(0, 1),1.0/a)
  x=GammaDist(a,1)
  return x/(x+GammaDist(b,1))
END METHOD
</pre>

<p>I give an <a href="https://peteroupc.github.io/exporand.html"><strong>error-bounded sampler</strong></a> for the beta distribution (when <code>a</code> and <code>b</code> are both 1 or greater) in a separate page.</p>

<p><a id=Noncentral_Hypergeometric_Distributions></a></p>

<h4>Noncentral Hypergeometric Distributions</h4>

<p>The following variants of the hypergeometric distribution are described in detail by Agner Fog in &quot;<a href="https://cran.r-project.org/web/packages/BiasedUrn/vignettes/UrnTheory.pdf"><strong>Biased Urn Theory</strong></a>&quot;.</p>

<p>Let there be <em>m</em> balls that each have one of two or more colors.  For each color, assign each ball of that color the same weight (a real number 0 or greater).  Then:</p>

<ol>
<li><strong>Wallenius&#39;s hypergeometric distribution:</strong> Choose one ball not yet chosen, with probability equal to its weight divided by the sum of weights of balls not yet chosen.  Repeat until exactly <em>n</em> items are chosen this way.  Then for each color, count the number of items of that color chosen this way.</li>
<li><strong>Fisher&#39;s hypergeometric distribution:</strong> For each ball, choose it with probability equal to its weight divided by the sum of weights of all balls.  (Thus, each ball is independently chosen or not chosen depending on its weight.)  If exactly <em>n</em> items were chosen this way, stop.  Otherwise, start over.  Then among the last <em>n</em> items chosen this way, count the number of items of each color.</li>
</ol>

<p>For both distributions, if there are two colors, there are four parameters: <em>m</em>, <em>ones</em>, <em>n</em>, <em>weight</em>, such that&mdash;</p>

<ul>
<li>for the first color, there are <em>ones</em> many balls each with weight <em>weight</em>;</li>
<li>for the second color, there are (<em>m</em>&minus;<em>ones</em>) many balls each with weight 1; and</li>
<li>the random variate is the number of chosen balls of the first color.</li>
</ul>

<p><a id=von_Mises_Distribution></a></p>

<h4>von Mises Distribution</h4>

<p>The <em>von Mises distribution</em> describes a distribution of circular angles and uses two parameters: <code>mean</code> is the mean angle and <code>kappa</code> is a shape parameter.  The distribution is uniform at <code>kappa = 0</code> and approaches a normal distribution with increasing <code>kappa</code>.</p>

<p>The algorithm below samples a number from the von Mises distribution, and is based on the Best&ndash;Fisher algorithm from 1979 (as described in (Devroye 1986)<sup><a href="#Note13"><strong>(13)</strong></a></sup> with errata incorporated).</p>

<pre>METHOD VonMises(mean, kappa)
    if kappa &lt; 0: return error
    if kappa == 0
        return RNDRANGEMinMaxExc(mean-pi, mean+pi)
    end
    r = 1.0 + sqrt(4 * kappa * kappa + 1)
    rho = (r - sqrt(2 * r)) / (kappa * 2)
    s = (1 + rho * rho) / (2 * rho)
    while true
        u = RNDRANGEMaxExc(-pi, pi)
        v = RNDRANGEMinMaxExc(0, 1)
        z = cos(u)
        w = (1 + s*z) / (s + z)
        y = kappa * (s - w)
        if y*(2 - y) - v &gt;=0 or ln(y / v) + 1 - y &gt;= 0
           if angle&lt;-1: angle=-1
           if angle&gt;1: angle=1
           // NOTE: Inverse cosine replaced here
           // with `atan2` equivalent
           angle = atan2(sqrt(1-w*w),w)
           if u &lt; 0: angle = -angle
           return mean + angle
        end
    end
END METHOD
</pre>

<p><a id=Stable_Distribution></a></p>

<h4>Stable Distribution</h4>

<p>As more and more numbers, sampled independently at random in the same way, are added together, their distribution tends to a <a href="https://en.wikipedia.org/wiki/Stable_distribution"><strong><em>stable distribution</em></strong></a>, which resembles a curve with a single peak, but with generally &quot;fatter&quot; tails than the normal distribution.  (Here, the stable distribution means the &quot;alpha-stable distribution&quot;.) The pseudocode below uses the Chambers&ndash;Mallows&ndash;Stuck algorithm.  The <code>Stable</code> method, implemented below, takes two parameters:</p>

<ul>
<li><code>alpha</code> is a stability index in the interval (0, 2].</li>
<li><code>beta</code> is an asymmetry parameter in the interval [-1, 1]; if <code>beta</code> is 0, the curve is symmetric.</li>
</ul>

<p>&nbsp;</p>

<pre>METHOD Stable(alpha, beta)
    if alpha &lt;=0 or alpha &gt; 2: return error
    if beta &lt; -1 or beta &gt; 1: return error
    halfpi = pi * 0.5
    unif=RNDRANGEMinMaxExc(-halfpi, halfpi)
    c=cos(unif)
    if alpha == 1
       s=sin(unif)
       if beta == 0: return s/c
       expo=Expo(1)
       return 2.0*((unif*beta+halfpi)*s/c -
         beta * ln(halfpi*expo*c/(unif*beta+halfpi)))/pi
    else
       z=-tan(alpha*halfpi)*beta
       ug=unif+atan2(-z, 1)/alpha
       cpow=pow(c, -1.0 / alpha)
       return pow(1.0+z*z, 1.0 / (2*alpha))*
          (sin(alpha*ug)*cpow)*
          pow(cos(unif-alpha*ug)/expo, (1.0 - alpha) / alpha)
    end
END METHOD
</pre>

<p>Methods implementing the strictly geometric stable and general geometric stable distributions are shown below (Kozubowski 2000)<sup><a href="#Note14"><strong>(14)</strong></a></sup>.  Here, <code>alpha</code> is in (0, 2], <code>lamda</code> is greater than 0, and <code>tau</code>&#39;s absolute value is not more than min(1, 2/<code>alpha</code> &minus; 1).  The result of <code>GeometricStable</code> is a symmetric Linnik distribution if <code>tau = 0</code>, or a Mittag-Leffler distribution if <code>tau = 1</code> and <code>alpha &lt; 1</code>.</p>

<pre>METHOD GeometricStable(alpha, lamda, tau)
   rho = alpha*(1-tau)/2
   sign = -1
   if tau==1 or RNDINT(1)==0 or RNDRANGE(0, 1) &lt; tau
       rho = alpha*(1+tau)/2
       sign = 1
   end
   w = 1
   if rho != 1
      rho = rho * pi
      cotparam = RNDRANGE(0, rho)
      w = sin(rho)*cos(cotparam)/sin(cotparam)-cos(rho)
   end
   return Expo(1) * sign * pow(lamda*w, 1.0/alpha)
END METHOD

METHOD GeneralGeoStable(alpha, beta, mu, sigma)
   z = Expo(1)
   if alpha == 1: return mu*z+Stable(alpha, beta)*sigma*z+
          sigma*z*beta*2*pi*ln(sigma*z)
   else: return mu*z+
          Stable(alpha, beta)*sigma*pow(z, 1.0/alpha)
END METHOD
</pre>

<p><a id=Multivariate_Normal_Multinormal_Distribution></a></p>

<h4>Multivariate Normal (Multinormal) Distribution</h4>

<p>The following pseudocode calculates a random vector (list of numbers) that follows a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution"><strong><em>multivariate normal (multinormal) distribution</em></strong></a>.  The method <code>MultivariateNormal</code> takes the following parameters:</p>

<ul>
<li>A list, <code>mu</code> (&mu;), which indicates the means to add to the random vector&#39;s components. <code>mu</code> can be <code>nothing</code>, in which case each component will have a mean of zero.</li>
<li>A list of lists <code>cov</code>, that specifies a <em>covariance matrix</em> (&Sigma;, a symmetric positive definite N&times;N matrix, where N is the number of components of the random vector).</li>
</ul>

<p>&nbsp;</p>

<pre>METHOD Decompose(matrix)
  numrows = size(matrix)
  if size(matrix[0])!=numrows: return error
  // Does a Cholesky decomposition of a matrix
  // assuming it&#39;s positive definite and invertible
  ret=NewList()
  for i in 0...numrows
    submat = NewList()
    for j in 0...numrows: AddItem(submat, 0)
    AddItem(ret, submat)
  end
  s1 = sqrt(matrix[0][0])
  if s1==0: return ret // For robustness
  for i in 0...numrows
    ret[0][i]=matrix[0][i]*1.0/s1
  end
  for i in 0...numrows
    msum=0.0
    for j in 0...i: msum = msum + ret[j][i]*ret[j][i]
    sq=matrix[i][i]-msum
    if sq&lt;0: sq=0 // For robustness
    ret[i][i]=math.sqrt(sq)
  end
  for j in 0...numrows
    for i in (j + 1)...numrows
      // For robustness
      if ret[j][j]==0: ret[j][i]=0
      if ret[j][j]!=0
        msum=0
        for k in 0...j: msum = msum + ret[k][i]*ret[k][j]
        ret[j][i]=(matrix[j][i]-msum)*1.0/ret[j][j]
      end
    end
  end
  return ret
END METHOD

METHOD MultivariateNormal(mu, cov)
  mulen=size(cov)
  if mu != nothing
    mulen = size(mu)
    if mulen!=size(cov): return error
    if mulen!=size(cov[0]): return error
  end
  // NOTE: If multiple random points will
  // be generated using the same covariance
  // matrix, an implementation can consider
  // precalculating the decomposed matrix
  // in advance rather than calculating it here.
  cho=Decompose(cov)
  i=0
  ret=NewList()
  vars=NewList()
  for j in 0...mulen: AddItem(vars, Normal(0, 1))
  while i&lt;mulen
    nv=Normal(0,1)
    msum = 0
    if mu == nothing: msum=mu[i]
    for j in 0...mulen: msum=msum+vars[j]*cho[j][i]
    AddItem(ret, msum)
    i=i+1
  end
  return ret
end
</pre>

<blockquote>
<p><strong>Note:</strong> The <a href="https://peteroupc.github.io/randomgen.zip"><strong>Python sample code</strong></a> contains a variant of this
method for generating multiple random vectors in one call.</p>

<p><strong>Examples:</strong></p>

<ol>
<li>A vector that follows a <strong>binormal distribution</strong> (two-variable multinormal distribution) is a vector of two numbers from the normal distribution, and can be sampled using the following idiom: <code>MultivariateNormal([mu1, mu2], [[s1*s1, s1*s2*rho], [rho*s1*s2, s2*s2]])</code>, where <code>mu1</code> and <code>mu2</code> are the means of the vector&#39;s two components, <code>s1</code> and <code>s2</code> are their standard deviations, and <code>rho</code> is a <em>correlation coefficient</em> greater than -1 and less than 1 (0 means no correlation).</li>
<li><strong>Log-multinormal distribution</strong>: Generate a multinormal random vector, then apply <code>exp(n)</code> to each component <code>n</code>.</li>
<li>A <strong>Beckmann distribution</strong>: Generate a random binormal vector <code>vec</code>, then apply <code>Norm(vec)</code> to that vector.</li>
<li>A <strong>Rice (Rician) distribution</strong> is a Beckmann distribution in which the binormal random pair is generated with <code>m1 = m2 = a / sqrt(2)</code>, <code>rho = 0</code>, and <code>s1 = s2 = b</code>, where <code>a</code> and <code>b</code> are the parameters to the Rice distribution.</li>
<li><strong>Rice&ndash;Norton distribution</strong>: Generate <code>vec = MultivariateNormal([v,v,v],[[w,0,0],[0,w,0],[0,0,w]])</code> (where <code>v = a/sqrt(m*2)</code>, <code>w = b*b/m</code>, and <code>a</code>, <code>b</code>, and <code>m</code> are the parameters to the Rice&ndash;Norton distribution), then apply <code>Norm(vec)</code> to that vector.</li>
<li>A <strong>standard</strong> <a href="https://en.wikipedia.org/wiki/Complex_normal_distribution"><strong>complex normal distribution</strong></a> is a binormal distribution in which the binormal random pair is generated with <code>s1 = s2 = sqrt(0.5)</code> and <code>mu1 = mu2 = 0</code> and treated as the real and imaginary parts of a complex number.</li>
<li><strong>Multivariate Linnik distribution</strong>: Generate a multinormal random vector, then multiply each component by <code>GeometricStable(alpha/2.0, 1, 1)</code>, where <code>alpha</code> is a parameter in (0, 2] (Kozubowski 2000)<sup><a href="#Note14"><strong>(14)</strong></a></sup>.</li>
</ol>
</blockquote>

<p><a id=Gaussian_and_Other_Copulas></a></p>

<h4>Gaussian and Other Copulas</h4>

<p>A <em>copula</em> is a way to describe the dependence between randomly sampled numbers.</p>

<p>One example is a <em>Gaussian copula</em>; this copula is sampled by sampling from a <a href="#Multivariate_Normal_Multinormal_Distribution"><strong>multinormal distribution</strong></a>, then converting the resulting numbers to <em>dependent</em> uniform random values. In the following pseudocode, which implements a Gaussian copula:</p>

<ul>
<li>The parameter <code>covar</code> is the covariance matrix for the multinormal distribution.</li>
<li><code>erf(v)</code> is the <a href="https://en.wikipedia.org/wiki/Error_function"><strong>error function</strong></a> of the number <code>v</code> (see the appendix).</li>
</ul>

<p>&nbsp;</p>

<pre>METHOD GaussianCopula(covar)
   mvn=MultivariateNormal(nothing, covar)
   for i in 0...size(covar)
      // Apply the normal distribution&#39;s CDF
      // to get uniform numbers
      mvn[i] = (erf(mvn[i]/(sqrt(2)*sqrt(covar[i][i])))+1)*0.5
   end
   return mvn
END METHOD
</pre>

<p>Each of the resulting uniform random values will be in the interval [0, 1], and each one can be further transformed to any other probability distribution (which is called a <em>marginal distribution</em> or <em>marginal</em> here) by taking the quantile of that uniform number for that distribution (see &quot;<a href="https://peteroupc.github.io/randomfunc.html#Inverse_Transform_Sampling"><strong>Inverse Transform Sampling</strong></a>&quot;, and see also (Cario and Nelson 1997)<sup><a href="#Note15"><strong>(15)</strong></a></sup>.)</p>

<blockquote>
<p><strong>Note:</strong> The Gaussian copula is also known as the <em>normal-to-anything</em> method.</p>

<p><strong>Examples:</strong></p>

<ol>
<li>To generate two correlated uniform random values with a Gaussian copula, generate <code>GaussianCopula([[1, rho], [rho, 1]])</code>, where <code>rho</code> is the Pearson correlation coefficient, in the interval [-1, 1]. (Other correlation coefficients besides <code>rho</code> exist. For example, for a two-variable Gaussian copula, the <a href="https://en.wikipedia.org/wiki/Rank_correlation"><strong>Spearman correlation coefficient</strong></a> <code>srho</code> can be converted to <code>rho</code> by <code>rho = sin(srho * pi / 6) * 2</code>.  Other correlation coefficients, and other measures of dependence between randomly sampled numbers, are not further discussed in this document.)</li>
<li><p>The following example generates a two-dimensional random vector that follows a Gaussian copula with exponential marginals (<code>rho</code> is the Pearson correlation coefficient, and <code>rate1</code> and <code>rate2</code> are the rates of the two exponential marginals).</p>

<pre>METHOD CorrelatedExpo(rho, rate1, rate2)
   copula = GaussianCopula([[1, rho], [rho, 1]])
   // Transform to exponentials using that
   // distribution&#39;s quantile function
   return [-log1p(-copula[0]) / rate1,
     -log1p(-copula[1]) / rate2]
END METHOD
</pre></li>
<li><p>The <em><strong>T</strong>&ndash;Poisson hierarchy</em> (Knudson et al. 2021)<sup><a href="#Note16"><strong>(16)</strong></a></sup> is a way to generate N-dimensional Poisson-distributed random vectors via copulas.  Each of the N dimensions is associated with a parameter <code>lamda</code> and a marginal that must be a <em>continuous non-negative</em> probability distribution (one that takes on any of an uncountable number of non-negative values, such as any number 0 or greater).  To sample from the <strong>T</strong>&ndash;Poisson hierarchy&mdash;</p>

<ol>
<li>sample an N-dimensional random vector via a copula (such as <code>GaussianCopula</code>), producing an N-dimensional vector of correlated uniform numbers; then</li>
<li>for each component in the vector, replace it with that component&#39;s quantile for the corresponding marginal; then</li>
<li>for each component in the vector, replace it with <code>Poisson(lamda * c)</code>, where <code>c</code> is that component and <code>lamda</code> is the <code>lamda</code> parameter for the corresponding dimension.</li>
</ol>

<p>The following example implements the T-Poisson hierarchy using a Gaussian copula and exponential marginals.</p>

<pre>METHOD PoissonH(rho, rate1, rate2, lambda1, lambda2)
   vec = CorrelatedExpo(rho, rate1, rate2)
   return [Poisson(lambda1*vec[0]),Poisson(lambda2*vec[1])]
END METHOD
</pre></li>
</ol>
</blockquote>

<p>Other kinds of copulas describe different kinds of dependence between randomly sampled numbers.  Examples of other copulas are&mdash;</p>

<ul>
<li>the <strong>Fr&eacute;chet&ndash;Hoeffding upper bound copula</strong> <em>[x, x, ..., x]</em> (e.g., <code>[x, x]</code>), where <code>x = RNDRANGE(0, 1)</code>,</li>
<li>the <strong>Fr&eacute;chet&ndash;Hoeffding lower bound copula</strong> <code>[x, 1.0 - x]</code> where <code>x = RNDRANGE(0, 1)</code>,</li>
<li>the <strong>product copula</strong>, where each number is a separately generated <code>RNDRANGE(0, 1)</code> (indicating no dependence between the numbers), and</li>
<li>the <strong>Archimedean copulas</strong>, described by M. Hofert and M. M&auml;chler (2011)<sup><a href="#Note17"><strong>(17)</strong></a></sup>.</li>
</ul>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<ul>
<li><small><sup id=Note1>(1)</sup> Karney, C.F.F., &quot;<a href="https://arxiv.org/abs/1303.6257v2"><strong>Sampling exactly from the normal distribution</strong></a>&quot;, arXiv:1303.6257v2  [physics.comp-ph], 2014.</small></li>
<li><small><sup id=Note2>(2)</sup> Thomas, D., et al., &quot;Gaussian Random Number Generators&quot;, <em>ACM Computing Surveys</em> 39(4), 2007.</small></li>
<li><small><sup id=Note3>(3)</sup> Malik, J.S., Hemani, A., &quot;Gaussian random number generation: A survey on hardware architectures&quot;, <em>ACM Computing Surveys</em> 49(3), 2016.</small></li>
<li><small><sup id=Note4>(4)</sup> Du, Yusong, Baoying Fan, and Baodian Wei. &quot;An improved exact sampling algorithm for the standard normal distribution.&quot; Computational Statistics (2021): 1-17, also arXiv:2008.03855 [cs.DS].</small></li>
<li><small><sup id=Note5>(5)</sup> Micciancio, D. and Walter, M., &quot;Gaussian sampling over the integers: Efficient, generic, constant-time&quot;, in Annual International Cryptology Conference, August 2017 (pp. 455-485).</small></li>
<li><small><sup id=Note6>(6)</sup> Kabal, P., &quot;Generating Gaussian Pseudo-Random Variates&quot;, McGill University, 2000/2019.</small></li>
<li><small><sup id=Note7>(7)</sup> Thomas, D.B., 2014, May. FPGA Gaussian random number generators with guaranteed statistical accuracy. In <em>2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Computing Machines</em> (pp. 149-156).</small></li>
<li><small><sup id=Note8>(8)</sup> Luu, T., &quot;Fast and Accurate Parallel Computation of Quantile Functions for Random Number Generation&quot;, Dissertation, University College London, 2016.</small></li>
<li><small><sup id=Note9>(9)</sup> Thistleton, W., Marsh, J., et al., &quot;Generalized Box-Müller Method for Generating q-Gaussian Random Deviates&quot;, <em>IEEE Transactions on Information Theory</em> 53(12), 2007.</small></li>
<li><small><sup id=Note10>(10)</sup> Marsaglia, G., Tsang, W.W., &quot;A simple method for generating gamma variables&quot;, <em>ACM Transactions on Mathematical Software</em> 26(3), 2000.</small></li>
<li><small><sup id=Note11>(11)</sup> Liu, C., Martin, R., Syring, N., &quot;<a href="https://arxiv.org/abs/1302.1884v3"><strong>Simulating from a gamma distribution with small shape parameter</strong></a>&quot;, arXiv:1302.1884v3  [stat.CO], 2015.</small></li>
<li><small><sup id=Note12>(12)</sup> A. Stuart, &quot;Gamma-distributed products of independent random variables&quot;, <em>Biometrika</em> 49, 1962.</small></li>
<li><small><sup id=Note13>(13)</sup> Devroye, L., <a href="http://luc.devroye.org/rnbookindex.html"><strong><em>Non-Uniform Random Variate Generation</em></strong></a>, 1986.</small></li>
<li><small><sup id=Note14>(14)</sup> Tomasz J. Kozubowski, &quot;<a href="https://www.sciencedirect.com/science/article/pii/S0377042799003180"><strong>Computer simulation of geometric stable distributions</strong></a>&quot;, <em>Journal of Computational and Applied Mathematics</em> 116(2), pp. 221-229, 2000. <a href="https://doi.org/10.1016/S0377-0427%2899%2900318-0"><strong>https://doi.org/10.1016/S0377-0427(99)00318-0</strong></a></small></li>
<li><small><sup id=Note15>(15)</sup> Cario, M. C., B. L. Nelson, &quot;Modeling and generating random vectors with arbitrary marginal distributions and correlation matrix&quot;, 1997.</small></li>
<li><small><sup id=Note16>(16)</sup> Knudson, A.D., Kozubowski, T.J., et al., &quot;A flexible multivariate model for high-dimensional correlated count data&quot;, <em>Journal of Statistical Distributions and Applications</em> 8:6, 2021.</small></li>
<li><small><sup id=Note17>(17)</sup> Hofert, M., and Maechler, M.  &quot;Nested Archimedean Copulas Meet R: The nacopula Package&quot;.  <em>Journal of Statistical Software</em> 39(9), 2011, pp. 1-20.</small></li>
<li><small><sup id=Note18>(18)</sup> Devroye, L., Gravel, C., &quot;<a href="https://arxiv.org/abs/1502.02539v6"><strong>Random variate generation using only finitely many unbiased, independently and identically distributed random bits</strong></a>&quot;, arXiv:1502.02539v6  [cs.IT], 2020.</small></li>
<li><small><sup id=Note19>(19)</sup> Oberhoff, Sebastian, &quot;<a href="https://dc.uwm.edu/etd/1888"><strong>Exact Sampling and Prefix Distributions</strong></a>&quot;, <em>Theses and Dissertations</em>, University of Wisconsin Milwaukee, 2018.</small></li>
</ul>

<p><a id=Appendix></a></p>

<h2>Appendix</h2>

<p><a id=Implementation_of_erf></a></p>

<h3>Implementation of <code>erf</code></h3>

<p>The pseudocode below shows an approximate implementation of the <a href="https://en.wikipedia.org/wiki/Error_function"><strong>error function</strong></a> <code>erf</code>, in case the programming language used doesn&#39;t include a built-in version of <code>erf</code> (such as JavaScript at the time of this writing).   In the pseudocode, <code>EPSILON</code> is a very small number to end the iterative calculation.</p>

<pre>METHOD erf(v)
    if v==0: return 0
    if v&lt;0: return -erf(-v)
    if v==infinity: return 1
    // NOTE: For Java `double`, the following
    // line can be added:
    // if v&gt;=6: return 1
    i=1
    ret=0
    zp=-(v*v)
    zval=1.0
    den=1.0
    while i &lt; 100
        r=v*zval/den
        den=den+2
        ret=ret+r
        // NOTE: EPSILON can be pow(10,14),
        // for example.
        if abs(r)&lt;EPSILON: break
        if i==1: zval=zp
        else: zval = zval*zp/i
        i = i + 1
    end
    return ret*2/sqrt(pi)
END METHOD
</pre>

<p><a id=Exact_Error_Bounded_and_Approximate_Algorithms></a></p>

<h3>Exact, Error-Bounded, and Approximate Algorithms</h3>

<p>There are three kinds of randomization algorithms:</p>

<ol>
<li><p>An <em>exact algorithm</em> is an algorithm that samples from the exact distribution requested, assuming that computers&mdash;</p>

<ul>
<li>can store and operate on real numbers (which have unlimited precision), and</li>
<li>can generate independent uniform random real numbers</li>
</ul>

<p>(Devroye 1986, p. 1-2)<sup><a href="#Note13"><strong>(13)</strong></a></sup>.  However, an exact algorithm implemented on real-life computers can incur error due to the use of fixed precision, such as rounding and cancellations, especially when floating-point numbers are involved. An exact algorithm can achieve a guaranteed bound on accuracy (and thus be an <em>error-bounded algorithm</em>) using either arbitrary-precision or interval arithmetic (see also Devroye 1986, p. 2)<sup><a href="#Note13"><strong>(13)</strong></a></sup>. All methods given on this page are exact unless otherwise noted.  Note that the <code>RNDRANGE</code> method is exact in theory, but has no required implementation.</p></li>
<li><p>An <em>error-bounded algorithm</em> is a sampling algorithm with the following requirements:</p>

<ul>
<li>If the ideal distribution is discrete (takes on a countable number of values), the algorithm samples exactly from that distribution.</li>
<li>If the ideal distribution is continuous, the algorithm samples from a distribution that is close to the ideal within a user-specified error tolerance (see below for details).  The algorithm can instead sample a number from the distribution only partially, as long as the fully sampled number can be made close to the ideal within any error tolerance desired.</li>
<li>In sampling from a distribution, the algorithm incurs no approximation error not already present in the inputs (except errors needed to round the final result to the user-specified error tolerance).</li>
</ul>

<p>Many error-bounded algorithms use random bits as their only source of randomness. An application should use error-bounded algorithms whenever possible.</p></li>
<li>An <em>inexact</em>, <em>approximate</em>, or <em>biased algorithm</em> is neither exact nor error-bounded; it uses &quot;a mathematical approximation of sorts&quot; to sample from a distribution that is close to the desired distribution (Devroye 1986, p. 2)<sup><a href="#Note13"><strong>(13)</strong></a></sup>.  An application should use this kind of algorithm only if it&#39;s willing to trade accuracy for speed.</li>
</ol>

<p>Most algorithms on this page, though, are not <em>error-bounded</em>, but even so, they may still be useful to an application willing to trade accuracy for speed.</p>

<p>There are many ways to describe closeness between two distributions.  One suggestion by Devroye and Gravel (2020)<sup><a href="#Note18"><strong>(18)</strong></a></sup> is Wasserstein distance (or &quot;earth-mover distance&quot;).  Here, an algorithm has accuracy &epsilon; (the user-specified error tolerance) if it samples from a distribution that is close to the ideal distribution by a Wasserstein distance of not more than &epsilon;.</p>

<blockquote>
<p><strong>Examples:</strong></p>

<ol>
<li>Sampling from the exponential distribution via <code>-ln(RNDRANGE(0, 1))</code> is an <em>exact algorithm</em> (in theory), but not an <em>error-bounded</em> one for common floating-point number formats.  The same is true of the Box&ndash;Muller transformation.</li>
<li>Sampling from the exponential distribution using the <code>ExpoExact</code> method in the page &quot;<a href="https://peteroupc.github.io/randmisc.html#ExpoExact"><strong>Miscellaneous Observations on Randomization</strong></a>&quot; is an <em>error-bounded algorithm</em>.  Karney&#39;s algorithm for the normal distribution (Karney 2014)<sup><a href="#Note1"><strong>(1)</strong></a></sup> is also error-bounded because it returns a result that can be made to come close to the normal distribution within any error tolerance desired simply by appending more random digits to the end.  See also (Oberhoff 2018)<sup><a href="#Note19"><strong>(19)</strong></a></sup>.</li>
<li>Examples of <em>approximate algorithms</em> include sampling from a Gaussian-like distribution via a sum of <code>RNDRANGE(0, 1)</code>, or most cases of modulo reduction to produce uniform-like integers at random (see notes in the section &quot;<a href="https://peteroupc.github.io/randomfunc.html#RNDINT_Random_Integers_in_0_N"><strong>RNDINT</strong></a>&quot;).</li>
</ol>
</blockquote>

<p><a id=License></a></p>

<h2>License</h2>

<p>Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under <a href="https://creativecommons.org/publicdomain/zero/1.0/"><strong>Creative Commons Zero</strong></a>.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>

<div class="noprint">
<p>
<a href="//twitter.com/intent/tweet">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
if("share" in navigator){
 document.getElementById("sharer").href="javascript:void(null)";
 document.getElementById("sharer").innerHTML="Share This Page";
 navigator.share({title:document.title,url:document.location.href}).then(
   function(){});
} else {
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
}
</script>
</body></html>
