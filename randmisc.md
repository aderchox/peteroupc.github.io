# Miscellaneous Observations on Randomization

[**Peter Occil**](mailto:poccil14@gmail.com)

<a id=On_a_Binomial_Sampler></a>
## On a Binomial Sampler

Take the following sampler of a binomial(_n_, 1/2) distribution (where _n_ is even), which is equivalent to the one that appeared in (Bringmann et al. 2014)<sup>[**(1)**](#Note1)</sup>, and adapted to be more programmer-friendly.

1. Set _m_ to floor(sqrt(_n_)) + 1.
2. (First, sample from an envelope of the binomial curve.) Generate unbiased random bits (zeros or ones) until a zero is generated this way.  Set _k_ to the number of ones generated this way.
3. Set _s_ to an integer in [0, _m_) chosen uniformly at random, then set _i_ to _k_\*_m_ + _s_.
4. Set _ret_ to either _n_/2+_i_ or _n_/2&minus;_i_&minus;1 with equal probability.
5. (Second, accept or reject _ret_.) If _ret_ < 0 or _ret_ > _n_, go to step 2.
6. With probability choose(_n_, _ret_)\*_m_\*2<sup>_k_&minus;(_n_+2)</sup>, return _ret_.  Otherwise, go to step 2. (Here, choose(_n_, _k_) is a binomial coefficient.<sup>[**(2)**](#Note2)</sup>)

This algorithm has an acceptance rate of 1/16 regardless of the value of _n_.  However, step 6 will generally require a growing amount of storage and time to exactly calculate the given probability as _n_ gets large, notably due to the inherent factorial in the binomial coefficient.  The Bringmann paper suggests approximating this factorial via Spouge's approximation; however, it seems hard to do so without using floating-point arithmetic, which the paper ultimately resorts to. Alternatively, the logarithm of that probability can be calculated that is much more economical in terms of storage than the full exact probability.  Then, an exponential random number can be generated, negated, and compared with that logarithm to determine whether the step succeeds.

More specifically, step 6 can be changed as follows:

- (6.) Let _p_ be loggamma(_n_+1)&minus;loggamma(_k_+1)&minus;loggamma((_n_&minus;_k_)+1)+ln(_m_)+ln(2)\*_k_&minus;(_n_+2) (where loggamma(_x_) is the logarithm of the gamma function).
- (6a.) Generate an exponential random number with rate 1 (which is the negative natural logarithm of a uniform(0,1) random number).  Set _e_ to 0 minus that number.
- (6b.) If _e_ is greater than _p_, go to step 2.  Otherwise, return _ret_. (This step can be replaced by calculating lower and upper bounds that converge to _p_.  In that case, go to step 2 if _e_ is greater than the upper bound, or return _ret_ if _e_ is less than the lower bound, or compute better bounds and repeat this step otherwise.  See also chapter 4 of (Devroye 1986)<sup>[**(3)**](#Note3)</sup>.)

My implementation of loggamma and the natural logarithm ([**interval.py**](https://peteroupc.github.io/interval.py)) relies on rational interval arithmetic (Daumas et al. 2007)<sup>[**(4)**](#Note4)</sup> and a fast converging version of Stirling's formula for the factorial's natural logarithm (Schumacher 2016)<sup>[**(5)**](#Note5)</sup>.

Also, according to the Bringmann paper, _m_ can be set such that _m_ is in the interval \[sqrt(_n_), sqrt(_n_)+3\], so I implement step 1 by starting with _u_ = 2<sup>floor((1+ceil(log2(_n_+1)))/2)</sup>, then calculating _v_ = floor(_u_+floor(_n_/_u_)/2), _w_ = _u_, _u_ = _v_  until _v_ >= _w_, then setting _m_ to _w_ + 1.

> **Notes:**
>
> - A binomial(_n_, 1/2) random number, where _n_ is odd, can be generated by adding an unbiased random bit (zero or one) to a binomial(_n_&minus;1, 1/2) random number.
> - As pointed out by Farach-Colton and Tsai (2015)<sup>[**(6)**](#Note6)</sup>, a binomial(_n_, _p_) random number, where _p_ is in the interval (0, 1), can be generated using binomial(_n_, 1/2) numbers using a procedure equivalent to the following:
>     1. Set _k_ to 0 and _ret_ to 0.
>     2. If the binary digit at position _k_ after the point in _p_'s binary expansion (that is, 0.bbbb... where each b is a zero or one) is 1, add a binomial(_n_, 1/2) random number to _ret_ and subtract the same random number from _n_; otherwise, set _n_ to a binomial(_n_, 1/2) random number.
>     3. If _n_ is greater than 0, add 1 to _k_ and go to step 2; otherwise, return _ret_. (Positions start at 0 where 0 is the most significant digit after the point, 1 is the next, etc.)

<a id=On_a_Geometric_Sampler></a>
## On a Geometric Sampler

The following algorithm is equivalent to the geometric(_px_/_py_) sampler that appeared in (Bringmann and Friedrich 2013)<sup>[**(7)**](#Note7)</sup>, but adapted to be more programmer-friendly.  As used in that paper, a geometric(_p_) random number expresses the number of failing trials before the first success, where each trial is independent and has success probability _p_. (Note that the terminology "geometric random number" has conflicting meanings in academic works.  Note also that the algorithm uses the rational number _px_/_py_, not an arbitrary real number _p_; some of the notes in this section indicate how to adapt the algorithm to an arbitrary _p_.)

1. Set _pn_ to _px_, _k_ to 0, and _d_ to 0.
2. While _pn_\*2 <= _py_, add 1 to _k_ and multiply _pn_ by 2.  (Equivalent to finding the largest _k_ >= 0 such that _p_\*2<sup>_k_</sup> <= 1.  For the case when _p_ need not be rational, enough of its binary expansion can be calculated to carry out this step accurately, but in this case any _k_ such that _p_ is greater than 1/(2<sup>_k_+2</sup>) and less than or equal to 1/(2<sup>_k_</sup>) will suffice, as the Bringmann paper points out.)
3. With probability (1&minus;_px_/_py_)<sup>2<sup>_k_</sup></sup>, add 1 to _d_ and repeat this step. (To simulate this probability, the first sub-algorithm below can be used.)
4. Generate a uniform random integer in [0, 2<sup>_k_</sup>), call it _m_, then with probability (1&minus;_px_/_py_)<sup>_m_</sup>, return _d_\*2<sup>_k_</sup>+_m_. (The Bringmann paper, though, suggests to simulate this probability by sampling only as many bits of _m_ as needed to do so, rather than just generating _m_ in one go, then using the first sub-algorithm on _m_.  However, the implementation, given as the second sub-algorithm below, is much more complicated and is not crucial for correctness.)

The first sub-algorithm returns 1 with probability (1&minus;_px_/_py_)<sup>_n_</sup>, assuming that _n_\*_px_/_py_ <= 1.  It implements the approach from the Bringmann paper by rewriting the probability using the binomial theorem. (For the case when _p_ need not be rational, the probability (1&minus;_p_)<sup>_n_</sup> can be simulated using _Bernoulli factory_ algorithms, or by calculating its digit expansion or series expansion and using the appropriate algorithm for [**simulating irrational constants**](https://peteroupc.github.io/bernoulli.html#Algorithms_for_Irrational_Constants). Run that algorithm _n_ times or until it outputs 1, whichever comes first.  This sub-algorithm returns 1 if all the runs return 0, or 1 otherwise.)

1. Set _pnum_, _pden_, and _j_  to 1, then set _r_ to 0, then set _qnum_ to _px_, and _qden_ to _py_, then set _i_ to 2.
2. If _j_ is greater than _n_, go to step 5.
3. If _j_ is even, set _pnum_ to _pnum_\*_qden_ + _pden_\*_qnum_\*choose(_n_,_j_). Otherwise, set _pnum_ to _pnum_\*_qden_ &minus; _pden_\*_qnum_\*choose(_n_,_j_).
4. Multiply _pden_ by _qden_, then multiply _qnum_ by _px_, then multiply _qden_ by _py_, then add 1 to _j_.
5. If _j_ is less than or equal to 2 and less than or equal to _n_, go to step 2.
6. Multiply _r_ by 2, then add an unbiased random bit (either 0 or 1 with equal probability) to _r_.
7. If _r_ <= floor((_pnum_\*_i_)/_pden_) &minus; 2, return 1. If _r_ >= floor((_pnum_\*_i_)/_pden_) + 1, return 0.  If neither is the case, multiply _i_ by 2 and go to step 2.

The second sub-algorithm returns an integer _m_ in [0, 2<sup>_k_</sup>) with probability (1&minus;_px_/_py_)<sup>_m_</sup>, or &minus;1 with the opposite probability.  It assumes that 2<sup>_k_</sup>\*_px_/_py_ <= 1.

1. Set _r_ and _m_ to 0.
2. Set _b_ to 0, then while _b_ is less than _k_:
    1. (Sum _b_+2 summands of the binomial equivalent of the desired probability.  First, append an additional bit to _m_, from most to least significant.) Generate either 0 or 2<sup>_k_&minus;_b_</sup> with equal probability, then add that number to _m_.
    2. (Now build up the binomial probability.) Set _pnum_, _pden_, and _j_  to 1, then set _qnum_ to _px_, and _qden_ to _py_.
    3. If _j_ is greater than _m_ or greater than _b_ + 2, go to the sixth substep.
    4. If _j_ is even, set _pnum_ to _pnum_\*_qden_ + _pden_\*_qnum_\*choose(_m_,_j_). Otherwise, set _pnum_ to _pnum_\*_qden_ &minus; _pden_\*_qnum_\*choose(_m_,_j_).
    5. Multiply _pden_ by _qden_, then multiply _qnum_ by _px_, then multiply _qden_ by _py_, then add 1 to _j_, then go to the third substep.
    6. (Now check the probability.) Multiply _r_ by 2, then add an unbiased random bit (either 0 or 1 with equal probability) to _r_.
    7. If _r_ <= floor((_pnum_\*2<sup>_b_</sup>)/_pden_) &minus; 2, add a uniform random integer in [0, 2<sup>_k_\*_b_</sup>) to _m_ and return _m_ (and, if requested, the number _k_&minus;_b_&minus;1). If _r_ >= floor((_pnum_\*2<sup>_b_</sup>)/_pden_) + 1, return &minus;1 (and, if requested, an arbitrary value).  If neither is the case, add 1 to _b_.
8. Add an unbiased random bit to _m_. (At this point, _m_ is fully sampled.)
9. Run the first sub-algorithm with _n_ = _m_, except in step 1 of that sub-algorithm, set _r_ to the value of _r_ built up by this algorithm, rather than 0, and set _i_ to 2<sup>_k_</sup>, rather than 2.  If that sub-algorithm returns 1, return _m_ (and, if requested, the number &minus;1).  Otherwise, return &minus;1 (and, if requested, an arbitrary value).

As used in the Bringmann paper, a bounded geometric(_p_, _n_) random number is a geometric(_p_) random number or _n_ (an integer greater than 0), whichever is less.  The following algorithm is equivalent to the algorithm given in that paper, but adapted to be more programmer-friendly.

1. Set _pn_ to _px_, _k_ to 0, _d_ to 0, and _m2_ to the smallest power of 2 that is greater than _n_ (or equivalently, 2<sup>_bits_</sup> where _bits_ is the minimum number of bits needed to store _n_).
2. While _pn_\*2 <= _py_, add 1 to _k_ and multiply _pn_ by 2.
3. With probability (1&minus;_px_/_py_)<sup>2<sup>_k_</sup></sup>, add 1 to _d_ and then either return _n_ if _d_\*2<sup>_k_</sup> is greater than or equal to _m2_, or repeat this step if less. (To simulate this probability, the first sub-algorithm above can be used.)
4. Generate a uniform random integer in [0, 2<sup>_k_</sup>), call it _m_, then with probability (1&minus;_px_/_py_)<sup>_m_</sup>, return min(_n_, _d_\*2<sup>_k_</sup>+_m_). In the Bringmann paper, this step is implemented in a manner equivalent to the following (this alternative implementation, though, is not crucial for correctness):
    1. Run the second sub-algorithm above, except return two values, rather than one, in the situations given in the sub-algorithm.  Call these two values _m_ and _mbit_.
    2. If _m_ < 0, go to the first substep.
    3. If _mbit_ >= 0, add 2<sup>_mbit_</sup> times an unbiased random bit to _m_ and subtract 1 from _mbit_.  If that bit is 1 or _mbit_ < 0, go to the next substep; otherwise, repeat this substep.
    4. Return _n_ if _d_\*2<sup>_k_</sup> is greater than or equal to _m2_.
    5. Add a uniform random integer in [0, 2<sup>_mbit_+1</sup>) to _m_, then return min(_n_, _d_\*2<sup>_k_</sup>+_m_).

<a id=Sampling_Unbounded_Monotone_Density_Functions></a>
## Sampling Unbounded Monotone Density Functions

This section shows a preprocessing algorithm to generate a random number in [0, 1] from a distribution whose probability density function (PDF)&mdash;

- is continuous in the interval [0, 1],
- is monotonically decreasing in [0, 1], and
- has an unbounded peak at 0.

The trick here is to sample the peak in such a way that the result is either forced to be 0 or forced to belong to the bounded part of the PDF.  This algorithm does not require the area under the curve of the PDF in [0, 1] to be 1; in other words, this algorithm works even if the PDF is known up to a normalizing constant.  The algorithm is as follows.

1. Set _i_ to 1.
2. Calculate the cumulative probability of the interval [0, 2<sup>&minus;_i_</sup>] and that of [0, 2<sup>&minus;(_i_ &minus; 1)</sup>], call them _p_ and _t_, respectively.
3. With probability _p_/_t_, add 1 to _i_ and go to step 2. (Alternatively, if _i_ is equal to or higher than the desired number of fractional bits in the result, return 0 instead of adding 1 and going to step 2.)
4. At this point, the PDF at [2<sup>&minus;_i_</sup>, 2<sup>&minus;(_i_ &minus; 1)</sup>) is bounded from above, so sample a random number in this interval using any appropriate algorithm, including rejection sampling.  Because the PDF is monotonically decreasing, the peak of the PDF at this interval is located at 2<sup>&minus;_i_</sup>, so that rejection sampling becomes trivial.

It is relatively straightforward to adapt this algorithm for monotonically increasing PDFs with the unbounded peak at 1, or to PDFs with a different domain than \[0, 1\].

This algorithm is similar to the "inversion-rejection" algorithm mentioned in section 4.4 of chapter 7 of Devroye's _Non-Uniform Random Variate Generation_ (1986)<sup>[**(3)**](#Note3)</sup>.  I was unaware of that algorithm at the time I started writing the text that became this section (Jul. 25, 2020).  The difference here is that it assumes the whole distribution (including its PDF and cumulative distribution function) is supported on the interval [0, 1], while the algorithm presented in this article doesn't make that assumption (e.g., the interval [0, 1] can cover only part of the PDF's support).

By the way, this algorithm arose while trying to devise an algorithm that can generate an integer power of a uniform random number, with arbitrary precision, without actually calculating that power (a naïve calculation that is merely an approximation and usually introduces bias); for more information, see my other article on [**partially-sampled random numbers**](https://peteroupc.github.io/exporand.html).  Even so, the algorithm I have come up with in this note may be of independent interest.

In the case of powers of a uniform \[0, 1\] random number _X_, namely _X_<sup>_n_</sup>, the ratio _p_/_t_ in this algorithm has a very simple form, namely (1/2)<sup>1/_n_</sup>, which is possible to simulate using a so-called _Bernoulli factory_ algorithm without actually having to calculate this ratio.  Note that this formula is the same regardless of _i_.  This is found by taking the PDF f(_x_) = _x_<sup>1/_n_</sup>/(_x_ * _n_)</sup> and finding the appropriate _p_/_t_ ratios by integrating _f_ over the two intervals mentioned in step 2 of the algorithm.

<a id=Certain_Families_of_Distributions></a>
## Certain Families of Distributions

This section is a note on certain families of univariate (one-variable) distributions of random numbers, with
emphasis on sampling random numbers from them.  Some of these families are described in Ahmad et al. (2019)<sup>[**(8)**](#Note8)</sup>.

In general, families of the form "X-G" (such as "beta-G" (Eugene et al., 2002)<sup>[**(9)**](#Note9)</sup>) use two distributions, X and G, where X is a continuous distribution supported on the interval \[0, 1\] and G is a distribution with an easy-to-compute quantile function (also known as inverse cumulative distribution function or inverse CDF).  The following algorithm samples a random number following a distribution from this kind of family:

1. Generate a random number that follows the distribution X. (Or generate a uniform [**partially-sampled random number (PSRN)**](https://peteroupc.github.io/exporand.html) that follows the distribution X.)  Call the number _x_.
2. Calculate the quantile for G of _x_, and return that quantile. (If _x_ is a uniform PSRN, see the note at the end of this section.)

Certain special cases of the "X-G" families, such as the following, use a specially designed distribution for X:

- The _alpha power_ or _alpha power transformed_ family (Mahdavi and Kundu 2017)<sup>[**(10)**](#Note10)</sup>. The family uses a shape parameter _&alpha;_ > 0, and the algorithm for the "X-G" families is used, except step 1 now reads: "Generate a uniform(0, 1) random number _U_, then set _x_ to ln((_&alpha;_&minus;1)\*_U_ + 1)/ln(_&alpha;_) if _&alpha;_ != 1, and _U_ otherwise."
- The _exponentiated_ family (Mudholkar and Srivastava 1993)<sup>[**(11)**](#Note11)</sup>. The family uses a shape parameter _a_ > 1; step 1 is modified to read: "Generate a uniform(0, 1) random number _U_, then set _x_ to _U_<sup>1/_a_</sup>."
- The _transmuted-G_ family (described, for example, by Tahir and Cordeiro (2016)<sup>[**(12)**](#Note12)</sup>). The family uses a shape parameter _&eta;_ in the interval [&minus;1, 1]; step 1 is modified to read: "Generate a piecewise linear random number in [0, 1] with weight 1&minus;_&eta;_ at 0 and weight 1+_&eta;_ at 1, call the number _x_."

In fact, the "X-G" families are a special case of the so-called "transformed&ndash;transformer" family of distributions introduced by Alzaatreh et al. (2013)<sup>[**(13)**](#Note13)</sup> that uses two distributions, X and G, where X (the "transformed") is an arbitrary continuous distribution, G (the "transformer") is a distribution with an easy-to-compute quantile function, and _W_ is a nondecreasing function that maps a number in [0, 1] to a number with the same support as X and meets certain other conditions.  The following algorithm samples a random number from this kind of family:

1. Generate a random number that follows the distribution X. (Or generate a uniform PSRN that follows X.) Call the number _x_.
2. Calculate the quantile for G of _W_<sup>&minus;1</sup>(_x_) (where _W_<sup>&minus;1</sup>(.) is the inverse of _W_), and return that quantile. (If _x_ is a uniform PSRN, see the note at the end of this section.)

The following are special cases of the "transformed&ndash;transformer" family:

- The "T-R{_Y_}" family (Aljarrah et al., 2014)<sup>[**(14)**](#Note14)</sup>, in which _T_ is an arbitrary continuous distribution (X in the algorithm above), _R_ is a distribution with an easy-to-compute quantile function (G in the algorithm above), and _W_ is the quantile function for the distribution _Y_, whose support must be included in the support of _T_ (so that _W_<sup>&minus;1</sup>(_x_) is the CDF for _Y_).
- Several versions of _W_ have been proposed for the case when distribution X is supported on \[0, &infin;\), such as the Rayleigh and gamma distributions.  They include:
    - _W_(_x_) = &minus;ln(1&minus;_x_) (_W_<sup>&minus;1</sup>(_x_) = 1&minus;exp(&minus;_x_)).  Suggested in the original paper by Alzaatreh et al.
    - _W_(_x_) = _x_/(1&minus;_x_) (_W_<sup>&minus;1</sup>(_x_) = _x_/(1+_x_)).  Suggested in the original paper by Alzaatreh et al.  This choice forms the so-called "odd X G" family, examples of which include the "odd log-logistic G" family (Gleaton and Lynch 2006)<sup>[**(15)**](#Note15)</sup> and the "generalized odd Weibull generated" family (where X is the Weibull distribution and G is arbitrary) (Korkmaz et al. 2018)<sup>[**(16)**](#Note16)</sup>.

Many special cases of the "transformed&ndash;transformer" family have been proposed in many papers, and usually their names suggest the distributions that make up this family.  Some members of the "odd X G" family have names that begin with the word "generalized", and in most such cases this corresponds to _W_<sup>&minus;1</sup>(_x_) = (_x_/(1+_x_))<sup>1/_a_</sup>, where _a_ > 0 is a shape parameter; an example is the "generalized odd gamma-G" family (Hosseini et al. 2018)<sup>[**(17)**](#Note17)</sup>.

A family very similar to the "transformed&ndash;transformer" family uses a _decreasing_ _W_.  When distribution X is supported on \[0, &infin;), one such _W_ that has been proposed is _W_(_x_) = &minus;ln(_x_) (_W_<sup>&minus;1</sup>(_x_) = exp(&minus;_x_); examples include the "Rayleigh-G" family or "Rayleigh&ndash;Rayleigh" distribution (Al Noor and Assi 2020)<sup>[**(18)**](#Note18)</sup>, as well as the "generalized gamma-G" family, where "generalized gamma" refers to the Stacy distribution (Boshi et al. 2020)<sup>[**(19)**](#Note19)</sup>).

A _compound distribution_ is simply the minimum of _N_ random variables distributed as _X_, where _N_ >= 1 is an integer distributed as the discrete distribution _Y_ (Tahir and Cordeiro 2016)<sup>[**(12)**](#Note12)</sup>.  For example, the "beta-G-geometric" family represents the minimum of _N_ beta-G random variables, where _N_ is a random number expressing 1 plus the number of failures before the first success, with each success having the same probability.

A _complementary compound distribution_ is the maximum of _N_ random variables distributed as _X_, where _N_ >= 1 is an integer distributed as the discrete distribution _Y_.  An example is the "geometric zero-truncated Poisson distribution", where _X_ is the distribution of 1 plus the number of failures before the first success, with each success having the same probability, and _Y_ is the zero-truncated Poisson distribution (Akdoğan et al., 2020)<sup>[**(20)**](#Note20)</sup>.

An _inverse X distribution_ (or _inverted X distribution_) is generally the distribution of the reciprocal of a random number distributed as _X_.  But an _inverse exponential distribution_ (Keller and Kamath 1982)<sup>[**(21)**](#Note21)</sup> is distributed as &minus;_&theta;_/ln(_U_) where _&theta;_ > 0 and _U_ is a uniform(0, 1) random number.

A _weight-biased X_ or _weighted X distribution_ uses a distribution X and a weight function _w_(_x_) whose values lie in [0, 1] everywhere in X's support.  The following algorithm samples from a weighted distribution (see also (Devroye 1986, p. 47)<sup>[**(3)**](#Note3)</sup>):

1. Generate a random number that follows the distribution X. (Or generate a uniform PSRN that follows X.) Call the number _x_.
2. With probability _w_(_x_), return _x_.  Otherwise, go to step 1.

> **Note**: This is a note on quantile generation using uniform [**partially-sampled random numbers (PSRNs)**](https://peteroupc.github.io/exporand.html).
>
> A uniform PSRN is ultimately a number that lies in an interval \[_a_, _b_\].  Let G be a distribution for which the quantile is wanted, and let _f_(.) be a function applied to _a_ or _b_ before calculating the quantile.  When a random number _x_ is a uniform PSRN, then to implement this quantile calculation (see (Devroye and Gravel 2020)<sup>[**(22)**](#Note22)</sup>):
>
> 1. Generate additional digits of _x_ uniformly at random&mdash;thus shortening the interval \[_a_, _b_\]&mdash;until a lower bound of the quantile of _f_(_a_) and an upper bound of the quantile of _f_(_b_) differ by no more than 2\*_&epsilon;_, where _&epsilon;_ is the desired accuracy.  Call the two bounds _low_ and _high_, respectively.
> 2. Return (_low_+_high_)/2.
>
> The disadvantage is that the desired accuracy has to be made known to the algorithm in advance.  To generate a quantile to any accuracy (even if the accuracy is not known in advance), a rejection sampling approach is needed, which requires knowing G's probability density function or a function proportional to it, and that the density function must be continuous almost everywhere and bounded from above (see also (Devroye and Gravel 2020)<sup>[**(22)**](#Note22)</sup>).  This involves calculating lower and upper bounds of the quantiles of _f_(_a_) and _f_(_b_) (the bounds are \[_alow_, _ahigh_\] and \[_blow_, _bhigh_\] respectively) and applying an arbitrary-precision rejection sampler such as Oberhoff's method (described in an [**appendix to the PSRN article**](https://peteroupc.github.io/exporand.html#Oberhoff_s_Exact_Rejection_Sampling_Method)) to the distribution G limited to the interval \[_alow_, _bhigh_\] and accepting the resulting PSRN if it clearly lies in \[_ahigh_, _blow_\] or rejecting it if it clearly lies outside \[_alow_, _bhigh_\].  When neither of these is the case, then it gets more complicated; more digits of the input or output PSRN have to be generated (uniformly at random) until it's clear whether to accept or reject the output PSRN.

<a id=Certain_Distributions></a>
## Certain Distributions

A _power function(a, c) distribution_ is distributed as _c_\*_U_<sup>1/_a_</sup>, where _U_ is a uniform(0,1) random number, _a_ > 0, and _c_ is a scale parameter greater than 0.

A _right-truncated Weibull(a, b, c) distribution_ (truncated at _c_) is distributed as the minimum of _N_ power function(_b_, _c_) random variables, where _N_ is distributed as the zero-truncated Poisson(_a_\*_c_<sup>_b_</sup>) distribution.  (Jodrá 2020)<sup>[**(23)**](#Note23)</sup>.

<a id=Notes></a>
## Notes

- <small><sup id=Note1>(1)</sup> K. Bringmann, F. Kuhn, et al., “Internal DLA: Efficient Simulation of a Physical Growth Model.” In: _Proc. 41st International Colloquium on Automata, Languages, and Programming (ICALP'14)_, 2014.</small>
- <small><sup id=Note2>(2)</sup> choose(_n_, _k_) = _n_!/(_k_! * (_n_ &minus; _k_)!) is a binomial coefficient.  It can be calculated, for example, by calculating _i_/(_n_&minus;_i_+1) for each integer _i_ in \[_n_&minus;_k_+1, _n_\], then multiplying the results (Yannis Manolopoulos. 2002. "[**Binomial coefficient computation: recursion or iteration?**](https://doi.org/10.1145/820127.820168)", SIGCSE Bull. 34, 4 (December 2002), 65–67).  Note that for all _m_>0, choose(_m_, 0) = choose(_m_, _m_) = 1 and choose(_m_, 1) = choose(_m_, _m_&minus;1) = _m_.</small>
- <small><sup id=Note3>(3)</sup> Devroye, L., [**_Non-Uniform Random Variate Generation_**](http://luc.devroye.org/rnbookindex.html), 1986.</small>
- <small><sup id=Note4>(4)</sup> Daumas, M., Lester, D., Muñoz, C., "[**Verified Real Number Calculations: A Library for Interval Arithmetic**](https://arxiv.org/abs/0708.3721)", arXiv:0708.3721 [cs.MS], 2007.</small>
- <small><sup id=Note5>(5)</sup> R. Schumacher, "[**Rapidly Convergent Summation Formulas involving Stirling Series**](https://arxiv.org/abs/1602.00336v1)", arXiv:1602.00336v1 [math.NT], 2016.</small>
- <small><sup id=Note6>(6)</sup> Farach-Colton, M. and Tsai, M.T., 2015. Exact sublinear binomial sampling. _Algorithmica_ 73(4), pp. 637-651.</small>
- <small><sup id=Note7>(7)</sup> Bringmann, K., and Friedrich, T., 2013, July. Exact and efficient generation of geometric random variates and random graphs, in _International Colloquium on Automata, Languages, and Programming_ (pp. 267-278).</small>
- <small><sup id=Note8>(8)</sup> Ahmad, Z. et al. "Recent Developments in Distribution Theory: A Brief Survey and Some New Generalized Classes of distributions." Pakistan Journal of Statistics and Operation Research 15 (2019): 87-110.</small>
- <small><sup id=Note9>(9)</sup> Eugene, N., Lee, C., Famoye, F., "Beta-normal distribution and its applications", _Commun. Stat. Theory Methods_ 31, 2002.</small>
- <small><sup id=Note10>(10)</sup> Mahdavi, Abbas, and Debasis Kundu. "A new method for generating distributions with an application to exponential distribution." Communications in Statistics-Theory and Methods 46, no. 13 (2017): 6543-6557.</small>
- <small><sup id=Note11>(11)</sup> Mudholkar, G. S., Srivastava, D. K., "Exponentiated Weibull family for analyzing bathtub failure-rate data", _IEEE Transactions on Reliability 42(2), 299-302, 1993.</small>
- <small><sup id=Note12>(12)</sup> Tahir, M.H., Cordeiro, G.M., "Compounding of distributions: a survey and new generalized classes", _Journal of Statistical Distributions and Applications_ 3(13), 2016.</small>
- <small><sup id=Note13>(13)</sup> Alzaatreh, A., Famoye, F., Lee, C., "A new method for generating families of continuous distributions", _Metron_ 71:63–79 (2013).</small>
- <small><sup id=Note14>(14)</sup> Aljarrah, M.A., Lee, C. and Famoye, F., "On generating T-X family of distributions using quantile functions", Journal of Statistical Distributions and Applications,1(2), 2014.</small>
- <small><sup id=Note15>(15)</sup> Gleaton, J.U., Lynch, J. D., "Properties of generalized log-logistic families of lifetime distributions", _Journal of Probability and Statistical Science_ 4(1), 2006.</small>
- <small><sup id=Note16>(16)</sup> Korkmaz, M.Ç., Alizadeh, M., et al., "The Generalized Odd Weibull Generated Family of Distributions: Statistical Properties and Applications", _Pak. J. Stat. Oper. Res._ XIV(3), 2018.</small>
- <small><sup id=Note17>(17)</sup> Hosseini, B., Afshari, M., "The Generalized Odd Gamma-G Family of Distributions:  Properties and Application", _Austrian Journal of Statistics_ vol. 47, Feb. 2018.</small>
- <small><sup id=Note18>(18)</sup> N.H. Al Noor and N.K. Assi, "Rayleigh-Rayleigh Distribution: Properties and Applications", _Journal of Physics: Conference Series_ 1591, 012038 (2020).  The underlying Rayleigh distribution uses a parameter _&theta;_ (or _&lambda;_), which is different from _Mathematica_'s parameterization with _&sigma;_ = sqrt(1/_&theta;_<sup>2</sup>) = sqrt(1/_&lambda;_<sup>2</sup>).  The first Rayleigh distribution uses _&theta;_ and the second, _&lambda;_.</small>
- <small><sup id=Note19>(19)</sup> Boshi, M.A.A., et al., "Generalized Gamma – Generalized Gompertz Distribution", _Journal of Physics: Conference Series_ 1591, 012043 (2020).</small>
- <small><sup id=Note20>(20)</sup> Akdoğan, Y., Kus, C., et al., "Geometric-Zero Truncated Poisson Distribution: Properties and Applications", _Gazi University Journal of Science_ 32(4), 2019.</small>
- <small><sup id=Note21>(21)</sup> Keller, A.Z., Kamath A.R., "Reliability analysis of CNC machine tools", _Reliability Engineering_ 3 (1982).</small>
- <small><sup id=Note22>(22)</sup> Devroye, L., Gravel, C., "[**Random variate generation using only finitely many unbiased, independently and identically distributed random bits**](https://arxiv.org/abs/1502.02539v6)", arXiv:1502.02539v6  [cs.IT], 2020.</small>
- <small><sup id=Note23>(23)</sup> Jodrá, P., "A note on the right truncated Weibull distribution and the minimum of power function distributions", 2020.</small>

<a id=License></a>
## License

Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under [**Creative Commons Zero**](https://creativecommons.org/publicdomain/zero/1.0/).
