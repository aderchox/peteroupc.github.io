<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>Bernoulli Factory Algorithms</title><meta name="citation_title" content="Bernoulli Factory Algorithms"><meta name="og:title" content="Bernoulli Factory Algorithms"><meta name="og:type" content="article"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="Bernoulli Factory Algorithms"><meta name="author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css"></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>Bernoulli Factory Algorithms</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><a id=Introduction></a></p>

<h2>Introduction</h2>

<p>This page catalogs algorithms to turn coins biased one way into coins biased another way, also known as <em>Bernoulli factories</em>.  Many of them were suggested in (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>, but without step-by-step instructions in many cases.  This page provides these instructions to help programmers implement the Bernoulli factories they describe.  The Python module <a href="https://peteroupc.github.io/bernoulli.py"><strong><em>bernoulli.py</em></strong></a> includes implementations of several Bernoulli factories.</p>

<p>This page also contains algorithms to exactly simulate probabilities that are irrational numbers, using only random bits, which is likewise related to the Bernoulli factory problem.  Again, many of these were suggested in (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.</p>

<p>This page is focused on sampling methods that <em>exactly</em> simulate the probability described, without introducing rounding errors or other errors beyond those already present in the inputs (and assuming that we have a source of &quot;truly&quot; random numbers).</p>

<p><a id=About_This_Document></a></p>

<h3>About This Document</h3>

<p><strong>This is an open-source document; for an updated version, see the</strong> <a href="https://github.com/peteroupc/peteroupc.github.io/raw/master/bernoulli.md"><strong>source code</strong></a> <strong>or its</strong> <a href="https://github.com/peteroupc/peteroupc.github.io/blob/master/bernoulli.md"><strong>rendering on GitHub</strong></a><strong>.  You can send comments on this document on the</strong> <a href="https://github.com/peteroupc/peteroupc.github.io/issues"><strong>GitHub issues page</strong></a><strong>.  You are welcome to suggest additional Bernoulli factory algorithms, especially&mdash;</strong></p>

<ul>
<li><strong>specific</strong> <a href="#Continued_Fractions"><strong>continued fraction expansions</strong></a>,</li>
<li><strong>series expansions for the</strong> <a href="#Certain_Power_Series"><strong>power series</strong></a> <strong>algorithms below, and</strong></li>
<li><strong>algorithms that simulate probability mass functions or probability density functions, with or without a normalizing constant.</strong></li>
</ul>

<p><a id=Contents></a></p>

<h2>Contents</h2>

<ul>
<li><a href="#Introduction"><strong>Introduction</strong></a>

<ul>
<li><a href="#About_This_Document"><strong>About This Document</strong></a></li>
</ul></li>
<li><a href="#Contents"><strong>Contents</strong></a></li>
<li><a href="#About_Bernoulli_Factories"><strong>About Bernoulli Factories</strong></a></li>
<li><a href="#Algorithms"><strong>Algorithms</strong></a>

<ul>
<li><a href="#Algorithms_for_Functions_of_lambda"><strong>Algorithms for Functions of &lambda;</strong></a>

<ul>
<li><a href="#Certain_Power_Series"><strong>Certain Power Series</strong></a></li>
<li><a href="#exp_minus_lambda"><strong>exp(&minus;&lambda;)</strong></a></li>
<li><a href="#exp_minus_lambda__k___x"><strong>exp(&minus;(&lambda;<sup><em>k</em></sup> * <em>x</em>))</strong></a></li>
<li><a href="#exp_minus_lambda__k___x___m"><strong>exp(&minus;(&lambda;<sup><em>k</em></sup> * (<em>x</em> + <em>m</em>)))</strong></a></li>
<li><a href="#exp_minus_lambda__m___k"><strong>exp(&minus;(&lambda; + <em>m</em>)<sup><em>k</em></sup>)</strong></a></li>
<li><a href="#exp_lambda_1_minus_lambda"><strong>exp(&lambda;)*(1&minus;&lambda;)</strong></a></li>
<li><a href="#1_minus_lambda_cos_lambda"><strong>(1&minus;&lambda;)/cos(&lambda;)</strong></a></li>
<li><a href="#1_minus_lambda_tan_lambda"><strong>(1&minus;&lambda;) * tan(&lambda;)</strong></a></li>
<li><a href="#exp_lambda__c__minus__c"><strong>exp(&lambda; * <em>c</em> &minus; <em>c</em>)</strong></a></li>
<li><a href="#exp_minus_lambda_minus__c"><strong>exp(&minus;&lambda; &minus; <em>c</em>)</strong></a></li>
<li><a href="#1_1_lambda"><strong>1/(1+&lambda;)</strong></a></li>
<li><a href="#ln_1_lambda"><strong>ln(1+&lambda;)</strong></a></li>
<li><a href="#1_minus_ln_1_lambda"><strong>1 &minus; ln(1+&lambda;)</strong></a></li>
<li><a href="#c__lambda_beta_beta__c__lambda__d__mu_minus_beta_minus_1__c___d"><strong><em>c</em> * &lambda; * &beta; / (&beta; * (<em>c</em> * &lambda; + <em>d</em> * &mu;) &minus; (&beta; &minus; 1) * (<em>c</em> + <em>d</em>))</strong></a></li>
<li><a href="#c__lambda__c__lambda__d__or__c___d__lambda_1__c___d__lambda"><strong><em>c</em> * &lambda; / (<em>c</em> * &lambda; + <em>d</em>) or (<em>c</em>/<em>d</em>) * &lambda; / (1 + (<em>c</em>/<em>d</em>) * &lambda;))</strong></a></li>
<li><a href="#1__c__lambda"><strong>1 / (<em>c</em> + &lambda;)</strong></a></li>
<li><a href="#d__lambda__c"><strong>(<em>d</em> + &lambda;) / <em>c</em></strong></a></li>
<li><a href="#d___c__lambda"><strong><em>d</em> / (<em>c</em> + &lambda;)</strong></a></li>
<li><a href="#d__mu__c__lambda"><strong>(<em>d</em> + &mu;) / (<em>c</em> + &lambda;)</strong></a></li>
<li><a href="#lambda_mu"><strong>&lambda; + &mu;</strong></a></li>
<li><a href="#lambda_minus_mu"><strong>&lambda; &minus; &mu;</strong></a></li>
<li><a href="#1__c__lambda_2"><strong>1/(<em>c</em> + &lambda;)</strong></a></li>
<li><a href="#1_minus_lambda"><strong>1 &minus; &lambda;</strong></a></li>
<li><a href="#nu_lambda_1_minus_nu_mu"><strong>&nu; * &lambda; + (1 &minus; &nu;) * &mu;</strong></a></li>
<li><a href="#lambda_mu_minus_lambda_mu"><strong>&lambda; + &mu; &minus; (&lambda; * &mu;)</strong></a></li>
<li><a href="#lambda_mu_2"><strong>(&lambda; + &mu;) / 2</strong></a></li>
<li><a href="#arctan_lambda_lambda"><strong>arctan(&lambda;) /&lambda;</strong></a></li>
<li><a href="#arctan_lambda"><strong>arctan(&lambda;)</strong></a></li>
<li><a href="#cos_lambda"><strong>cos(&lambda;)</strong></a></li>
<li><a href="#sin_lambda"><strong>sin(&lambda;)</strong></a></li>
<li><a href="#lambda__x___y"><strong>&lambda;<sup><em>x</em>/<em>y</em></sup></strong></a></li>
<li><a href="#lambda_mu_3"><strong>&lambda;<sup>&mu;</sup></strong></a></li>
<li><a href="#sqrt_lambda"><strong>sqrt(&lambda;)</strong></a></li>
<li><a href="#arcsin_lambda_sqrt_1_minus_lambda_2_minus_1"><strong>arcsin(&lambda;) + sqrt(1 &minus; &lambda;<sup>2</sup>) &minus; 1</strong></a></li>
<li><a href="#arcsin_lambda_2"><strong>arcsin(&lambda;) / 2</strong></a></li>
<li><a href="#lambda_mu_4"><strong>&lambda; * &mu;</strong></a></li>
<li><a href="#lambda__x___y__linear_Bernoulli_factories"><strong>&lambda; * <em>x</em>/<em>y</em> (linear Bernoulli factories)</strong></a></li>
<li><a href="#lambda__x___y___i"><strong>(&lambda; * <em>x</em>/<em>y</em>)<sup><em>i</em></sup></strong></a></li>
<li><a href="#x03F5_lambda"><strong>&#x03F5; / &lambda;</strong></a></li>
<li><a href="#Certain_Rational_Functions"><strong>Certain Rational Functions</strong></a></li>
<li><a href="#Bernstein_Polynomials"><strong>Bernstein Polynomials</strong></a></li>
<li><a href="#Certain_Algebraic_Functions"><strong>Certain Algebraic Functions</strong></a></li>
<li><a href="#Expressions_Involving_Polylogarithms"><strong>Expressions Involving Polylogarithms</strong></a></li>
</ul></li>
<li><a href="#Algorithms_for_Irrational_Constants"><strong>Algorithms for Irrational Constants</strong></a>

<ul>
<li><a href="#Digit_Expansions"><strong>Digit Expansions</strong></a></li>
<li><a href="#Continued_Fractions"><strong>Continued Fractions</strong></a></li>
<li><a href="#Continued_Logarithms"><strong>Continued Logarithms</strong></a></li>
<li><a href="#1_phi"><strong>1 / &phi;</strong></a></li>
<li><a href="#sqrt_2_minus_1"><strong>sqrt(2) &minus; 1</strong></a></li>
<li><a href="#1_sqrt_2"><strong>1/sqrt(2)</strong></a></li>
<li><a href="#tanh_1_2_or_exp_1_minus_1_exp_1_1"><strong>tanh(1/2) or (exp(1) &minus; 1) / (exp(1) + 1)</strong></a></li>
<li><a href="#arctan__x___y___y___x"><strong>arctan(<em>x</em>/<em>y</em>) * <em>y</em>/<em>x</em></strong></a></li>
<li><a href="#pi_12"><strong>&pi; / 12</strong></a></li>
<li><a href="#pi_4"><strong>&pi; / 4</strong></a></li>
<li><a href="#1_pi"><strong>1 / &pi;</strong></a></li>
<li><a href="#a___b___x___y"><strong>(<em>a</em>/<em>b</em>)<sup><em>x</em>/<em>y</em></sup></strong></a></li>
<li><a href="#exp_minus__x___y"><strong>exp(&minus;<em>x</em>/<em>y</em>)</strong></a></li>
<li><a href="#exp_minus__z"><strong>exp(&minus;<em>z</em>)</strong></a></li>
<li><a href="#a___b___z"><strong>(<em>a</em>/<em>b</em>)<sup><em>z</em></sup></strong></a></li>
<li><a href="#1_1_exp__x___y__2_prec__LogisticExp"><strong>1 / 1 + exp(<em>x</em> / (<em>y</em> * 2<sup><em>prec</em></sup>)) (LogisticExp)</strong></a></li>
<li><a href="#1_1_exp__z__2_prec__LogisticExp"><strong>1 / 1 + exp(<em>z</em> / 2<sup><em>prec</em></sup>)) (LogisticExp)</strong></a></li>
<li><a href="#Polylogarithmic_Constants"><strong>Polylogarithmic Constants</strong></a></li>
<li><a href="#zeta_3_3_4_and_Other_Zeta_Related_Constants"><strong>&zeta;(3) * 3 / 4 and Other Zeta-Related Constants</strong></a></li>
<li><a href="#erf__x__erf_1"><strong>erf(<em>x</em>)/erf(1)</strong></a></li>
<li><a href="#2_1_exp_2_or_1_exp_0_1_exp_1"><strong>2 / (1 + exp(2)) or (1 + exp(0)) / (1 + exp(1))</strong></a></li>
<li><a href="#1_exp_1_1_exp_2"><strong>(1 + exp(1)) / (1 + exp(2))</strong></a></li>
</ul></li>
<li><a href="#General_Algorithms"><strong>General Algorithms</strong></a>

<ul>
<li><a href="#Convex_Combinations"><strong>Convex Combinations</strong></a></li>
<li><a href="#Simulating_the_Probability_Generating_Function"><strong>Simulating the Probability Generating Function</strong></a></li>
<li><a href="#Integrals"><strong>Integrals</strong></a></li>
</ul></li>
</ul></li>
<li><a href="#Open_Questions"><strong>Open Questions</strong></a></li>
<li><a href="#Correctness_and_Performance_Charts"><strong>Correctness and Performance Charts</strong></a>

<ul>
<li><a href="#The_Charts"><strong>The Charts</strong></a></li>
</ul></li>
<li><a href="#Notes"><strong>Notes</strong></a></li>
<li><a href="#Appendix"><strong>Appendix</strong></a>

<ul>
<li><a href="#Randomized_vs_Non_Randomized_Algorithms"><strong>Randomized vs. Non-Randomized Algorithms</strong></a></li>
<li><a href="#Simulating_Probabilities_vs_Estimating_Probabilities"><strong>Simulating Probabilities vs. Estimating Probabilities</strong></a></li>
<li><a href="#Convergence_of_Bernoulli_Factories"><strong>Convergence of Bernoulli Factories</strong></a></li>
<li><a href="#Alternative_Implementation_of_Bernoulli_Factories"><strong>Alternative Implementation of Bernoulli Factories</strong></a></li>
<li><a href="#Correctness_Proof_for_the_Continued_Logarithm_Simulation_Algorithm"><strong>Correctness Proof for the Continued Logarithm Simulation Algorithm</strong></a></li>
<li><a href="#Correctness_Proof_for_Continued_Fraction_Simulation_Algorithm_3"><strong>Correctness Proof for Continued Fraction Simulation Algorithm 3</strong></a></li>
<li><a href="#The_von_Neumann_Schema"><strong>The von Neumann Schema</strong></a></li>
<li><a href="#Probabilities_Arising_from_the_Forsythe_Method"><strong>Probabilities Arising from the Forsythe Method</strong></a></li>
<li><a href="#Probabilities_Arising_from_Certain_Permutations"><strong>Probabilities Arising from Certain Permutations</strong></a></li>
<li><a href="#Other_Algorithms_for_exp_minus_lambda"><strong>Other Algorithms for exp(&minus;&lambda;)</strong></a></li>
<li><a href="#Sketch_of_Derivation_of_the_Algorithm_for_1_pi"><strong>Sketch of Derivation of the Algorithm for 1 / &pi;</strong></a></li>
</ul></li>
<li><a href="#License"><strong>License</strong></a></li>
</ul>

<p><a id=About_Bernoulli_Factories></a></p>

<h2>About Bernoulli Factories</h2>

<p>A <em>Bernoulli factory</em> (Keane and O&#39;Brien 1994)<sup><a href="#Note2"><strong>(2)</strong></a></sup> is an algorithm that takes an input coin (a method that returns 1, or heads, with an unknown probability, or 0, or tails, otherwise) and returns 0 or 1 with a probability that depends on the input coin&#39;s probability of heads.  For example, a Bernoulli factory algorithm can take a coin that returns heads with probability &lambda; and produce a coin that returns heads with probability exp(&minus;&lambda;).</p>

<p>A <em>factory function</em> is a function that relates the old probability to the new one.  Its domain is [0, 1] and returns a probability in [0, 1].  There are certain requirements for factory functions.  As shown by Keane and O&#39;Brien (1994)<sup><a href="#Note2"><strong>(2)</strong></a></sup>, a function <em>f</em>(&lambda;) can serve as a factory function if and only if <em>f</em>, in a given interval in [0, 1]&mdash;</p>

<ul>
<li>is continuous everywhere,</li>
<li>does not go to 0 or 1 exponentially fast, and</li>
<li>either returns a constant value in [0, 1] everywhere, or returns a value in [0, 1] at each of the points 0 and 1 and a value in (0, 1) at each other point.</li>
</ul>

<p>As one example, the function <em>f</em> = 2*&lambda; cannot serve as a factory function, since its graph touches 1 somewhere in the open interval (0, 1).</p>

<p>If a function&#39;s graph touches 0 or 1 somewhere in (0, 1), papers have suggested dealing with this by modifying the function so it no longer touches 0 or 1 there (for example, <em>f</em> = 2*&lambda; might become <em>f</em> = min(2 * &lambda;, 1 &minus; &#x03F5;) where &#x03F5; is in (0, 1/2) (Keane and O&#39;Brien 1994)<sup><a href="#Note2"><strong>(2)</strong></a></sup>, (Huber 2014, introduction)<sup><a href="#Note3"><strong>(3)</strong></a></sup>), or by somehow ensuring that &lambda; does not come close to the point where the graph touches 0 or 1 (Nacu and Peres 2005, theorem 1)<sup><a href="#Note4"><strong>(4)</strong></a></sup>.</p>

<p>The next section will show algorithms for a number of factory functions, allowing different kinds of probabilities to be simulated from input coins.</p>

<p><a id=Algorithms></a></p>

<h2>Algorithms</h2>

<p>In the following algorithms:</p>

<ul>
<li>&lambda; is the unknown probability of heads of the input coin.</li>
<li>The instruction to &quot;generate a uniform(0, 1) random number&quot; can be implemented&mdash;

<ul>
<li>by creating a <a href="https://peteroupc.github.io/exporand.html"><strong>uniform partially-sampled random number (PSRN)</strong></a> with a sign of 1 or positive, an integer part of 0, and an empty fractional part (most accurate), or</li>
<li>by generating <code>RNDEXCRANGE(0, 1)</code> or <code>RNDINT(1000)</code> (less accurate).</li>
</ul></li>
<li>The instruction to &quot;generate an exponential random number&quot; can be implemented&mdash;

<ul>
<li>by creating an empty <a href="https://peteroupc.github.io/exporand.html"><strong>exponential PSRN</strong></a> (most accurate), or</li>
<li>by generating <code>-ln(1/RNDEXCRANGE(0, 1))</code> (less accurate).</li>
</ul></li>
<li>To <strong>sample from a random number <em>u</em></strong> means to generate a number that is 1 with probability <em>u</em> and 0 otherwise.

<ul>
<li>If the number is a uniform PSRN, call the <strong>SampleGeometricBag</strong> algorithm with the PSRN and take the result of that call (which will be 0 or 1) (most accurate). (<strong>SampleGeometricBag</strong> is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Otherwise, this can be implemented by generating another uniform random number <em>v</em> and generating 1 if <em>v</em> is less than <em>u</em> or 0 otherwise (less accurate).</li>
</ul></li>
<li>Where an algorithm says &quot;if <em>a</em> is less than <em>b</em>&quot;, where <em>a</em> and <em>b</em> are random numbers, it means to run the <strong>RandLess</strong> algorithm on the two numbers (if they are both PSRNs), or do a less-than operation on <em>a</em> and <em>b</em>, as appropriate. (<strong>RandLess</strong> is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Where a step in the algorithm says &quot;with probability <em>x</em>&quot; to refer to an event that may or may not happen, then this can be implemented in one of the following ways:

<ul>
<li>Convert <em>x</em> to a rational number <em>y</em>/<em>z</em>, then call <code>ZeroOrOne(y, z)</code>.  The event occurs if the call returns 0. (Most accurate.)  For example, if an instruction says &quot;With probability 3/5, return 1&quot;, then implement it as &quot;Call <code>ZeroOrOne(3, 5)</code>. If the call returns 1, return 1.&quot;  <code>ZeroOrOne</code> is described in my article on <a href="https://peteroupc.github.io/randomfunc.html#Boolean_True_False_Conditions"><strong>random sampling methods</strong></a>.</li>
<li>Generate a uniform random number <em>v</em>. The event occurs if <em>v</em> is less than <em>x</em>.  (Less accurate.)</li>
</ul></li>
<li>For best results, the algorithms should be implemented using exact rational arithmetic (such as <code>Fraction</code> in Python or <code>Rational</code> in Ruby).  Floating-point arithmetic is discouraged because it can introduce rounding error.</li>
</ul>

<p>The algorithms as described here do not always lead to the best performance.  An implementation may change these algorithms as long as they produce the same results as the algorithms as described here.</p>

<p>The algorithms assume that a source of independent and unbiased random bits is available, in addition to the input coins.  But it&#39;s possible to implement these algorithms using nothing but those coins as a source of randomness.  See the <a href="#Randomized_vs_Non_Randomized_Algorithms"><strong>appendix</strong></a> for details.</p>

<p>Bernoulli factory algorithms that simulate <em>f</em>(&lambda;) are equivalent to unbiased estimators of <em>f</em>(&lambda;). See the <a href="#Simulating_Probabilities_vs_Estimating_Probabilities"><strong>appendix</strong></a> for details.</p>

<p><a id=Algorithms_for_Functions_of_lambda></a></p>

<h3>Algorithms for Functions of &lambda;</h3>

<p>&nbsp;</p>

<p><a id=Certain_Power_Series></a></p>

<h4>Certain Power Series</h4>

<p>Mendo (2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup> gave a Bernoulli factory algorithm for certain functions that can be rewritten as a series of the form&mdash;</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;1 &minus; (<em>c</em>[0] * (1 &minus; &lambda;) + ... + <em>c</em>[<em>i</em>] * (1 &minus; &lambda;)<sup><em>i</em> + 1</sup> + ...),</p>

<p>where <em>c</em>[<em>i</em>] &gt;= 0 are the coefficients of the series and sum to 1.  The algorithm follows:</p>

<ol>
<li>Let <em>v</em> be 1 and let <em>result</em> be 1.</li>
<li>Set <em>dsum</em> to 0 and <em>i</em> to 0.</li>
<li>Flip the input coin.  If it returns <em>v</em>, return <em>result</em>.</li>
<li>If <em>i</em> is equal to or greater than the number of coefficients, set <em>ci</em> to 0.  Otherwise, set <em>ci</em> to <em>c</em>[<em>i</em>].</li>
<li>With probability <em>ci</em>/(1 &minus; <em>dsum</em>), return 1 minus <em>result</em>.</li>
<li>Add <em>ci</em> to <em>dsum</em>, add 1 to <em>i</em>, and go to step 3.</li>
</ol>

<p>As pointed out in Mendo (2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup>, variants of this algorithm work for power series of the form&mdash;</p>

<ol>
<li>(<em>c</em>[0] * (1 &minus; &lambda;) + ... + <em>c</em>[<em>i</em>] * (1 &minus; &lambda;)<sup><em>i</em> + 1</sup> + ...), or</li>
<li>(<em>c</em>[0] * &lambda; + ... + <em>c</em>[<em>i</em>] * &lambda;<sup><em>i</em> + 1</sup> + ...), or</li>
<li>1 &minus; (<em>c</em>[0] * &lambda; + ... + <em>c</em>[<em>i</em>] * &lambda;<sup><em>i</em> + 1</sup> + ...).</li>
</ol>

<p>In the first two cases, replace &quot;let <em>result</em> be 1&quot; in the algorithm with &quot;let <em>result</em> be 0&quot;.  In the last two cases, replace &quot;let <em>v</em> be 1&quot; with &quot;let <em>v</em> be 0&quot;.</p>

<p>(Łatuszyński et al. 2009/2011)<sup><a href="#Note6"><strong>(6)</strong></a></sup> gave an algorithm that works for a wide class of series and other constructs that converge to the desired probability from above and from below.</p>

<p>One of these constructs is an alternating series of the form&mdash;</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>d[0]</em> &minus; <em>d[1]</em> * &lambda; + <em>d[2]</em> * &lambda;<sup>2</sup> &minus; ...,</p>

<p>where <em>d</em>[<em>i</em>] are all in the interval [0, 1] and form a non-increasing sequence of coefficients.</p>

<p>The following is the general algorithm for this kind of series, called the <strong>general martingale algorithm</strong>.  It takes a list of coefficients and an input coin, and returns 1 with probability given above, and 0 otherwise.</p>

<ol>
<li>Let <em>d[0]</em>, <em>d[1]</em>, etc. be the first, second, etc. coefficients of the alternating series.  Set <em>u</em> to <em>d[0]</em>, set <em>w</em> to 1, set <em>l</em> to 0, and set <em>n</em> to 1.</li>
<li>Generate a uniform(0, 1) random number <em>ret</em>.</li>
<li>If <em>w</em> is not 0, flip the input coin and multiply <em>w</em> by the result of the flip.</li>
<li>If <em>n</em> is even, set <em>u</em> to <em>l</em> + <em>w</em> * <em>d[n]</em>.  Otherwise, set <em>l</em> to <em>u</em> &minus; <em>w</em> * <em>d[n]</em>.</li>
<li>If <em>ret</em> is less than <em>l</em>, return 1.  If <em>ret</em> is less than <em>u</em>, go to the next step.  If neither is the case, return 0.  (If <em>ret</em> is a uniform PSRN, these comparisons should be done via the <strong>URandLessThanReal algorithm</strong>, which is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Add 1 to <em>n</em> and go to step 3.</li>
</ol>

<p>If the alternating series has the form&mdash;</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>d[0]</em> &minus; <em>d[1]</em> * &lambda;<sup>2</sup> + <em>d[2]</em> * &lambda;<sup>4</sup> &minus; ...,</p>

<p>then modify the general martingale algorithm by adding the following after step 3: &quot;3a. Repeat step 3 once.&quot;  (Examples of this kind of series are found in sin(&lambda;) and cos(&lambda;).)</p>

<p><a id=exp_minus_lambda></a></p>

<h4>exp(&minus;&lambda;)</h4>

<p>This algorithm converges quickly everywhere in (0, 1).  (In other words, the algorithm is <em>uniformly fast</em>, meaning the average running time is bounded from above for all choices of &lambda; and other parameters (Devroye 1986, esp. p. 717)<sup><a href="#Note7"><strong>(7)</strong></a></sup>.) This algorithm is adapted from the general martingale algorithm (in &quot;Certain Power Series&quot;, above), and makes use of the fact that exp(&minus;&lambda;) can be rewritten as 1 &minus; &lambda; + &lambda;<sup>2</sup>/2 &minus; &lambda;<sup>3</sup>/6 + &lambda;<sup>4</sup>/24 &minus; ..., which is an alternating series whose coefficients are 1, 1, 1/(2!), 1/(3!), 1/(4!), ....</p>

<ol>
<li>Set <em>u</em> to 1, set <em>w</em> to 1, set <em>l</em> to 0, and set <em>n</em> to 1.</li>
<li>Generate a uniform(0, 1) random number <em>ret</em>.</li>
<li>If <em>w</em> is not 0, flip the input coin, multiply <em>w</em> by the result of the flip, and divide <em>w</em> by <em>n</em>. (This is changed from the general martingale algorithm to take account of the factorial more efficiently in the second and later coefficients.)</li>
<li>If <em>n</em> is even, set <em>u</em> to <em>l</em> + <em>w</em>.  Otherwise, set <em>l</em> to <em>u</em> &minus; <em>w</em>.</li>
<li>If <em>ret</em> is less than <em>l</em>, return 1.  If <em>ret</em> is less than <em>u</em>, go to the next step.  If neither is the case, return 0.  (If <em>ret</em> is a uniform PSRN, these comparisons should be done via the <strong>URandLessThanReal algorithm</strong>, which is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Add 1 to <em>n</em> and go to step 3.</li>
</ol>

<p>See the appendix for other algorithms.</p>

<p><a id=exp_minus_lambda__k___x></a></p>

<h4>exp(&minus;(&lambda;<sup><em>k</em></sup> * <em>x</em>))</h4>

<p>In the following algorithm, which applies the general martingale algorithm, <em>k</em> is an integer 0 or greater, and <em>x</em> is a rational number in the interval [0, 1].  It represents the series 1 &minus; &lambda;<sup><em>k</em></sup>*<em>x</em> + &lambda;<sup>2*<em>k</em></sup>*<em>x</em>/2! &minus; &lambda;<sup>3*<em>k</em></sup>*<em>x</em>/3!, ..., and the coefficients are 1, <em>x</em>, <em>x</em>/(2!), <em>x</em>/(3!), ....</p>

<ol>
<li>Special cases: If <em>x</em> is 0, return 1.  If <em>k</em> is 0, run the <strong>algorithm for exp(&minus;<em>x</em>/<em>y</em>)</strong> (given later in this page) with <em>x</em>/<em>y</em> = <em>x</em>, and return the result.</li>
<li>Set <em>u</em> to 1, set <em>w</em> to 1, set <em>l</em> to 0, and set <em>n</em> to 1.</li>
<li>Generate a uniform(0, 1) random number <em>ret</em>.</li>
<li>If <em>w</em> is not 0, flip the input coin <em>k</em> times or until the flip returns 0.  If any of the flips returns 0, set <em>w</em> to 0, or if all the flips return 1, divide <em>w</em> by <em>n</em>.  Then, multiply <em>w</em> by a number that is 1 with probability <em>x</em> and 0 otherwise.</li>
<li>If <em>n</em> is even, set <em>u</em> to <em>l</em> + <em>w</em>.  Otherwise, set <em>l</em> to <em>u</em> &minus; <em>w</em>.</li>
<li>If <em>ret</em> is less than <em>l</em>, return 1.  If <em>ret</em> is less than <em>u</em>, go to the next step.  If neither is the case, return 0.  (If <em>ret</em> is a uniform PSRN, these comparisons should be done via the <strong>URandLessThanReal algorithm</strong>, which is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Add 1 to <em>n</em> and go to step 4.</li>
</ol>

<p><a id=exp_minus_lambda__k___x___m></a></p>

<h4>exp(&minus;(&lambda;<sup><em>k</em></sup> * (<em>x</em> + <em>m</em>)))</h4>

<p>In the following algorithm, <em>k</em> and <em>m</em> are both integers 0 or greater, and <em>x</em> is a rational number in the interval [0, 1].</p>

<ol>
<li>Call the <strong>algorithm for exp(&minus;(&lambda;<sup><em>k</em></sup> * <em>x</em>))</strong> <em>m</em> times with <em>k</em> = <em>k</em> and <em>x</em> = 1.  If any of these calls returns 0, return 0.</li>
<li>If <em>x</em> is 0, return 1.</li>
<li>Call the <strong>algorithm for exp(&minus;(&lambda;<sup><em>k</em></sup> * <em>x</em>))</strong> once, with <em>k</em> = <em>k</em> and <em>x</em> = <em>x</em>.  Return the result of this call.</li>
</ol>

<p><a id=exp_minus_lambda__m___k></a></p>

<h4>exp(&minus;(&lambda; + <em>m</em>)<sup><em>k</em></sup>)</h4>

<p>In the following algorithm, <em>m</em> and <em>k</em> are both integers 0 or greater.</p>

<ol>
<li>If <em>k</em> is 0, run the <strong>algorithm for exp(&minus;<em>x</em>/<em>y</em>)</strong> (given later on this page) with <em>x</em>/<em>y</em> = 1/1, and return the result.</li>
<li>If <em>k</em> is 1 and <em>m</em> is 0, run the <strong>algorithm for exp(&minus;&lambda;)</strong> and return the result.</li>
<li>Run the <strong>algorithm for exp(&minus;<em>x</em>/<em>y</em>)</strong> with <em>x</em>/<em>y</em> = <em>m</em><sup><em>k</em></sup> / 1.  If the algorithm returns 0, return 0.</li>
<li>Run the <strong>algorithm for exp(&minus;(&lambda;<sup><em>k</em></sup> * <em>x</em>))</strong>, with <em>k</em> = <em>k</em> and <em>x</em> = 1.  If the algorithm returns 0, return 0.</li>
<li>If <em>m</em> is 0, return 1.</li>
<li>Set <em>i</em> to 1, then while <em>i</em> &lt; <em>k</em>:

<ol>
<li>Set <em>z</em> to choose(<em>k</em>, <em>i</em>) * <em>m</em><sup><em>k</em> &minus; <em>i</em></sup>. (Here, choose(<em>k</em>, <em>i</em>) is a binomial coefficient.)</li>
<li>Run the <strong>algorithm for exp(&minus;(&lambda;<sup><em>k</em></sup> * <em>x</em>))</strong> <em>z</em> times, with <em>k</em> = <em>i</em> and <em>x</em> = 1.  If any of these calls returns 0, return 0.</li>
<li>Add 1 to <em>i</em>.</li>
</ol></li>
<li>Return 1.</li>
</ol>

<p><a id=exp_lambda_1_minus_lambda></a></p>

<h4>exp(&lambda;)*(1&minus;&lambda;)</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Set <em>k</em> and <em>w</em> each to 0.</li>
<li>Flip the input coin.  If it returns 0, return 1.</li>
<li>Generate a uniform(0, 1) random number <em>U</em>.</li>
<li>If <em>k</em> &gt; 0 and <em>w</em> is less than <em>U</em>, return 0.</li>
<li>Set <em>w</em> to <em>U</em>, add 1 to <em>k</em>, and go to step 2.</li>
</ol>

<p><a id=1_minus_lambda_cos_lambda></a></p>

<h4>(1&minus;&lambda;)/cos(&lambda;)</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Flip the input coin until the flip returns 0.  Then set <em>G</em> to the number of times the flip returns 1 this way.</li>
<li>If <em>G</em> is <strong>odd</strong>, return 0.</li>
<li>Generate a uniform(0, 1) random number <em>U</em>, then set <em>i</em> to 1.</li>
<li>While <em>i</em> is less than <em>G</em>:

<ol>
<li>Generate a uniform(0, 1) random number <em>V</em>.</li>
<li>If <em>i</em> is odd and <em>V</em> is less than <em>U</em>, return 0.</li>
<li>If <em>i</em> is even and <em>U</em> is less than <em>V</em>, return 0.</li>
<li>Add 1 to <em>i</em>, then set <em>U</em> to <em>V</em>.</li>
</ol></li>
<li>Return 1.</li>
</ol>

<p><a id=1_minus_lambda_tan_lambda></a></p>

<h4>(1&minus;&lambda;) * tan(&lambda;)</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Flip the input coin until the flip returns 0.  Then set <em>G</em> to the number of times the flip returns 1 this way.</li>
<li>If <em>G</em> is <strong>even</strong>, return 0.</li>
<li>Generate a uniform(0, 1) random number <em>U</em>, then set <em>i</em> to 1.</li>
<li>While <em>i</em> is less than <em>G</em>:

<ol>
<li>Generate a uniform(0, 1) random number <em>V</em>.</li>
<li>If <em>i</em> is odd and <em>V</em> is less than <em>U</em>, return 0.</li>
<li>If <em>i</em> is even and <em>U</em> is less than <em>V</em>, return 0.</li>
<li>Add 1 to <em>i</em>, then set <em>U</em> to <em>V</em>.</li>
</ol></li>
<li>Return 1.</li>
</ol>

<p><a id=exp_lambda__c__minus__c></a></p>

<h4>exp(&lambda; * <em>c</em> &minus; <em>c</em>)</h4>

<p>Used in (Dughmi et al. 2017)<sup><a href="#Note8"><strong>(8)</strong></a></sup> to apply an exponential weight (here, <em>c</em>) to an input coin.</p>

<ol>
<li>Generate a Poisson(<em>c</em>) random integer, call it <em>N</em>.</li>
<li>Flip the input coin until the flip returns 0 or the coin is flipped <em>N</em> times.  Return 1 if all the coin flips, including the last, returned 1 (or if <em>N</em> is 0); or return 0 otherwise.</li>
</ol>

<p><a id=exp_minus_lambda_minus__c></a></p>

<h4>exp(&minus;&lambda; &minus; <em>c</em>)</h4>

<p>To the best of my knowledge, I am not aware of any article or paper by others that presents this particular Bernoulli factory. In this algorithm, <em>c</em> is an integer that is 0 or greater.</p>

<ol>
<li>Run the <strong>algorithm for exp(&minus;<em>c</em>/1)</strong> described later in this document.  Return 0 if the algorithm returns 0.</li>
<li>Return the result of the <strong>algorithm for exp(&minus;&lambda;)</strong>.</li>
</ol>

<p><a id=1_1_lambda></a></p>

<h4>1/(1+&lambda;)</h4>

<p>One algorithm is the general martingale algorithm, since when &lambda; is in [0, 1], this function is an alternating series of the form <code>1 - x + x^2 - x^3 + ...</code>, whose coefficients are 1, 1, 1, 1, ....  However, this algorithm converges slowly when &lambda; is very close to 1.</p>

<p>A second algorithm is the so-called &quot;even-parity&quot; construction of (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.  However, this algorithm too converges slowly when &lambda; is very close to 1.</p>

<ol>
<li>Flip the input coin.  If it returns 0, return 1.</li>
<li>Flip the input coin.  If it returns 0, return 0.  Otherwise, go to step 1.</li>
</ol>

<p>A third algorithm is a special case of the two-coin Bernoulli factory of (Gonçalves et al., 2017)<sup><a href="#Note9"><strong>(9)</strong></a></sup> and is uniformly fast, unlike the previous two algorithms.  It will be called the <strong>two-coin special case</strong> in this document.</p>

<ol>
<li>With probability 1/2, return 1. (For example, generate an unbiased random bit and return 1 if that bit is 1.)</li>
<li>Flip the input coin.  If it returns 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=ln_1_lambda></a></p>

<h4>ln(1+&lambda;)</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>.</li>
<li>Flip the input coin.  If it returns 0, flip the coin again and return the result.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a>. If the result is 0, flip the input coin and return the result.</li>
<li>Flip the input coin.  If it returns 0, return 0.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a>. If the result is 0, return 0.  Otherwise, go to step 2.</li>
</ol>

<p>Observing that the even-parity construction used in the Flajolet paper is equivalent to the two-coin special case, which is uniformly fast for all &lambda; parameters, the algorithm above can be made uniformly fast as follows:</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>.</li>
<li>With probability 1/2, flip the input coin and return the result.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em>.</strong></a>, then flip the input coin.  If the call and the flip both return 1, return 0.  Otherwise, go to step 2.</li>
</ol>

<p><a id=1_minus_ln_1_lambda></a></p>

<h4>1 &minus; ln(1+&lambda;)</h4>

<p>Invert the result of the algorithm for ln(1+&lambda;) (make it 1 if it&#39;s 0 and vice versa).<sup><a href="#Note10"><strong>(10)</strong></a></sup></p>

<p><a id=c__lambda_beta_beta__c__lambda__d__mu_minus_beta_minus_1__c___d></a></p>

<h4><em>c</em> * &lambda; * &beta; / (&beta; * (<em>c</em> * &lambda; + <em>d</em> * &mu;) &minus; (&beta; &minus; 1) * (<em>c</em> + <em>d</em>))</h4>

<p>This is the general two-coin algorithm of (Gonçalves et al., 2017)<sup><a href="#Note9"><strong>(9)</strong></a></sup> and (Vats et al. 2020)<sup><a href="#Note11"><strong>(11)</strong></a></sup>.  It takes two input coins that each output heads (1) with probability &lambda; or &mu;, respectively.  It also takes a parameter &beta; in the interval [0, 1], which is a so-called &quot;portkey&quot; or early rejection parameter (when &beta; = 1, the formula simplifies to <em>c</em> * &lambda; / (<em>c</em> * &lambda; + <em>d</em> * &mu;)).</p>

<ol>
<li>With probability &beta;, go to step 2.  Otherwise, return 0. (For example, call <code>ZeroOrOne</code> with &beta;&#39;s numerator and denominator, and return 0 if that call returns 0, or go to step 2 otherwise.  <code>ZeroOrOne</code> is described in my article on <a href="https://peteroupc.github.io/randomfunc.html#Boolean_True_False_Conditions"><strong>random sampling methods</strong></a>.)</li>
<li>With probability <em>c</em> / (<em>c</em> + <em>d</em>), flip the &lambda; input coin.  Otherwise, flip the &mu; input coin.  If the &lambda; input coin returns 1, return 1.  If the &mu; input coin returns 1, return 0.  If the corresponding coin returns 0, go to step 1.</li>
</ol>

<p><a id=c__lambda__c__lambda__d__or__c___d__lambda_1__c___d__lambda></a></p>

<h4><em>c</em> * &lambda; / (<em>c</em> * &lambda; + <em>d</em>) or (<em>c</em>/<em>d</em>) * &lambda; / (1 + (<em>c</em>/<em>d</em>) * &lambda;))</h4>

<p>This algorithm, also known as the <strong>logistic Bernoulli factory</strong> (Huber 2016)<sup><a href="#Note12"><strong>(12)</strong></a></sup>, (Morina et al., 2019)<sup><a href="#Note13"><strong>(13)</strong></a></sup>, is a special case of the two-coin algorithm above, but this time uses only one input coin.</p>

<ol>
<li>With probability <em>d</em> / (<em>c</em> + <em>d</em>), return 0.</li>
<li>Flip the input coin.  If the flip returns 1, return 1.  Otherwise, go to step 1.</li>
</ol>

<p>(Note that Huber [2016] specifies this Bernoulli factory in terms of a Poisson point process, which seems to require much more randomness on average.)</p>

<p><a id=1__c__lambda></a></p>

<h4>1 / (<em>c</em> + &lambda;)</h4>

<p>In this algorithm, <em>c</em> must be 1 or greater.  For example, this algorithm can simulate a probability of the form 1 / <em>z</em>, where <em>z</em> is greater than 0 and made up of an integer part (<em>c</em>) and a fractional part (&lambda;) that can be simulated by a Bernoulli factory.  See also the algorithms for continued fractions.</p>

<ol>
<li>With probability <em>c</em> / (1 + <em>c</em>), return a number that is 1 with probability 1/<em>c</em> and 0 otherwise.</li>
<li>Flip the input coin.  If the flip returns 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=d__lambda__c></a></p>

<h4>(<em>d</em> + &lambda;) / <em>c</em></h4>

<p>This algorithm currently works only if <em>d</em> and <em>c</em> are integers and 0 &lt;= <em>d</em> &lt; <em>c</em>.</p>

<ol>
<li>Generate an integer in [0, <em>c</em>) uniformly at random, call it <em>i</em>.</li>
<li>If <em>i</em> &lt; <em>d</em>, return 1.  If <em>i</em> = <em>d</em>, flip the input coin and return the result.  If neither is the case, go to step 1.</li>
</ol>

<p><a id=d___c__lambda></a></p>

<h4><em>d</em> / (<em>c</em> + &lambda;)</h4>

<p>In this algorithm, <em>c</em> must be 1 or greater and <em>d</em> must be in the interval [0, <em>c</em>].  See also the algorithms for continued fractions.</p>

<ol>
<li>With probability <em>c</em> / (1 + <em>c</em>), return a number that is 1 with probability <em>d</em>/<em>c</em> and 0 otherwise.</li>
<li>Flip the input coin.  If the flip returns 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=d__mu__c__lambda></a></p>

<h4>(<em>d</em> + &mu;) / (<em>c</em> + &lambda;)</h4>

<p>Combines the algorithms in the previous two sections.  This algorithm currently works only if <em>d</em> and <em>c</em> are integers and 0 &lt;= <em>d</em> &lt; <em>c</em>.</p>

<ol>
<li>With probability <em>c</em> / (1 + <em>c</em>), do the following:

<ol>
<li>Generate an integer in [0, <em>c</em>) uniformly at random, call it <em>i</em>.</li>
<li>If <em>i</em> &lt; <em>d</em>, return 1.  If <em>i</em> = <em>d</em>, flip the &mu; input coin and return the result.  If neither is the case, go to the previous substep.</li>
</ol></li>
<li>Flip the &lambda; input coin.  If the flip returns 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=lambda_mu></a></p>

<h4>&lambda; + &mu;</h4>

<p>(Nacu and Peres 2005, proposition 14(iii))<sup><a href="#Note4"><strong>(4)</strong></a></sup>.  This algorithm takes two input coins that simulate &lambda; or &mu;, respectively, and a parameter &#x03F5;, which must be greater than 0 and chosen such that &lambda; + &mu; &lt; 1 &minus; &#x03F5;.</p>

<ol>
<li>Create a &nu; input coin that does the following: &quot;With probability 1/2, flip the &lambda; input coin and return the result.  Otherwise, flip the &mu; input coin and return the result.&quot;</li>
<li>Call the <strong>2014 algorithm</strong>, the <strong>2016 algorithm</strong>, or the <strong>2019 algorithm</strong>, described later, using the &nu; input coin, <em>x</em>/<em>y</em> = 2/1, <em>i</em> = 1 (for the 2019 algorithm), and &#x03F5; = &#x03F5;, and return the result.</li>
</ol>

<p><a id=lambda_minus_mu></a></p>

<h4>&lambda; &minus; &mu;</h4>

<p>(Nacu and Peres 2005, proposition 14(iii-iv))<sup><a href="#Note4"><strong>(4)</strong></a></sup>.  This algorithm takes two input coins that simulate &lambda; or &mu;, respectively, and a parameter &#x03F5;, which must be greater than 0 and chosen such that &lambda; &minus; &mu; &gt; &#x03F5; (and should be chosen such that &#x03F5; is slightly less than &lambda; &minus; &mu;).</p>

<ol>
<li>Create a &nu; input coin that does the following: &quot;With probability 1/2, flip the &lambda; input coin and return <strong>1 minus the result</strong>.  Otherwise, flip the &mu; input coin and return the result.&quot;</li>
<li>Call the <strong>2014 algorithm</strong>, the <strong>2016 algorithm</strong>, or the <strong>2019 algorithm</strong>, described later, using the &nu; input coin, <em>x</em>/<em>y</em> = 2/1, <em>i</em> = 1 (for the 2019 algorithm), and &#x03F5; = &#x03F5;, and return 1 minus the result.</li>
</ol>

<p><a id=1__c__lambda_2></a></p>

<h4>1/(<em>c</em> + &lambda;)</h4>

<p>Works only if <em>c</em> &gt; 0.</p>

<ol>
<li>With probability <em>c</em>/(1 + <em>c</em>), return a number that is 1 with probability 1/<em>c</em> and 0 otherwise.</li>
<li>Flip the input coin.  If the flip returns 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=1_minus_lambda></a></p>

<h4>1 &minus; &lambda;</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>: Flip the &lambda; input coin and return 0 if the result is 1, or 1 otherwise.</p>

<p><a id=nu_lambda_1_minus_nu_mu></a></p>

<h4>&nu; * &lambda; + (1 &minus; &nu;) * &mu;</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>: Flip the &nu; input coin.  If the result is 0, flip the &lambda; input coin and return the result.  Otherwise, flip the &mu; input coin and return the result.</p>

<p><a id=lambda_mu_minus_lambda_mu></a></p>

<h4>&lambda; + &mu; &minus; (&lambda; * &mu;)</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>: Flip the &lambda; input coin and the &mu; input coin.  Return 1 if either flip returns 1, and 0 otherwise.</p>

<p><a id=lambda_mu_2></a></p>

<h4>(&lambda; + &mu;) / 2</h4>

<p>(Nacu and Peres 2005, proposition 14(iii))<sup><a href="#Note4"><strong>(4)</strong></a></sup>; (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>: With probability 1/2, flip the &lambda; input coin and return the result.  Otherwise, flip the &mu; input coin and return the result.</p>

<p><a id=arctan_lambda_lambda></a></p>

<h4>arctan(&lambda;) /&lambda;</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> twice, and flip the input coin twice.  If any of these calls or flips returns 0, return 1.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> twice, and flip the input coin twice.  If any of these calls or flips returns 0, return 0.  Otherwise, go to step 2.</li>
</ol>

<p>Observing that the even-parity construction used in the Flajolet paper is equivalent to the two-coin special case, which is uniformly fast for all &lambda; parameters, the algorithm above can be made uniformly fast as follows:</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>.</li>
<li>With probability 1/2, return 1.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> twice, and flip the input coin twice.  If all of these calls and flips return 1, return 0.  Otherwise, go to step 2.</li>
</ol>

<p><a id=arctan_lambda></a></p>

<h4>arctan(&lambda;)</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>: Call the <strong>algorithm for arctan(&lambda;) /&lambda;</strong> and flip the input coin.  Return 1 if the call and flip both return 1, or 0 otherwise.</p>

<p><a id=cos_lambda></a></p>

<h4>cos(&lambda;)</h4>

<p>This algorithm adapts the general martingale algorithm for this function&#39;s series expansion.  In fact, this is a special case of Algorithm 3 of (Łatuszyński et al. 2009/2011)<sup><a href="#Note6"><strong>(6)</strong></a></sup> (which is more general than Proposition 3.4, the general martingale algorithm). The series expansion for cos(&lambda;) is 1 &minus; &lambda;<sup>2</sup>/(2!) + &lambda;<sup>4</sup>/(4!) &minus; ..., which is an alternating series except the exponent is increased by 2 (rather than 1) with each term.  The coefficients are thus 1, 1/(2!), 1/(4!), ....  A <em>lower truncation</em> of the series is a truncation of that series that ends with a minus term, and the corresponding <em>upper truncation</em> is the same truncation but without the last minus term.  This series expansion meets the requirements of Algorithm 3 because&mdash;</p>

<ul>
<li>the lower truncation is less than or equal to its corresponding upper truncation almost surely,</li>
<li>the lower and upper truncations are in the interval [0, 1],</li>
<li>each lower truncation is greater than or equal to the previous lower truncation almost surely,</li>
<li>each upper truncation is less than or equal to the previous upper truncation almost surely, and</li>
<li>the lower and upper truncations have an expected value that approaches &lambda; from below and above.</li>
</ul>

<p>The algorithm to simulate cos(&lambda;) follows.</p>

<ol>
<li>Set <em>u</em> to 1, set <em>w</em> to 1, set <em>l</em> to 0, set <em>n</em> to 1, and set <em>fac</em> to 2.</li>
<li>Generate a uniform(0, 1) random number <em>ret</em>.</li>
<li>If <em>w</em> is not 0, flip the input coin. If the flip returns 0, set <em>w</em> to 0. Do this step again. (Note that in the general martingale algorithm, only one coin is flipped in this step. Up to two coins are flipped instead because the exponent increases by 2 rather than 1.)</li>
<li>If <em>n</em> is even, set <em>u</em> to <em>l</em> + <em>w</em> / <em>fac</em>.  Otherwise, set <em>l</em> to <em>u</em> &minus; <em>w</em> / <em>fac</em>. (Here we divide by the factorial of 2-times-<em>n</em>.)</li>
<li>If <em>ret</em> is less than <em>l</em>, return 1.  If <em>ret</em> is less than <em>u</em>, go to the next step.  If neither is the case, return 0.  (If <em>ret</em> is a uniform PSRN, these comparisons should be done via the <strong>URandLessThanReal algorithm</strong>, which is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Add 1 to <em>n</em>, then multiply <em>fac</em> by (<em>n</em> * 2 &minus; 1) * (<em>n</em> * 2), then go to step 3.</li>
</ol>

<p><a id=sin_lambda></a></p>

<h4>sin(&lambda;)</h4>

<p>This algorithm is likewise a special case of Algorithm 3 of (Łatuszyński et al. 2009/2011)<sup><a href="#Note6"><strong>(6)</strong></a></sup>.  sin(&lambda;) can be rewritten as &lambda; * (1 &minus; &lambda;<sup>2</sup>/(3!) + &lambda;<sup>4</sup>/(5!) &minus; ...), which includes an alternating series where the exponent is increased by 2 (rather than 1) with each term.  The coefficients are thus 1, 1/(3!), 1/(5!), .... This series expansion meets the requirements of Algorithm 3 for the same reasons as the cos(&lambda;) series does.</p>

<p>The algorithm to simulate sin(&lambda;) follows.</p>

<ol>
<li>Flip the input coin.  If it returns 0, return 0.</li>
<li>Set <em>u</em> to 1, set <em>w</em> to 1, set <em>l</em> to 0, set <em>n</em> to 1, and set <em>fac</em> to 6.</li>
<li>Generate a uniform(0, 1) random number <em>ret</em>.</li>
<li>If <em>w</em> is not 0, flip the input coin. If the flip returns 0, set <em>w</em> to 0. Do this step again.</li>
<li>If <em>n</em> is even, set <em>u</em> to <em>l</em> + <em>w</em> / <em>fac</em>.  Otherwise, set <em>l</em> to <em>u</em> &minus; <em>w</em> / <em>fac</em>.</li>
<li>If <em>ret</em> is less than <em>l</em>, return 1.  If <em>ret</em> is less than <em>u</em>, go to the next step.  If neither is the case, return 0.  (If <em>ret</em> is a uniform PSRN, these comparisons should be done via the <strong>URandLessThanReal algorithm</strong>, which is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Add 1 to <em>n</em>, then multiply <em>fac</em> by (<em>n</em> * 2) * (<em>n</em> * 2 + 1), then go to step 3.</li>
</ol>

<p><a id=lambda__x___y></a></p>

<h4>&lambda;<sup><em>x</em>/<em>y</em></sup></h4>

<p>In the algorithm below, the case where <em>x</em>/<em>y</em> is in (0, 1) is due to recent work by Mendo (2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup>.  The algorithm works only when <em>x</em>/<em>y</em> is 0 or greater.</p>

<ol>
<li>If <em>x</em>/<em>y</em> is 0, return 1.</li>
<li>If <em>x</em>/<em>y</em> is equal to 1, flip the input coin and return the result.</li>
<li>If <em>x</em>/<em>y</em> is greater than 1:

<ol>
<li>Set <em>ipart</em> to floor(<em>x</em>/<em>y</em>) and <em>fpart</em> to <code>rem(x, y)</code>.</li>
<li>If <em>fpart</em> is greater than 0, subtract 1 from <em>ipart</em>, then call this algorithm recursively with <em>x</em> = floor(<em>fpart</em>/2) and <em>y</em> = <em>y</em>, then call this algorithm, again recursively, with <em>x</em> = <em>fpart</em> &minus; floor(<em>fpart</em>/2) and <em>y</em> = <em>y</em>. Return 0 if either call returns 0.  (This is done rather than the more obvious approach in order to avoid calling this algorithm with fractional parts very close to 0, because the algorithm runs much more slowly than for fractional parts closer to 1.)</li>
<li>If <em>ipart</em> is 1 or greater, flip the input coin <em>ipart</em> many times.  Return 0 if any of these flips returns 1.</li>
<li>Return 1.</li>
</ol></li>
<li><em>x</em>/<em>y</em> is less than 1, so set <em>i</em> to 1.</li>
<li>Flip the input coin; if it returns 1, return 1.</li>
<li>With probability <em>x</em>/(<em>y</em>*<em>i</em>), return 0.</li>
<li>Add 1 to <em>i</em> and go to step 5.</li>
</ol>

<blockquote>
<p><strong>Note:</strong> When <em>x</em>/<em>y</em> is less than 1, the minimum number of coin flips needed, on average, by this algorithm will grow without bound as &lambda; approaches 0.  In fact, no fast Bernoulli factory algorithm can avoid this unbounded growth without additional information on &lambda; (Mendo 2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup>.  See also the appendix, which also shows an alternative way to implement this and other Bernoulli factory algorithms using partially-sampled random numbers (PSRNs), which exploits knowledge of &lambda; but is not the focus of this article since it involves arithmetic.</p>
</blockquote>

<p><a id=lambda_mu_3></a></p>

<h4>&lambda;<sup>&mu;</sup></h4>

<p>This algorithm is based on the previous one, but changed to accept a second input coin (which outputs heads with probability &mu;) rather than a fixed value for the exponent. To the best of my knowledge, I am not aware of any article or paper by others that presents this particular Bernoulli factory.</p>

<ol>
<li>Set <em>i</em> to 1.</li>
<li>Flip the input coin that simulates the base, &lambda;; if it returns 1, return 1.</li>
<li>Flip the input coin that simulates the exponent, &mu;; if it returns 1, return 0 with probability 1/<em>i</em>.</li>
<li>Add 1 to <em>i</em> and go to step 1.</li>
</ol>

<p><a id=sqrt_lambda></a></p>

<h4>sqrt(&lambda;)</h4>

<p>Use the algorithm for &lambda;<sup>1/2</sup>.</p>

<p><a id=arcsin_lambda_sqrt_1_minus_lambda_2_minus_1></a></p>

<h4>arcsin(&lambda;) + sqrt(1 &minus; &lambda;<sup>2</sup>) &minus; 1</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.  The algorithm given here uses the special two-coin case rather than the even-parity construction.</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>.</li>
<li>Create a secondary coin &mu; that does the following: &quot;<a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> twice, and flip the input coin twice.  If all of these calls and flips return 1, return 0.  Otherwise, return 1.&quot;</li>
<li>Call the <strong>algorithm for &mu;<sup>1/2</sup></strong> using the secondary coin &mu;.  If it returns 0, return 0.</li>
<li>With probability 1/2, flip the input coin and return the result.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> once, and flip the input coin once.  If both the call and flip return 1, return 0.  Otherwise, go to step 4.</li>
</ol>

<p><a id=arcsin_lambda_2></a></p>

<h4>arcsin(&lambda;) / 2</h4>

<p>The Flajolet paper doesn&#39;t explain in detail how arcsin(&lambda;)/2 arises out of arcsin(&lambda;) + sqrt(1 &minus; &lambda;<sup>2</sup>) &minus; 1 via Bernoulli factory constructions, but here is an algorithm.<sup><a href="#Note14"><strong>(14)</strong></a></sup> Note, however, that the number of input coin flips is expected to grow without bound as &lambda; approaches 1.</p>

<ol>
<li>With probability 1/2, run the <strong>algorithm for arcsin(&lambda;) + sqrt(1 &minus; &lambda;<sup>2</sup>) &minus; 1</strong> and return the result.</li>
<li>Create a secondary coin &mu; that does the following: &quot;Flip the input coin twice.  If both flips return 1, return 0.  Otherwise, return 1.&quot; (The coin simulates 1 &minus; &lambda;<sup>2</sup>.)</li>
<li>Call the <strong>algorithm for &mu;<sup>1/2</sup></strong> using the secondary coin &mu;.  If it returns 0, return 1; otherwise, return 0. (This step effectively cancels out the sqrt(1 &minus; &lambda;<sup>2</sup>) &minus; 1 part and divides by 2.)</li>
</ol>

<p><a id=lambda_mu_4></a></p>

<h4>&lambda; * &mu;</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>: Flip the &lambda; input coin and the &mu; input coin.  Return 1 if both flips return 1, and 0 otherwise.</p>

<p><a id=lambda__x___y__linear_Bernoulli_factories></a></p>

<h4>&lambda; * <em>x</em>/<em>y</em> (linear Bernoulli factories)</h4>

<p>In general, this function will touch 0 or 1 somewhere in [0, 1], when <em>x</em>/<em>y</em> &gt; 0.  This makes the function relatively non-trivial to simulate in this case.</p>

<p>Huber has suggested several algorithms for this function over the years.</p>

<p>The first algorithm is called the <strong>2014 algorithm</strong> in this document (Huber 2014)<sup><a href="#Note3"><strong>(3)</strong></a></sup>.  It uses three parameters: <em>x</em>, <em>y</em>, and &#x03F5;, such that <em>x</em>/<em>y</em> &gt; 0 and &#x03F5; is greater than 0.  When <em>x</em>/<em>y</em> is greater than 1, the &#x03F5; parameter has to be chosen such that &lambda; * <em>x</em>/<em>y</em> &lt; 1 &minus; &#x03F5;, in order to bound the function away from 0 and 1.  As a result, some knowledge of &lambda; has to be available to the algorithm.  (In fact, as simulation results show, the choice of &#x03F5; is crucial to this algorithm&#39;s performance; for best results, &#x03F5; should be chosen such that &lambda; * <em>x</em>/<em>y</em> is slightly less than 1 &minus; &#x03F5;.) The algorithm as described below also includes certain special cases, not mentioned in Huber, to make it more general.</p>

<ol>
<li>Special cases: If <em>x</em> is 0, return 0.  Otherwise, if <em>x</em> equals <em>y</em>, flip the input coin and return the result.  Otherwise, if <em>x</em> is less than <em>y</em>, then: (a) With probability <em>x</em>/<em>y</em>, flip the input coin and return the result; otherwise (b) return 0.</li>
<li>Set <em>c</em> to <em>x</em>/<em>y</em>, and set <em>k</em> to 23 / (5 * &#x03F5;).</li>
<li>If &#x03F5; is greater than 644/1000, set &#x03F5; to 644/1000.</li>
<li>Set <em>i</em> to 1.</li>
<li>Flip the input coin.  If it returns 0, then generate numbers that are each 1 with probability (<em>c</em> &minus; 1) / <em>c</em> and 0 otherwise, until 0 is generated this way, then add 1 to <em>i</em> for each number generated this way (including the last).</li>
<li>Subtract 1 from <em>i</em>, then if <em>i</em> is 0, return 1.</li>
<li>If <em>i</em> is less than <em>k</em>, go to step 5.</li>
<li>If <em>i</em> is <em>k</em> or greater:

<ol>
<li>Generate <em>i</em> numbers that are each 1 with probability 2 / (&#x03F5; + 2) or 0 otherwise.  If any of those numbers is 0, return 0.</li>
<li>Multiply <em>c</em> by 2 / (&#x03F5; + 2), divide &#x03F5; by 2, and multiply <em>k</em> by 2.</li>
</ol></li>
<li>If <em>i</em> is 0, return 1.  Otherwise, go to step 5.</li>
</ol>

<p>The second algorithm is called the <strong>2016 algorithm</strong> (Huber 2016)<sup><a href="#Note12"><strong>(12)</strong></a></sup> and uses the same parameters <em>x</em>, <em>y</em>, and &#x03F5;, and its description uses the same special cases.  The difference here is that it involves a so-called &quot;logistic Bernoulli factory&quot;, which is replaced in this document with a different one that simulates the same function.  When <em>x</em>/<em>y</em> is greater than 1, the &#x03F5; parameter has to be chosen such that &lambda; * <em>x</em>/<em>y</em> &lt;= 1 &minus; &#x03F5;.</p>

<ol>
<li>The same special cases as for the 2014 algorithm apply.</li>
<li>Set <em>m</em> to ceil(1 + 9 / (2 * &#x03F5;)).</li>
<li>Set &beta; to 1 + 1 / (<em>m</em> &minus; 1).</li>
<li><strong>Algorithm A</strong> is what Huber calls this step.  Set <em>s</em> to 1, then while <em>s</em> is greater than 0 and less than <em>m</em>:

<ol>
<li>Run the <strong>logistic Bernoulli factory</strong> algorithm with <em>c</em>/<em>d</em> = &beta; * <em>x</em>/<em>y</em>.</li>
<li>Set <em>s</em> to <em>s</em> &minus; <em>z</em> * 2 + 1, where <em>z</em> is the result of the logistic Bernoulli factory.</li>
</ol></li>
<li>If <em>s</em> is other than 0, return 0.</li>
<li>With probability 1/&beta;, return 1.</li>
<li>Run this algorithm recursively, with <em>x</em>/<em>y</em> = &beta; * <em>x</em>/<em>y</em> and &#x03F5; = 1 &minus; &beta; * (1 &minus; &#x03F5;).  If it returns 0, return 0.</li>
<li>The <strong>high-power logistic Bernoulli factory</strong> is what Huber calls this step.  Set <em>s</em> to 1, then while <em>s</em> is greater than 0 and less than or equal to <em>m</em> minus 2:

<ol>
<li>Run the <strong>logistic Bernoulli factory</strong> algorithm with <em>c</em>/<em>d</em> = &beta; * <em>x</em>/<em>y</em>.</li>
<li>Set <em>s</em> to <em>s</em> + <em>z</em> * 2 &minus; 1, where <em>z</em> is the result of the logistic Bernoulli factory.</li>
</ol></li>
<li>If <em>s</em> is equal to <em>m</em> minus 1, return 1.</li>
<li>Subtract 1 from <em>m</em> and go to step 7.</li>
</ol>

<p>The paper that presented the 2016 algorithm also included a third algorithm, described below, that works only if &lambda; * <em>x</em> / <em>y</em> is known to be less than 1/2.  This third algorithm takes three parameters: <em>x</em>, <em>y</em>, and <em>m</em>, and <em>m</em> has to be chosen such that &lambda; * <em>x</em> / <em>y</em> &lt;= <em>m</em> &lt; 1/2.</p>

<ol>
<li>The same special cases as for the 2014 algorithm apply.</li>
<li>Run the <strong>logistic Bernoulli factory</strong> algorithm with <em>c</em>/<em>d</em> = (<em>x</em>/<em>y</em>) / (1 &minus; 2 * <em>m</em>).  If it returns 0, return 0.</li>
<li>With probability 1 &minus; 2 * <em>m</em>, return 1.</li>
<li>Run the 2014 algorithm or 2016 algorithm with <em>x</em>/<em>y</em> = (<em>x</em>/<em>y</em>) / (2 * <em>m</em>) and &#x03F5; = 1 &minus; <em>m</em>.</li>
</ol>

<p><a id=lambda__x___y___i></a></p>

<h4>(&lambda; * <em>x</em>/<em>y</em>)<sup><em>i</em></sup></h4>

<p>(Huber 2019)<sup><a href="#Note15"><strong>(15)</strong></a></sup>.  This algorithm, called the <strong>2019 algorithm</strong> in this document, uses four parameters: <em>x</em>, <em>y</em>, <em>i</em>, and &#x03F5;, such that <em>x</em>/<em>y</em> &gt; 0, <em>i</em> &gt;= 0 is an integer, and &#x03F5; is greater than 0.  When <em>x</em>/<em>y</em> is greater than 1, the &#x03F5; parameter has to be chosen such that &lambda; * <em>x</em>/<em>y</em> &lt; 1 &minus; &#x03F5;.  It also has special cases not mentioned in Huber 2019.</p>

<ol>
<li> Special cases: If <em>i</em> is 0, return 1.  If <em>x</em> is 0, return 0.  Otherwise, if <em>x</em> equals <em>y</em> and <em>i</em> equals 1, flip the input coin and return the result.</li>
<li>Special case: If <em>x</em> is less than <em>y</em> and <em>i</em> = 1, then: (a) With probability <em>x</em>/<em>y</em>, flip the input coin and return the result; otherwise (b) return 0.</li>
<li>Special case: If <em>x</em> is less than <em>y</em>, then create a secondary coin &mu; that does the following: &quot;(a) With probability <em>x</em>/<em>y</em>, flip the input coin and return the result; otherwise (b) return 0&quot;, then run the <strong>algorithm for (&mu;<sup><em>i</em>/1</sup>)</strong> (described earlier) using this secondary coin.</li>
<li>Set <em>t</em> to 355/100 and <em>c</em> to <em>x</em>/<em>y</em>.</li>
<li>If <em>i</em> is 0, return 1.</li>
<li>While <em>i</em> = <em>t</em> / &#x03F5;:

<ol>
<li>Set &beta; to (1 &minus; &#x03F5; / 2) / (1 &minus; &#x03F5;).</li>
<li>Run the <strong>algorithm for (1/&beta;)<sup><em>i</em></sup></strong> (described later).  If it returns 0, return 0.</li>
<li>Multiply <em>c</em> by &beta;, then divide &#x03F5; by 2.</li>
</ol></li>
<li>Run the <strong>logistic Bernoulli factory</strong> with <em>c</em>/<em>d</em> = <em>c</em>, then set <em>z</em> to the result.  Set <em>i</em> to <em>i</em> + 1 &minus; <em>z</em> * 2, then go to step 5.</li>
</ol>

<p><a id=x03F5_lambda></a></p>

<h4>&#x03F5; / &lambda;</h4>

<p>(Lee et al. 2014)<sup><a href="#Note16"><strong>(16)</strong></a></sup>  This algorithm, in addition to the input coin, takes a parameter &#x03F5;, which must be greater than 0 and be chosen such that &#x03F5; is less than &lambda;.</p>

<ol>
<li>If &beta; to max(&#x03F5;, 1/2) and set &gamma; to 1 &minus; (1 &minus; &beta;) / (1 &minus; (&beta; / 2)).</li>
<li>Create a &mu; input coin that flips the input coin and returns 1 minus the result.</li>
<li>With probability &#x03F5;, return 1.</li>
<li>Run the <strong>2014 algorithm</strong>, <strong>2016 algorithm</strong>, or <strong>2019 algorithm</strong>, with the &mu; input coin, <em>x</em>/<em>y</em> = 1 / (1 &minus; &#x03F5;),  <em>i</em> = 1 (for the 2019 algorithm), and &#x03F5; = &gamma;. If the result is 0, return 0.  Otherwise, go to step 3.  (Note that running the algorithm this way simulates the probability (&lambda; &minus; &#x03F5;)/(1 &minus; &#x03F5;) or 1 &minus; (1 &minus; &lambda;)/(1 &minus; &#x03F5;)).</li>
</ol>

<p><a id=Certain_Rational_Functions></a></p>

<h4>Certain Rational Functions</h4>

<p>According to (Mossel and Peres 2005)<sup><a href="#Note17"><strong>(17)</strong></a></sup>, a function can be simulated by a finite-state machine (equivalently, a &quot;probabilistic regular grammar&quot; (Smith and Johnson 2007)<sup><a href="#Note18"><strong>(18)</strong></a></sup>, (Icard 2019)<sup><a href="#Note19"><strong>(19)</strong></a></sup>) if and only if the function can be written as a rational function with rational coefficients, that takes in an input &lambda; in some subset of (0, 1) and outputs a number in the interval (0, 1).</p>

<p>The following algorithm is suggested from the Mossel and Peres paper and from (Thomas and Blanchet 2012)<sup><a href="#Note20"><strong>(20)</strong></a></sup>.  It assumes the rational function is of the form <em>D</em>(&lambda;)/<em>E</em>(&lambda;), where&mdash;</p>

<ul>
<li><em>D</em>(&lambda;) = &Sigma;<sub><em>i</em> = 0, ..., <em>n</em></sub> &lambda;<sup><em>i</em></sup> * (1 &minus; &lambda;)<sup><em>n</em> &minus; <em>i</em></sup> * <em>d</em>[<em>i</em>],</li>
<li><em>E</em>(&lambda;) = &Sigma;<sub><em>i</em> = 0, ..., <em>n</em></sub> &lambda;<sup><em>i</em></sup> * (1 &minus; &lambda;)<sup><em>n</em> &minus; <em>i</em></sup> * <em>e</em>[<em>i</em>],</li>
<li>every <em>d</em>[<em>i</em>] is less than or equal to the corresponding <em>e</em>[<em>i</em>], and</li>
<li>each <em>d</em>[<em>i</em>] and each <em>e</em>[<em>i</em>] is an integer or rational number in the interval [0, choose(<em>n</em>, <em>i</em>)], where the upper bound is the total number of <em>n</em>-bit words with <em>i</em> ones.</li>
</ul>

<p>Here, <em>d</em>[<em>i</em>] is akin to the number of &quot;passing&quot; <em>n</em>-bit words with <em>i</em> ones, and <em>e</em>[<em>i</em>] is akin to that number plus the number of &quot;failing&quot; <em>n</em>-bit words with <em>i</em> ones.  choose(<em>n</em>, <em>k</em>) = <em>n</em>!/(<em>k</em>! * (<em>n</em> &minus; <em>k</em>)!) is the binomial coefficient.</p>

<p>The algorithm follows.</p>

<ol>
<li>Flip the input coin <em>n</em> times, and let <em>j</em> be the number of times the coin returned 1 this way.</li>
<li>Call <strong>WeightedChoice</strong>(<strong>NormalizeRatios</strong>([<em>e</em>[<em>j</em>] &minus; <em>d</em>[<em>j</em>], <em>d</em>[<em>j</em>], choose(<em>n</em>, <em>j</em>) &minus; <em>e</em>[<em>j</em>]])), where <strong>WeightedChoice</strong> and <strong>NormalizeRatios</strong> are given in &quot;<a href="https://peteroupc.github.io/randomfunc.html"><strong>Randomization and Sampling Methods</strong></a>&quot;.  If the call returns 0 or 1, return that result.  Otherwise, go to step 1.</li>
</ol>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li><p>In the formulas above&mdash;</p>

<ul>
<li><em>d</em>[<em>i</em>] can be replaced with <em>&delta;</em>[<em>i</em>] * choose(<em>n</em>,<em>i</em>), where <em>&delta;</em>[<em>i</em>] is a rational number in the interval [0, 1] (and thus expresses the probability that a given word is a &quot;passing&quot; word among all <em>n</em>-bit words with <em>i</em> ones), and</li>
<li><em>e</em>[<em>i</em>] can be replaced with <em>&eta;</em>[<em>i</em>] * choose(<em>n</em>,<em>i</em>), where <em>&eta;</em>[<em>i</em>] is a rational number in the interval [0, 1] (and thus expresses the probability that a given word is a &quot;passing&quot; or &quot;failing&quot; word among all <em>n</em>-bit words with <em>i</em> ones),</li>
</ul>

<p>and then <em>&delta;</em>[<em>i</em>] and <em>&eta;</em>[<em>i</em>] can be seen as control points for two different 1-dimensional <a href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve"><strong>Bézier curves</strong></a>, where the <em>&delta;</em> curve is always on or &quot;below&quot; the <em>&eta;</em> curve.  For each curve, &lambda; is the relative position on that curve, the curve begins at  <em>&delta;</em>[0] or <em>&eta;</em>[0], and the curve ends at <em>&delta;</em>[<em>n</em>] or <em>&eta;</em>[<em>n</em>]. See also the next section.</p></li>
<li><p>This algorithm could be modified to avoid additional randomness besides the input coin flips by packing the coin flips into an <em>n</em>-bit word and looking up whether that word is &quot;passing&quot;, &quot;failing&quot;, or neither, among all <em>n</em>-bit words with <em>j</em> ones, but this is not so trivial to do (especially because in general, a lookup table first has to be built in a setup step, which can be impractical unless 2<sup><em>n</em></sup> is relatively small).  Moreover, this approach works only if <em>d</em>[<em>i</em>] and <em>e</em>[<em>i</em>] are integers (or rounded down to integers via the floor function, but this, of course, suffers from rounding error).  See also (Thomas and Blanchet 2012)<sup><a href="#Note20"><strong>(20)</strong></a></sup>.</p></li>
</ol>
</blockquote>

<p><a id=Bernstein_Polynomials></a></p>

<h4>Bernstein Polynomials</h4>

<p>A <em>Bernstein polynomial</em> is a polynomial of the form &Sigma;<sub><em>i</em> = 0, ..., <em>n</em></sub> choose(<em>n</em>, <em>i</em>) * &lambda;<sup><em>i</em></sup> * (1 &minus; &lambda;)<sup><em>n</em> &minus; <em>i</em></sup> * <em>a</em>[<em>i</em>], where <em>n</em> is the polynomial&#39;s degree and <em>a</em>[<em>i</em>] are the control points for the polynomial&#39;s corresponding Bézier curve.  According to (Goyal and Sigman 2012)<sup><a href="#Note21"><strong>(21)</strong></a></sup>, a function can be simulated with a fixed number of input coin flips if and only if it&#39;s a Bernstein polynomial whose control points are all in the interval [0, 1] (see also (Wästlund 1999, section 4)<sup><a href="#Note22"><strong>(22)</strong></a></sup>).  They also give an algorithm for simulating these polynomials, which is given below.</p>

<ol>
<li>Flip the input coin <em>n</em> times, and let <em>j</em> be the number of times the coin returned 1 this way.</li>
<li>With probability <em>a</em>[<em>j</em>], return 1.  Otherwise, return 0.</li>
</ol>

<blockquote>
<p><strong>Note</strong>: Each <em>a</em>[<em>i</em>] acts as a control point for a 1-dimensional <a href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve"><strong>Bézier curve</strong></a>, where &lambda; is the relative position on that curve, the curve begins at  <em>a</em>[0], and the curve ends at <em>a</em>[<em>n</em>].  For example, given control points 0.2, 0.3, and 0.6, the curve is at 0.2 when &lambda; = 0, and 0.6 when &lambda; = 1.  (Note that the curve is not at 0.3 when &lambda; = 1/2; in general, Bézier curves do not cross their control points other than the first and the last.)</p>
</blockquote>

<p><a id=Certain_Algebraic_Functions></a></p>

<h4>Certain Algebraic Functions</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup> showed how certain algebraic functions can be simulated by generating a bitstring and determining whether that bitstring belongs to a certain class of valid bitstrings.  The rules for determining whether a bitstring is valid are called a <em>binary stochastic grammar</em>, which uses an alphabet of only two &quot;letters&quot;.</p>

<p>The following algorithm simulates the following algebraic function<sup><a href="#Note23"><strong>(23)</strong></a></sup>:</p>

<ul>
<li>&Sigma;<sub><em>k</em> = 0, 1, 2, ...</sub> (&lambda;<sup><em>k</em></sup> * (1 &minus; &lambda;) * W(<em>k</em>) / &beta;<sup><em>k</em></sup>), or alternatively,</li>
<li>(1 &minus; &lambda;) * OGF(&lambda;/&beta;),</li>
</ul>

<p>where&mdash;</p>

<ul>
<li>&beta; is 2,</li>
<li>W(<em>k</em>) is the number of valid <em>k</em>-letter words,</li>
<li>the <em>ordinary generating function</em> OGF(<em>x</em>) = W(0) + W(1) * <em>x</em> + W(2) * <em>x</em><sup>2</sup> + W(3) * <em>x</em><sup>3</sup> + ..., and</li>
<li>the second formula incorporates a correction to Theorem 3.2 of the paper<sup><a href="#Note24"><strong>(24)</strong></a></sup>.</li>
</ul>

<p>(Here, the <em>k</em><sup>th</sup> coefficient of OGF(<em>x</em>) corresponds to W(<em>k</em>).)  The algorithm follows.</p>

<ol>
<li>Set <em>g</em> to 0.</li>
<li>With probability &lambda;, add 1 to <em>g</em> and repeat this step.  Otherwise, go to step 3.</li>
<li>Return a number that is 1 with probability W(<em>g</em>)/&beta;<sup><em>g</em></sup>, and 0 otherwise.  (In the Flajolet paper, this is done by generating a <em>g</em>-letter word uniformly at random and &quot;parsing&quot; that word using a binary stochastic grammar to determine whether that word is valid.  Note that the word can be determined to be valid as each of its &quot;letters&quot; is generated.)</li>
</ol>

<p>An extension to this algorithm, not mentioned in the Flajolet paper, is the use of stochastic grammars with a bigger alphabet than two &quot;letters&quot;.  For example, in the case of <em>ternary stochastic grammars</em>, the alphabet size is 3 and &beta; is 3 in the algorithm above.  In general, for <em>&beta;-ary stochastic grammars</em>, the alphabet size is &beta;, which can be any integer 2 or greater.</p>

<blockquote>
<p><strong>Examples:</strong> The following are examples from the Flajolet paper.</p>

<ol>
<li>A <em>g</em>-letter binary word can be &quot;parsed&quot; as follows to determine whether that word encodes a ternary tree: &quot;3. If <em>g</em> is 0, return 0.  Otherwise, set <em>i</em> to 1 and <em>d</em> to 1.; 3a. Generate an unbiased random bit, then subtract 1 from <em>d</em> if that bit is 0, or add 2 to <em>d</em> otherwise.; 3b. Add 1 to <em>i</em>. Then, if <em>i</em> &lt; <em>g</em> and <em>d</em> &gt; 0, go to step 3a.; 3c. Return 1 if <em>d</em> is 0 and <em>i</em> is <em>g</em>, or 0 otherwise.&quot;</li>
<li>If &beta; is 2 and if W(<em>g</em>), the number of valid <em>g</em>-letter words, has the form choose(<em>g</em>, <em>g</em>/<em>t</em>) if <em>g</em> is divisible by <em>t</em> (where <em>t</em> is 2 or greater), or 0 otherwise, step 3 of the algorithm can be done as follows: &quot;3. If <em>g</em> is not divisible by <em>t</em>, return 0. Otherwise, generate <em>g</em> unbiased random bits, then return 1 if exactly <em>g</em>/<em>t</em> zeros were generated this way, or 0 otherwise.&quot;</li>
</ol>

<p><strong>Note:</strong> The <em>square-root construction</em> sqrt(1 &minus; &lambda;) (mentioned in the Flajolet paper) can be expressed by a slightly different formula from the one given here, namely &Sigma;<sub><em>k</em> = 0, 1, 2, ...</sub> (&lambda;<sup><em>k</em></sup> * (1 &minus; &lambda;) * choose(<em>k</em> * &alpha;, <em>k</em>) / 2<sup><em>k</em> * &alpha;</sup>), where &alpha; = 2.  Thus, if the alphabet size is 2, the following replaces step 3 of the algorithm: &quot;3. Generate <em>g</em> * &alpha; unbiased random bits, then return 1 if exactly <em>g</em> zeros were generated this way, or 0 otherwise.&quot; Interestingly, I have found that if &alpha; is an integer 4 or greater, the formula simplifies to involve hypergeometric functions.  For example, for &alpha; = 6, the formula is (1 &minus; &lambda;) * <sub>5</sub><em>F</em><sub>4</sub>(1/6, 2/6, ..., 5/6; 1/5, ..., 4/5; 729 * &lambda; / 3125).</p>
</blockquote>

<p><a id=Expressions_Involving_Polylogarithms></a></p>

<h4>Expressions Involving Polylogarithms</h4>

<p>The following algorithm simulates the expression Li<sub><em>r</em></sub>(&lambda;) * (1 / &lambda; &minus; 1), where <em>r</em> is an integer 1 or greater.  If &lambda; is 1/2, this expression simplifies to Li<sub><em>r</em></sub>(1/2). See also (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.  Note, however, that even with a relatively small <em>r</em> such as 6, the expression quickly approaches a straight line.</p>

<ol>
<li>Flip the input coin until it returns 0, and let <em>t</em> be 1 plus the number of times the coin returned 1 this way.</li>
<li>Return a number that is 1 with probability 1/<em>t</em><sup><em>r</em></sup> and 0 otherwise.</li>
</ol>

<p><a id=Algorithms_for_Irrational_Constants></a></p>

<h3>Algorithms for Irrational Constants</h3>

<p>The following algorithms generate heads with a probability equal to an irrational number.  (On the other hand, probabilities that are <em>rational</em> constants are trivial to simulate.  If fair coins are available, the <code>ZeroOrOne</code> method, which is described in my article on <a href="https://peteroupc.github.io/randomfunc.html#Boolean_True_False_Conditions"><strong>random sampling methods</strong></a>, should be used.  If coins with unknown bias are available, then a <a href="https://peteroupc.github.io/randextract.html"><strong><em>randomness extraction</em></strong></a> method should be used to turn them into fair coins.)</p>

<p><a id=Digit_Expansions></a></p>

<h4>Digit Expansions</h4>

<p>Probabilities can be expressed as a digit expansion (of the form <code>0.dddddd...</code>).  The following algorithm returns 1 with probability <code>p</code> and 0 otherwise, where <code>p</code> is a probability in the interval [0, 1).  Note that the number 0 is also an infinite digit expansion of zeros, and the number 1 is also an infinite digit expansion of base-minus-ones.  Irrational numbers always have infinite digit expansions, which must be calculated &quot;on-the-fly&quot;.</p>

<p>In the algorithm (see also (Brassard et al., 2019)<sup><a href="#Note25"><strong>(25)</strong></a></sup>, (Devroye 1986, p. 769)<sup><a href="#Note7"><strong>(7)</strong></a></sup>), <code>BASE</code> is the digit base, such as 2 for binary or 10 for decimal.</p>

<ol>
<li>Set <code>u</code> to 0 and <code>k</code> to 1.</li>
<li>Set <code>u</code> to <code>(u * BASE) + v</code>, where <code>v</code> is a random integer in the interval [0, <code>BASE</code>) (such as <code>RNDINTEXC(BASE)</code>, or simply an unbiased random bit if <code>BASE</code> is 2).  Set <code>pk</code> to <code>p</code>&#39;s digit expansion up to the <code>k</code> digits after the point.  Example: If <code>p</code> is &pi;/4, <code>BASE</code> is 10, and <code>k</code> is 5, then <code>pk = 78539</code>.</li>
<li>If <code>pk + 1 &lt;= u</code>, return 0.  If <code>pk - 2 &gt;= u</code>, return 1.  If neither is the case, add 1 to <code>k</code> and go to step 2.</li>
</ol>

<p><a id=Continued_Fractions></a></p>

<h4>Continued Fractions</h4>

<p>The following algorithm simulates a probability expressed as a simple continued fraction of the following form: 0 + 1 / (<em>a</em>[1] + 1 / (<em>a</em>[2] + 1 / (<em>a</em>[3] + ... ))).  The <em>a</em>[<em>i</em>] are the <em>partial denominators</em>, none of which may have an absolute value less than 1.  Inspired by (Flajolet et al., 2010, &quot;Finite graphs (Markov chains) and rational functions&quot;)<sup><a href="#Note1"><strong>(1)</strong></a></sup>, I developed the following algorithm.</p>

<p>Algorithm 1. This algorithm works only if each <em>a</em>[<em>i</em>]&#39;s absolute value is 1 or greater and <em>a</em>[1] is positive, but otherwise, each  <em>a</em>[<em>i</em>] may be negative and/or a non-integer.  The algorithm begins with <em>pos</em> equal to 1.  Then the following steps are taken.</p>

<ol>
<li>Set <em>k</em> to <em>a</em>[<em>pos</em>].</li>
<li>If the partial denominator at <em>pos</em> is the last, return a number that is 1 with probability 1/<em>k</em> and 0 otherwise.</li>
<li>If <em>a</em>[<em>pos</em>] is less than 0, set <em>kp</em> to <em>k</em> &minus; 1 and <em>s</em> to 0.  Otherwise, set <em>kp</em> to <em>k</em> and <em>s</em> to 1. (This step accounts for negative partial denominators.)</li>
<li>With probability <em>kp</em>/(1+<em>kp</em>), return a number that is 1 with probability 1/<em>kp</em> and 0 otherwise.</li>
<li>Run this algorithm recursively, but with <em>pos</em> = <em>pos</em> + 1.  If the result is <em>s</em>, return 0.  Otherwise, go to step 4.</li>
</ol>

<p>A <em>generalized continued fraction</em> has the form 0 + <em>b</em>[1] / (<em>a</em>[1] + <em>b</em>[2] / (<em>a</em>[2] + <em>b</em>[3] / (<em>a</em>[3] + ... ))).  The <em>a</em>[<em>i</em>] are the same as before, but the <em>b</em>[<em>i</em>] are the <em>partial numerators</em>. The following are two algorithms to simulate a probability in the form of a generalized continued fraction.</p>

<p>Algorithm 2. This algorithm works only if each <em>b</em>[<em>i</em>]/<em>a</em>[<em>i</em>] is 1 or less, but otherwise, each <em>b</em>[<em>i</em>] and each  <em>a</em>[<em>i</em>] may be negative and/or a non-integer.  This algorithm employs an equivalence transform from generalized to simple continued fractions.  The algorithm begins with <em>pos</em> and <em>r</em> both equal to 1.  Then the following steps are taken.</p>

<ol>
<li>Set <em>r</em> to 1 / (<em>r</em> * <em>b</em>[<em>pos</em>]), then set <em>k</em> to <em>a</em>[<em>pos</em>] * <em>r</em>. (<em>k</em> is the partial denominator for the equivalent simple continued fraction.)</li>
<li>If the partial numerator/denominator pair at <em>pos</em> is the last, return a number that is 1 with probability 1/abs(<em>k</em>) and 0 otherwise.</li>
<li>Set <em>kp</em> to abs(<em>k</em>) and <em>s</em> to 1.</li>
<li>Set <em>r2</em> to 1 / (<em>r</em> * <em>b</em>[<em>pos</em> + 1]).  If <em>a</em>[<em>pos</em> + 1] * <em>r2</em> is less than 0, set <em>kp</em> to <em>kp</em> &minus; 1 and <em>s</em> to 0. (This step accounts for negative partial numerators and denominators.)</li>
<li>With probability <em>kp</em>/(1+<em>kp</em>), return a number that is 1 with probability 1/<em>kp</em> and 0 otherwise.</li>
<li>Run this algorithm recursively, but with <em>pos</em> = <em>pos</em> + 1 and <em>r</em> = <em>r</em>.  If the result is <em>s</em>, return 0.  Otherwise, go to step 5.</li>
</ol>

<p>Algorithm 3. This algorithm works only if each <em>b</em>[<em>i</em>]/<em>a</em>[<em>i</em>] is 1 or less and if each <em>b</em>[<em>i</em>] and each  <em>a</em>[<em>i</em>] is greater than 0, but otherwise, each <em>b</em>[<em>i</em>] and each <em>a</em>[<em>i</em>] may be a non-integer.  The algorithm begins with <em>pos</em> equal to 1.  Then the following steps are taken.</p>

<ol>
<li>If the partial numerator/denominator pair at <em>pos</em> is the last, return a number that is 1 with probability <em>b</em>[<em>pos</em>]/<em>a</em>[<em>pos</em>] and 0 otherwise.</li>
<li>With probability <em>a</em>[<em>pos</em>]/(1 + <em>a</em>[<em>pos</em>]), return a number that is 1 with probability <em>b</em>[<em>pos</em>]/<em>a</em>[<em>pos</em>] and 0 otherwise.</li>
<li>Run this algorithm recursively, but with <em>pos</em> = <em>pos</em> + 1.  If the result is 1, return 0.  Otherwise, go to step 2.</li>
</ol>

<p>See the appendix for a correctness proof of Algorithm 3.</p>

<blockquote>
<p><strong>Notes:</strong></p>

<ul>
<li><p>If any of these algorithms encounters a probability outside the interval [0, 1], the entire algorithm will fail for that continued fraction.</p></li>
<li><p>These algorithms will work for continued fractions of the form &quot;1 &minus; ...&quot; (rather than &quot;0 + ...&quot;) if&mdash;</p>

<ul>
<li>before running the algorithm, the first partial numerator and denominator have their sign removed, and</li>
<li>after running the algorithm, 1 minus the result (rather than just the result) is taken.</li>
</ul></li>
<li><p>These algorithms are designed to allow the partial numerators and denominators to be calculated &quot;on the fly&quot;.</p></li>
<li>The following is an alternative way to write Algorithm 1, which better shows the inspiration because it shows how the &quot;even parity construction&quot; (or the two-coin special case) as well as the &quot;1 &minus; <em>x</em>&quot; construction can be used to develop rational number simulators that are as big as their continued fraction expansions, as suggested in the cited part of the Flajolet paper.  However, it only works if the size of the continued fraction expansion (here, <em>size</em>) is known in advance.

<ol>
<li>Set <em>i</em> to <em>size</em>.</li>
<li>Create an input coin that does the following: &quot;Return a number that is 1 with probability 1/<em>a</em>[<em>size</em>] or 0 otherwise&quot;.</li>
<li>While <em>i</em> is 1 or greater:

<ol>
<li>Set <em>k</em> to <em>a</em>[<em>i</em>].</li>
<li>Create an input coin that takes the previous input coin and <em>k</em> and does the following: &quot;(a) With probability <em>k</em>/(1+<em>k</em>), return a number that is 1 with probability 1/<em>k</em> and 0 otherwise; (b) Flip the previous input coin.  If the result is 1, return 0.  Otherwise, go to step (a)&quot;.  (The probability <em>k</em>/(1+<em>k</em>) is related to &lambda;/(1+&lambda;) = 1 &minus; 1/(1+&lambda;), which involves the even-parity construction&mdash;or the two-coin special case&mdash;for 1/(1+&lambda;) as well as complementation for &quot;1 &minus; <em>x</em>&quot;.)</li>
<li>Subtract 1 from <em>i</em>.</li>
</ol></li>
<li>Flip the last input coin created by this algorithm, and return the result.</li>
</ol></li>
</ul>
</blockquote>

<p><a id=Continued_Logarithms></a></p>

<h4>Continued Logarithms</h4>

<p>The <em>continued logarithm</em> (Gosper 1978)<sup><a href="#Note26"><strong>(26)</strong></a></sup>, (Borwein et al., 2016)<sup><a href="#Note27"><strong>(27)</strong></a></sup> of a number in (0, 1) has the following continued fraction form: 0 + (1 / 2<sup><em>c</em>[1]</sup>) / (1 + (1 / 2<sup><em>c</em>[2]</sup>) / (1 + ...)), where <em>c</em>[<em>i</em>] are the coefficients of the continued logarithm and all 0 or greater.  I have come up with the following algorithm that simulates a probability expressed as a continued logarithm expansion.</p>

<p>The algorithm begins with <em>pos</em> equal to 1.  Then the following steps are taken.</p>

<ol>
<li>If the coefficient at <em>pos</em> is the last, return a number that is 1 with probability 1/(2<sup><em>c</em>[<em>pos</em>]</sup>) and 0 otherwise.</li>
<li>With probability 1/2, return a number that is 1 with probability 1/(2<sup><em>c</em>[<em>pos</em>]</sup>) and 0 otherwise.</li>
<li>Run this algorithm recursively, but with <em>pos</em> = <em>pos</em> + 1.  If the result is 1, return 0.  Otherwise, go to step 2.</li>
</ol>

<p>For a correctness proof, see the appendix.</p>

<p><a id=1_phi></a></p>

<h4>1 / &phi;</h4>

<p>This algorithm uses the algorithm described in the section on continued fractions to simulate 1 divided by the golden ratio, whose continued fraction&#39;s partial denominators are 1, 1, 1, 1, ....</p>

<ol>
<li>With probability 1/2, return 1.</li>
<li>Run this algorithm recursively.  If the result is 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=sqrt_2_minus_1></a></p>

<h4>sqrt(2) &minus; 1</h4>

<p>Another example of a continued fraction is that of the fractional part of the square root of 2, where the partial denominators are 2, 2, 2, 2, .... The algorithm to simulate this number is as follows:</p>

<ol>
<li>With probability 2/3, generate an unbiased random bit and return that bit.</li>
<li>Run this algorithm recursively.  If the result is 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=1_sqrt_2></a></p>

<h4>1/sqrt(2)</h4>

<p>This third example of a continued fraction shows how to simulate a probability 1/<em>z</em>, where <em>z</em> &gt; 1 has a known simple continued fraction expansion.  In this case, the partial denominators are as follows: floor(<em>z</em>), <em>a</em>[1], <em>a</em>[2], ..., where the <em>a</em>[<em>i</em>] are <em>z</em>&#39;s partial denominators (not including <em>z</em>&#39;s integer part).  In the example of 1/sqrt(2), the partial denominators are 1, 2, 2, 2, ..., where 1 comes first since floor(sqrt(2)) = 1.  The algorithm to simulate 1/sqrt(2) is as follows:</p>

<p>The algorithm begins with <em>pos</em> equal to 1.  Then the following steps are taken.</p>

<ol>
<li>If <em>pos</em> is 1, return 1 with probability 1/2.  If <em>pos</em> is greater than 1, then with probability 2/3, generate an unbiased random bit and return that bit.</li>
<li>Run this algorithm recursively, but with <em>pos</em> = <em>pos</em> + 1.  If the result is 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=tanh_1_2_or_exp_1_minus_1_exp_1_1></a></p>

<h4>tanh(1/2) or (exp(1) &minus; 1) / (exp(1) + 1)</h4>

<p>The algorithm begins with <em>k</em> equal to 2.  Then the following steps are taken.</p>

<ol>
<li>With probability <em>k</em>/(1+<em>k</em>), return a number that is 1 with probability 1/<em>k</em> and 0 otherwise.</li>
<li>Run this algorithm recursively, but with <em>k</em> = <em>k</em> + 4.  If the result is 1, return 0.  Otherwise, go to step 1.</li>
</ol>

<p><a id=arctan__x___y___y___x></a></p>

<h4>arctan(<em>x</em>/<em>y</em>) * <em>y</em>/<em>x</em></h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>.</li>
<li>Generate a number that is 1 with probability <em>x</em> * <em>x</em>/(<em>y</em> * <em>y</em>), or 0 otherwise.  If the number is 0, return 1.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> twice.  If either of these calls returns 0, return 1.</li>
<li>Generate a number that is 1 with probability <em>x</em> * <em>x</em>/(<em>y</em> * <em>y</em>), or 0 otherwise.  If the number is 0, return 0.</li>
<li><a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> twice.  If either of these calls returns 0, return 0.  Otherwise, go to step 2.</li>
</ol>

<p>Observing that the even-parity construction used in the Flajolet paper is equivalent to the two-coin special case, which is uniformly fast, the algorithm above can be made uniformly fast as follows:</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>.</li>
<li>With probability 1/2, return 1.</li>
<li>With probability <em>x</em> * <em>x</em>/(<em>y</em> * <em>y</em>), <a href="#Algorithms"><strong>sample from the number <em>u</em></strong></a> twice.  If both of these calls return 1, return 0.</li>
<li>Go to step 2.</li>
</ol>

<p><a id=pi_12></a></p>

<h4>&pi; / 12</h4>

<p>Two algorithms:</p>

<ul>
<li>First algorithm: Use the algorithm for <strong>arcsin(1/2) / 2</strong>.  Where the algorithm says to &quot;flip the input coin&quot;, instead generate an unbiased random bit.</li>
<li>Second algorithm: With probability 2/3, return 0.  Otherwise, run the algorithm for <strong>&pi; / 4</strong> and return the result.</li>
</ul>

<p><a id=pi_4></a></p>

<h4>&pi; / 4</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Generate a random integer in the interval [0, 6), call it <em>n</em>.</li>
<li>If <em>n</em> is less than 3, return the result of the <strong>algorithm for arctan(1/2) * 2</strong>.  Otherwise, if <em>n</em> is 3, return 0.  Otherwise, return the result of the <strong>algorithm for arctan(1/3) * 3</strong>.</li>
</ol>

<p><a id=1_pi></a></p>

<h4>1 / &pi;</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>:</p>

<ol>
<li>Set <em>t</em> to 0.</li>
<li>With probability 1/4, add 1 to <em>t</em> and repeat this step.  Otherwise, go to step 3.</li>
<li>With probability 1/4, add 1 to <em>t</em> and repeat this step.  Otherwise, go to step 4.</li>
<li>With probability 5/9, add 1 to <em>t</em>.</li>
<li>Generate 2*<em>t</em> unbiased random bits, and return 0 if there are more zeros than ones generated this way or more ones than zeros.  (Note that this condition can be checked even before all the bits are generated this way.)  Do this step two more times.</li>
<li>Return 1.</li>
</ol>

<p>For a sketch of how this algorithm is derived, see the appendix.</p>

<p><a id=a___b___x___y></a></p>

<h4>(<em>a</em>/<em>b</em>)<sup><em>x</em>/<em>y</em></sup></h4>

<p>In the algorithm below, <em>a</em>, <em>b</em>, <em>x</em>, and <em>y</em> are integers, and the case where <em>x</em>/<em>y</em> is in (0, 1) is due to recent work by Mendo (2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup>.  This algorithm works only if&mdash;</p>

<ul>
<li> <em>x</em>/<em>y</em> is 0 or greater and <em>a</em>/<em>b</em> is in the interval [0, 1], or</li>
<li> <em>x</em>/<em>y</em> is less than 0 and <em>a</em>/<em>b</em> is 1 or greater.</li>
</ul>

<p>The algorithm follows.</p>

<ol>
<li>If <em>x</em>/<em>y</em> is less than 0, swap <em>a</em> and <em>b</em>, and remove the sign from <em>x</em>/<em>y</em>.  If <em>a</em>/<em>b</em> is now no longer in the interval [0, 1], return an error.</li>
<li>If <em>x</em>/<em>y</em> is equal to 1, return 1 with probability <em>a</em>/<em>b</em> and 0 otherwise.</li>
<li>If <em>x</em> is 0, return 1.  Otherwise, if <em>a</em> is 0, return 0.  Otherwise, if <em>a</em> equals <em>b</em>, return 1.</li>
<li>If <em>x</em>/<em>y</em> is greater than 1:

<ol>
<li>Set <em>ipart</em> to floor(<em>x</em>/<em>y</em>) and <em>fpart</em> to <code>rem(x, y)</code>.</li>
<li>If <em>fpart</em> is greater than 0, subtract 1 from <em>ipart</em>, then call this algorithm recursively with <em>x</em> = floor(<em>fpart</em>/2) and <em>y</em> = <em>y</em>, then call this algorithm, again recursively, with <em>x</em> = <em>fpart</em> &minus; floor(<em>fpart</em>/2) and <em>y</em> = <em>y</em>. Return 0 if either call returns 0.  (This is done rather than the more obvious approach in order to avoid calling this algorithm with fractional parts very close to 0, because the algorithm runs much more slowly than for fractional parts closer to 1.)</li>
<li>If <em>ipart</em> is 1 or greater, generate a random number that is 1 with probability <em>a</em><sup><em>ipart</em></sup>/<em>b</em><sup><em>ipart</em></sup> or 0 otherwise. (Or generate <em>ipart</em> many random numbers that are each 1 with probability <em>a</em>/<em>b</em> or 0 otherwise, then multiply them all into one number.)  If that number is 0, return 0.</li>
<li>Return 1.</li>
</ol></li>
<li>Set <em>i</em> to 1.</li>
<li>With probability <em>a</em>/<em>b</em>, return 1.</li>
<li>Otherwise, with probability <em>x</em>/(<em>y</em>*<em>i</em>), return 0.</li>
<li>Add 1 to <em>i</em> and go to step 6.</li>
</ol>

<p><a id=exp_minus__x___y></a></p>

<h4>exp(&minus;<em>x</em>/<em>y</em>)</h4>

<p>This algorithm takes integers <em>x</em> &gt;= 0 and <em>y</em> &gt; 0 and outputs 1 with probability <code>exp(-x/y)</code> or 0 otherwise. It originates from (Canonne et al. 2020)<sup><a href="#Note28"><strong>(28)</strong></a></sup>.</p>

<ol>
<li>Special case: If <em>x</em> is 0, return 1. (This is because the probability becomes <code>exp(0) = 1</code>.)</li>
<li>If <code>x &gt; y</code> (so <em>x</em>/<em>y</em> is greater than 1), call this algorithm (recursively) <code>floor(x/y)</code> times with <em>x</em> = <em>y</em> = 1 and once with <em>x</em> = <em>x</em> &minus; floor(<em>x</em>/<em>y</em>) * <em>y</em> and <em>y</em> = <em>y</em>.  Return 1 if all these calls return 1; otherwise, return 0.</li>
<li>Set <em>r</em> to 1 and <em>i</em> to 1.</li>
<li>Return <em>r</em> with probability (<em>y</em> * <em>i</em> &minus; <em>x</em>) / (<em>y</em> * <em>i</em>).</li>
<li>Set <em>r</em> to 1 &minus; <em>r</em>, add 1 to <em>i</em>, and go to step 4.</li>
</ol>

<p><a id=exp_minus__z></a></p>

<h4>exp(&minus;<em>z</em>)</h4>

<p>This algorithm is similar to the previous algorithm, except that the exponent, <em>z</em>, can be any real number 0 or greater, as long as <em>z</em> can be rewritten as the sum of one or more components whose fractional parts can each be simulated by a Bernoulli factory algorithm that outputs heads with probability equal to that fractional part. (This makes use of the identity exp(&minus;a) = exp(&minus;b) * exp(&minus;c).)</p>

<p>More specifically:</p>

<ol>
<li>Decompose <em>z</em> into <em>n</em> &gt; 0 positive components that sum to <em>z</em>.  For example, if <em>z</em> = 3.5, it can be decomposed into only one component, 3.5 (whose fractional part is trivial to simulate), and if <em>z</em> = &pi;, it can be decomposed into four components that are all (&pi; / 4), which has a not-so-trivial simulation described earlier on this page.</li>
<li>For each component <em>LC</em>[<em>i</em>] found this way, let <em>LI</em>[<em>i</em>] be floor(<em>LC</em>[<em>i</em>]) and let <em>LF</em>[<em>i</em>] be <em>LC</em>[<em>i</em>] &minus; floor(<em>LC</em>[<em>i</em>]) (<em>LC</em>[<em>i</em>]&#39;s fractional part).</li>
</ol>

<p>The algorithm is then as follows:</p>

<ul>
<li>For each component <em>LC</em>[<em>i</em>], call the <strong>algorithm for exp(&minus; <em>LI</em>[<em>i</em>]/1)</strong>, and call the <strong>general martingale algorithm</strong> adapted for <strong>exp(&minus;&lambda;)</strong> using the input coin that simulates  <em>LF</em>[<em>i</em>].  If any of these calls returns 0, return 0; otherwise, return 1. (See also (Canonne et al. 2020)<sup><a href="#Note28"><strong>(28)</strong></a></sup>.)</li>
</ul>

<p><a id=a___b___z></a></p>

<h4>(<em>a</em>/<em>b</em>)<sup><em>z</em></sup></h4>

<p>This algorithm is similar to the previous algorithm for powering, except that the exponent, <em>z</em>,  can be any real number 0 or greater, as long as <em>z</em> can be rewritten as the sum of one or more components whose fractional parts can each be simulated by a Bernoulli factory algorithm that outputs heads with probability equal to that fractional part. This algorithm makes use of a similar identity as for <code>exp</code> and works only if <em>z</em> is 0 or greater and <em>a</em>/<em>b</em> is in the interval [0, 1].</p>

<p>Decompose <em>z</em> into <em>LC</em>[<em>i</em>], <em>LI</em>[<em>i</em>], and <em>LF</em>[<em>i</em>] just as for the <strong>exp(&minus; <em>z</em>)</strong> algorithm.  The algorithm is then as follows.</p>

<ul>
<li>If <em>z</em> is 0, return 1.  Otherwise, if <em>a</em> is 0, return 0.  Otherwise, for each component <em>LC</em>[<em>i</em>] (until the algorithm returns a number):

<ol>
<li>Call the <strong>algorithm for  (<em>a</em>/<em>b</em>)<sup><em>LI</em>[<em>i</em>]/1</sup></strong>.  If it returns 0, return 0.</li>
<li>Set <em>j</em> to 1.</li>
<li>Generate a random number that is 1 with probability <em>a</em>/<em>b</em> and 0 otherwise.  If that number is 1, abort these steps and move on to the next component or, if there are no more components, return 1.</li>
<li>Flip the input coin that simulates  <em>LF</em>[<em>i</em>] (which is the exponent); if it returns 1, return 0 with probability 1/<em>j</em>.</li>
<li>Add 1 to <em>j</em> and go to substep 2.</li>
</ol></li>
</ul>

<p><a id=1_1_exp__x___y__2_prec__LogisticExp></a></p>

<h4>1 / 1 + exp(<em>x</em> / (<em>y</em> * 2<sup><em>prec</em></sup>)) (LogisticExp)</h4>

<p>This is the probability that the bit at <em>prec</em> (the <em>prec</em><sup>th</sup> bit after the point) is set for an exponential random number with rate <em>x</em>/<em>y</em>.  This algorithm is a special case of the <strong>logistic Bernoulli factory</strong>.</p>

<ol>
<li>With probability 1/2, return 1.</li>
<li>Call the <strong>algorithm for exp(&minus; <em>x</em>/(<em>y</em> * 2<sup><em>prec</em></sup>))</strong>.  If the call returns 1, return 1.  Otherwise, go to step 1.</li>
</ol>

<p><a id=1_1_exp__z__2_prec__LogisticExp></a></p>

<h4>1 / 1 + exp(<em>z</em> / 2<sup><em>prec</em></sup>)) (LogisticExp)</h4>

<p>This is similar to the previous algorithm, except that <em>z</em> can be any real number described in the <strong>algorithm for exp(&minus;<em>z</em>)</strong>.</p>

<p>Decompose <em>z</em> into <em>LC</em>[<em>i</em>], <em>LI</em>[<em>i</em>], and <em>LF</em>[<em>i</em>] just as for the <strong>exp(&minus;<em>z</em>)</strong> algorithm.  The algorithm is then as follows.</p>

<ol>
<li>For each component <em>LC</em>[<em>i</em>], create an input coin that does the following: &quot;(a) With probability 1/(2<sup><em>prec</em></sup>), return 1 if the input coin that simulates <em>LF</em>[<em>i</em>] returns 1; (b) Return 0&quot;.</li>
<li>Return 0 with probability 1/2.</li>
<li>Call the <strong>algorithm for exp(&minus; <em>x</em>/<em>y</em>)</strong> with <em>x</em> = &Sigma;<sub><em>i</em></sub> <em>LI</em>[<em>i</em>] and <em>y</em> = 2<sup><em>prec</em></sup>.  If this call returns 0, go to step 2.</li>
<li>For each component <em>LC</em>[<em>i</em>], call the <strong>algorithm for exp(&minus;&lambda;)</strong>, using the corresponding input coin for  <em>LC</em>[<em>i</em>] created in step 1. If any of these calls returns 0, go to step 2.  Otherwise, return 1.</li>
</ol>

<p><a id=Polylogarithmic_Constants></a></p>

<h4>Polylogarithmic Constants</h4>

<p>The following algorithm simulates a polylogarithmic constant of the form Li<sub><em>r</em></sub>(1/2), where <em>r</em> is an integer 1 or greater.  See (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup> and &quot;Convex Combinations&quot; (the algorithm works by decomposing the series forming the polylogarithmic constant into <em>g</em>(<em>i</em>) = (1/2)<sup><em>i</em></sup>, which sums to 1, and <em>h</em><sub><em>i</em></sub>() = <em>i</em><sup><em>r</em></sup>, where <em>i</em> &gt;= 1).</p>

<ol>
<li>Set <em>t</em> to 1.</li>
<li>With probability 1/2, add 1 to <em>t</em> and repeat this step.  Otherwise, go to step 3.</li>
<li>Return a number that is 1 with probability 1/<em>t</em><sup><em>r</em></sup> and 0 otherwise.</li>
</ol>

<p><a id=zeta_3_3_4_and_Other_Zeta_Related_Constants></a></p>

<h4>&zeta;(3) * 3 / 4 and Other Zeta-Related Constants</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.  It can be seen as a triple integral whose integrand is 1/(1 + <em>a</em> * <em>b</em> * <em>c</em>), where <em>a</em>, <em>b</em>, and <em>c</em> are uniform random numbers.  This algorithm is given below, but using the two-coin special case instead of the even-parity construction.  Note that the triple integral in section 5 of the paper is &zeta;(3) * 3 / 4, not &zeta;(3) * 7 / 8.</p>

<ol>
<li>Generate three uniform(0,1) random numbers.</li>
<li>With probability 1/2, return 1.</li>
<li><a href="#Algorithms"><strong>Sample from each of the three numbers</strong></a> generated in step 1.  If all three calls return 1, return 0.  Otherwise, go to step 2. (This implements a triple integral involving the uniform random numbers.)</li>
</ol>

<p>This can be extended to cover any constant of the form &zeta;(<em>k</em>) * (1 &minus; 2<sup>&minus;(<em>k</em> &minus; 1)</sup>) where <em>k</em> &gt;= 2 is an integer, as suggested slightly by the Flajolet paper when it mentions &zeta;(5) * 31 / 32 (which should probably read &zeta;(5) * 15 / 16 instead), using the following algorithm.</p>

<ol>
<li>Generate <em>k</em> uniform(0,1) random numbers.</li>
<li>With probability 1/2, return 1.</li>
<li><a href="#Algorithms"><strong>Sample from each of the <em>k</em> numbers</strong></a> generated in step 1.  If all <em>k</em> calls return 1, return 0.  Otherwise, go to step 2.</li>
</ol>

<p><a id=erf__x__erf_1></a></p>

<h4>erf(<em>x</em>)/erf(1)</h4>

<p>In the following algorithm, <em>x</em> is a real number in the interval [0, 1].</p>

<ol>
<li>Generate a uniform(0, 1) random number, call it <em>ret</em>.</li>
<li>Set <em>u</em> to <em>ret</em>, and set <em>k</em> to 1.</li>
<li>(In this and the next step, we create <em>v</em>, which is the maximum of two uniform [0, 1] random numbers.) Generate two uniform random numbers, call them <em>a</em> and <em>b</em>.</li>
<li>If <em>a</em> is less than <em>b</em>, set <em>v</em> to <em>b</em>. Otherwise, set <em>v</em> to <em>a</em>.</li>
<li>If <em>v</em> is less than <em>u</em>, set <em>u</em> to <em>v</em>, then add 1 to <em>k</em>, then go to step 3.</li>
<li>If <em>k</em> is odd, return 1 if <em>ret</em> is less than <em>x</em>, or 0 otherwise. (If <em>ret</em> is implemented as a uniform PSRN, this comparison should be done via the <strong>URandLessThanReal algorithm</strong>, which is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Go to step 1.</li>
</ol>

<p>In fact, this algorithm takes advantage of a theorem related to the Forsythe method of random sampling (Forsythe 1972)<sup><a href="#Note29"><strong>(29)</strong></a></sup>.  See the section &quot;<a href="#Probabilities_Arising_from_the_Forsythe_Method"><strong>Probabilities Arising from the Forsythe Method</strong></a>&quot; in the appendix for more information.</p>

<p><a id=2_1_exp_2_or_1_exp_0_1_exp_1></a></p>

<h4>2 / (1 + exp(2)) or (1 + exp(0)) / (1 + exp(1))</h4>

<p>This algorithm takes advantage of formula 2 mentioned in the section &quot;<a href="#Probabilities_Arising_from_the_Forsythe_Method"><strong>Probabilities Arising from the Forsythe Method</strong></a>&quot; in the appendix.  Here, the relevant probability is rewritten as 1 &minus; (&int;<sub>(&minus;&infin;, 1)</sub> (1 &minus; exp(&minus;max(0, min(1, <em>z</em>)))) * exp(&minus;<em>z</em>) <em>dz</em>) / (&int;<sub>(&minus;&infin;, &infin;)</sub> (1 &minus; exp(&minus;max(0, min(1, <em>z</em>))) * exp(&minus;<em>z</em>) <em>dz</em>).</p>

<ol>
<li>Generate an <strong>exponential</strong> random number <em>ex</em>, then set <em>k</em> to 1.</li>
<li>Set <em>u</em> to <em>ex</em>.</li>
<li>Generate a <strong>uniform</strong> random number <em>v</em>.</li>
<li>Set <em>stop</em> to 1 if <em>u</em> is less than <em>v</em>, and 0 otherwise.</li>
<li>If <em>stop</em> is 1 and <em>k</em> <strong>is even</strong>, return a number that is 0 if <em>ex</em> is <strong>less than 1</strong>, and 1 otherwise.  Otherwise, if <em>stop</em> is 1, go to step 1.</li>
<li>Set <em>u</em> to <em>v</em>, then add 1 to <em>k</em>, then go to step 3.</li>
</ol>

<p><a id=1_exp_1_1_exp_2></a></p>

<h4>(1 + exp(1)) / (1 + exp(2))</h4>

<p>This algorithm takes advantage of the theorem mentioned in the section &quot;<a href="#Probabilities_Arising_from_the_Forsythe_Method"><strong>Probabilities Arising from the Forsythe Method</strong></a>&quot; in the appendix.  Here, the relevant probability is rewritten as 1 &minus; (&int;<sub>(&minus;&infin;, 1/2)</sub> exp(&minus;max(0, min(1, <em>z</em>))) * exp(&minus;<em>z</em>) <em>dz</em>) / (&int;<sub>(&minus;&infin;, &infin;)</sub> exp(&minus;max(0, min(1, <em>z</em>)) * exp(&minus;<em>z</em>) <em>dz</em>).</p>

<ol>
<li>Generate an <strong>exponential</strong> random number <em>ex</em>, then set <em>k</em> to 1.</li>
<li>Set <em>u</em> to <em>ex</em>.</li>
<li>Generate a <strong>uniform</strong> random number <em>v</em>.</li>
<li>Set <em>stop</em> to 1 if <em>u</em> is less than <em>v</em>, and 0 otherwise.</li>
<li>If <em>stop</em> is 1 and <em>k</em> <strong>is odd</strong>, return a number that is 0 if <em>ex</em> is <strong>less than 1/2</strong>, and 1 otherwise.  Otherwise, if <em>stop</em> is 1, go to step 1.</li>
<li>Set <em>u</em> to <em>v</em>, then add 1 to <em>k</em>, then go to step 3.</li>
</ol>

<p><a id=General_Algorithms></a></p>

<h3>General Algorithms</h3>

<p>&nbsp;</p>

<p><a id=Convex_Combinations></a></p>

<h4>Convex Combinations</h4>

<p>Assume we have one or more input coins <em>h</em><sub><em>i</em></sub>(&lambda;) that returns heads with a probability that depends on &lambda;.  (The number of coins may be infinite.) The following algorithm chooses one of these coins at random then flips that coin.  Specifically, the algorithm simulates the following function: <em>g</em>(0) * <em>h</em><sub>0</sub>(&lambda;) + <em>g</em>(1) * <em>h</em><sub>1</sub>(&lambda;) + ..., where <em>g</em>(<em>i</em>) is the probability that coin <em>i</em> will be chosen, <em>h</em><sub><em>i</em></sub> is the function simulated by coin <em>i</em>, and all the <em>g</em>(<em>i</em>) sum to 1.  See (Wästlund 1999, Theorem 2.7)<sup><a href="#Note22"><strong>(22)</strong></a></sup>.  (Alternatively, the algorithm can be seen as simulating <strong>E</strong>[<em>h</em><sub><em>X</em></sub>(&lambda;)], that is, the expected or average value of <em>h</em><sub><em>X</em></sub> where <em>X</em> is the number that identifies the randomly chosen coin.)</p>

<ol>
<li>Generate a random integer <em>X</em> in some way.  For example, it could be a uniform random integer in [1, 6], or it could be a Poisson random number.</li>
<li>Flip the coin represented by <em>X</em> and return the result.</li>
</ol>

<blockquote>
<p><strong>Examples:</strong></p>

<ol>
<li>Example 1. Generate a Poisson random number <em>X</em>, then flip the input coin.  With probability 1/(1+<em>X</em>), return the result of the coin flip; otherwise, return 0.</li>
<li>Example 2. Generate a Poisson(&mu;) random number <em>X</em> and return 1 if <em>X</em> is 0, or 0 otherwise.  This is a Bernoulli factory for exp(&minus;&mu;) mentioned earlier, and corresponds to <em>g</em>(<em>i</em>) being the Poisson(&mu;) probabilities and <em>h</em><sub><em>i</em></sub>() returning 1 if <em>i</em> is 0, and 0 otherwise.</li>
<li>Example 3. <em>Bernoulli Race</em> (Dughmi et al. 2017)<sup><a href="#Note8"><strong>(8)</strong></a></sup>: Say we have <em>n</em> coins, then choose one of them uniformly at random and flip that coin. If the flip returns 1, return <em>X</em>; otherwise, repeat this algorithm.  This algorithm chooses a random coin based on its probability of heads.</li>
</ol>
</blockquote>

<p><a id=Simulating_the_Probability_Generating_Function></a></p>

<h4>Simulating the Probability Generating Function</h4>

<p>The following algorithm is a special case of the convex combination method.  It generates heads with probability <strong>E</strong>[&lambda;<sup><em>X</em></sup>], that is, the expected or average value of &lambda;<sup><em>X</em></sup>.  <strong>E</strong>[&lambda;<sup><em>X</em></sup>] is the <em>probability generating function</em>, also known as <em>factorial moment generating function</em>, for the distribution of <em>X</em> (Dughmi et al. 2017)<sup><a href="#Note8"><strong>(8)</strong></a></sup>.</p>

<ol>
<li>Generate a random integer <em>X</em> in some way.  For example, it could be a uniform random integer in [1, 6], or it could be a Poisson random number.</li>
<li>Flip the input coin until the flip returns 0 or the coin is flipped <em>X</em> times.  Return 1 if all the coin flips, including the last, returned 1 (or if <em>X</em> is 0); or return 0 otherwise.</li>
</ol>

<p><a id=Integrals></a></p>

<h4>Integrals</h4>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup> showed how to turn an algorithm that simulates <em>f</em>(&lambda;) into an algorithm that simulates the following integral:</p>

<ul>
<li>(1/&lambda;) &int;<sub>[0, &lambda;]</sub> <em>f</em>(<em>u</em>) <em>du</em>, or equivalently,</li>
<li>&int;<sub>[0, 1]</sub> <em>f</em>(<em>u</em> * &lambda;) <em>du</em>.</li>
</ul>

<p>This can be done by modifying the algorithm as follows:</p>

<ul>
<li>Generate a uniform(0, 1) random number <em>u</em> at the start of the algorithm.</li>
<li>Instead of flipping the input coin, flip a coin that does the following: &quot;Flip the input coin, then <a href="#Algorithms"><strong>sample from the number <em>u</em></strong></a>.  Return 1 if both the call and the flip return 1, and return 0 otherwise.&quot;</li>
</ul>

<p>I have found that it&#39;s possible to simulate the following integral, namely&mdash;</p>

<ul>
<li>&int;<sub>[<em>a</em>, <em>b</em>]</sub> <em>f</em>(<em>u</em>) <em>du</em>,</li>
</ul>

<p>where [<em>a</em>, <em>b</em>] is [0, 1] or a closed interval therein, using different changes to the algorithm, namely:</p>

<ul>
<li>Add the following step at the start of the algorithm: &quot;Generate a uniform(0, 1) random number <em>u</em> at the start of the algorithm.  Then if <em>u</em> is less than <em>a</em> or is greater than <em>b</em>, repeat this step. (If <em>u</em> is a uniform PSRN, these comparisons should be done via the <strong>URandLessThanReal</strong> algorithm.)&quot;</li>
<li>Instead of flipping the input coin, flip a coin that does the following: &quot;<a href="#Algorithms"><strong>Sample from the number <em>u</em></strong></a> return the result.&quot;</li>
<li>If the algorithm would return 1, it returns 0 instead with probability 1 &minus; (<em>b</em> &minus; <em>a</em>).</li>
</ul>

<p><a id=Open_Questions></a></p>

<h2>Open Questions</h2>

<ul>
<li>Is there a simple Bernoulli factory algorithm that can simulate the probability (1+exp(<em>k</em>))/(1+exp(<em>k</em>+1)), without relying on floating-point arithmetic?</li>
<li>Is there a simple Bernoulli factory algorithm that can simulate the probability equal to Euler&#39;s constant &gamma;, without relying on floating-point arithmetic?  This repeats an open question given in (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup>.</li>
<li>See the open questions found in the section &quot;<a href="#Probabilities_Arising_from_Certain_Permutations"><strong>Probabilities Arising from Certain Permutations</strong></a>&quot; in the appendix.</li>
</ul>

<p><a id=Correctness_and_Performance_Charts></a></p>

<h2>Correctness and Performance Charts</h2>

<p>The following charts show the correctness of many of the algorithms on this page and show their performance in terms of the number of bits they use on average.  For each algorithm, and for each of 100 &lambda; values evenly spaced from 0.0001 to 0.9999:</p>

<ul>
<li>500 runs of the algorithm were done.  Then...</li>
<li>The number of bits used by the runs were averaged, as were the return values of the runs (since the return value is either 0 or 1, the mean return value will be in the interval [0, 1]).  The number of bits used included the number of bits used to produce each coin flip, assuming the coin flip procedure for &lambda; was generated using the <code>Bernoulli#coin()</code> method in <em>bernoulli.py</em>, which produces that probability in an optimal or near-optimal way.</li>
</ul>

<p>For each algorithm, if a single run was detected to use more than 5000 bits for a given &lambda;, the entire data point for that &lambda; was suppressed in the charts below.</p>

<p>In addition, for each algorithm, a chart appears showing the minimum number of input coin flips that any fast Bernoulli factory algorithm will need on average to simulate the given function, based on work by Mendo (2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup>.  Note that some functions require a growing number of coin flips as &lambda; approaches 0 or 1.  Note that for the 2014, 2016, and 2019 algorithms&mdash;</p>

<ul>
<li>an &#x03F5; of 1 &minus; (<em>x</em> + <em>c</em>) * 1.001 was used (or 0.0001 if &#x03F5; would be greater than 1), and</li>
<li>an &#x03F5; of (<em>x</em> &minus; <em>c</em>) * 0.9995 for the subtraction variants.</li>
</ul>

<p>Points with invalid &#x03F5; values were suppressed.  For the low-mean algorithm, an <em>m</em> of max(0.49999, <em>x</em><em><em>c</em></em>1.02) was used unless noted otherwise.</p>

<p><a id=The_Charts></a></p>

<h3>The Charts</h3>

<table><thead>
<tr>
<th>Algorithm</th>
<th>Simulated Mean</th>
<th>Average Bits Consumed</th>
<th>Coin Flips</th>
</tr>
</thead><tbody>
<tr>
<td>(1-x)*tan(x)</td>
<td><img src="bernoullicharts/1-x_tan_x__mean.svg" alt="**Simulated Mean for (1-x)\*tan(x)**"></td>
<td><img src="bernoullicharts/1-x_tan_x__bits.svg" alt="**Expected Bits Consumed by (1-x)\*tan(x)**"></td>
<td><img src="bernoullicharts/1-x_tan_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>(1-x)/cos(x)</td>
<td><img src="bernoullicharts/1-x_cos_x__mean.svg" alt="**Simulated Mean for (1-x)/cos(x)**"></td>
<td><img src="bernoullicharts/1-x_cos_x__bits.svg" alt="**Expected Bits Consumed by (1-x)/cos(x)**"></td>
<td><img src="bernoullicharts/1-x_cos_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>(1/3)*x/(1+(1/3)*x)</td>
<td><img src="bernoullicharts/1_3_x_1_1_3_x__mean.svg" alt="**Simulated Mean for (1/3)\*x/(1+(1/3)\*x)**"></td>
<td><img src="bernoullicharts/1_3_x_1_1_3_x__bits.svg" alt="**Expected Bits Consumed by (1/3)\*x/(1+(1/3)\*x)**"></td>
<td><img src="bernoullicharts/1_3_x_1_1_3_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>(2/3)*x/(1+(2/3)*x)</td>
<td><img src="bernoullicharts/2_3_x_1_2_3_x__mean.svg" alt="**Simulated Mean for (2/3)\*x/(1+(2/3)\*x)**"></td>
<td><img src="bernoullicharts/2_3_x_1_2_3_x__bits.svg" alt="**Expected Bits Consumed by (2/3)\*x/(1+(2/3)\*x)**"></td>
<td><img src="bernoullicharts/2_3_x_1_2_3_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>(3/2)*x/(1+(3/2)*x)</td>
<td><img src="bernoullicharts/3_2_x_1_3_2_x__mean.svg" alt="**Simulated Mean for (3/2)\*x/(1+(3/2)\*x)**"></td>
<td><img src="bernoullicharts/3_2_x_1_3_2_x__bits.svg" alt="**Expected Bits Consumed by (3/2)\*x/(1+(3/2)\*x)**"></td>
<td><img src="bernoullicharts/3_2_x_1_3_2_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>0.5*x/(1+0.5*x)</td>
<td><img src="bernoullicharts/0_5_x_1_0_5_x__mean.svg" alt="**Simulated Mean for 0.5\*x/(1+0.5\*x)**"></td>
<td><img src="bernoullicharts/0_5_x_1_0_5_x__bits.svg" alt="**Expected Bits Consumed by 0.5\*x/(1+0.5\*x)**"></td>
<td><img src="bernoullicharts/0_5_x_1_0_5_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>1 - ln(1+x) (Alt. Series)</td>
<td><img src="bernoullicharts/1_-_log_1_x_alt_series__mean.svg" alt="**Simulated Mean for 1 - ln(1+x) (Alt. Series)**"></td>
<td><img src="bernoullicharts/1_-_log_1_x_alt_series__bits.svg" alt="**Expected Bits Consumed by 1 - ln(1+x) (Alt. Series)**"></td>
<td><img src="bernoullicharts/1_-_log_1_x_alt_series__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>1/(1+x) (Alt. Series)</td>
<td><img src="bernoullicharts/1_1_x_alt_series__mean.svg" alt="**Simulated Mean for 1/(1+x) (Alt. Series)**"></td>
<td><img src="bernoullicharts/1_1_x_alt_series__bits.svg" alt="**Expected Bits Consumed by 1/(1+x) (Alt. Series)**"></td>
<td><img src="bernoullicharts/1_1_x_alt_series__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>1/(1+x) (Even Parity)</td>
<td><img src="bernoullicharts/1_1_x_even_parity__mean.svg" alt="**Simulated Mean for 1/(1+x) (Even Parity)**"></td>
<td><img src="bernoullicharts/1_1_x_even_parity__bits.svg" alt="**Expected Bits Consumed by 1/(1+x) (Even Parity)**"></td>
<td><img src="bernoullicharts/1_1_x_even_parity__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>1/(1+x) (Two-Coin Special Case)</td>
<td><img src="bernoullicharts/1_1_x_two-coin_special_case__mean.svg" alt="**Simulated Mean for 1/(1+x) (Two-Coin Special Case)**"></td>
<td><img src="bernoullicharts/1_1_x_two-coin_special_case__bits.svg" alt="**Expected Bits Consumed by 1/(1+x) (Two-Coin Special Case)**"></td>
<td><img src="bernoullicharts/1_1_x_two-coin_special_case__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>1/(3+x)</td>
<td><img src="bernoullicharts/1_3_x__mean.svg" alt="**Simulated Mean for 1/(3+x)**"></td>
<td><img src="bernoullicharts/1_3_x__bits.svg" alt="**Expected Bits Consumed by 1/(3+x)**"></td>
<td><img src="bernoullicharts/1_3_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>1/(5+x)</td>
<td><img src="bernoullicharts/1_5_x__mean.svg" alt="**Simulated Mean for 1/(5+x)**"></td>
<td><img src="bernoullicharts/1_5_x__bits.svg" alt="**Expected Bits Consumed by 1/(5+x)**"></td>
<td><img src="bernoullicharts/1_5_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 1.200000 eps=0.050000</td>
<td><img src="bernoullicharts/2014_1_200000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2014 1.200000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_1_200000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2014 1.200000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_1_200000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 1.500000 eps=0.050000</td>
<td><img src="bernoullicharts/2014_1_500000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2014 1.500000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_1_500000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2014 1.500000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_1_500000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 2.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2014_2_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2014 2.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_2_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2014 2.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_2_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 3.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2014_3_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2014 3.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_3_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2014 3.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_3_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 5.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2014_5_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2014 5.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_5_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2014 5.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2014_5_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Add. x+0.1</td>
<td><img src="bernoullicharts/2014_add_x_0_1_mean.svg" alt="**Simulated Mean for 2014 Add. x+0.1**"></td>
<td><img src="bernoullicharts/2014_add_x_0_1_bits.svg" alt="**Expected Bits Consumed by 2014 Add. x+0.1**"></td>
<td><img src="bernoullicharts/2014_add_x_0_1_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Add. x+0.2</td>
<td><img src="bernoullicharts/2014_add_x_0_2_mean.svg" alt="**Simulated Mean for 2014 Add. x+0.2**"></td>
<td><img src="bernoullicharts/2014_add_x_0_2_bits.svg" alt="**Expected Bits Consumed by 2014 Add. x+0.2**"></td>
<td><img src="bernoullicharts/2014_add_x_0_2_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Add. x+0.3</td>
<td><img src="bernoullicharts/2014_add_x_0_3_mean.svg" alt="**Simulated Mean for 2014 Add. x+0.3**"></td>
<td><img src="bernoullicharts/2014_add_x_0_3_bits.svg" alt="**Expected Bits Consumed by 2014 Add. x+0.3**"></td>
<td><img src="bernoullicharts/2014_add_x_0_3_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Add. x+0.5</td>
<td><img src="bernoullicharts/2014_add_x_0_5_mean.svg" alt="**Simulated Mean for 2014 Add. x+0.5**"></td>
<td><img src="bernoullicharts/2014_add_x_0_5_bits.svg" alt="**Expected Bits Consumed by 2014 Add. x+0.5**"></td>
<td><img src="bernoullicharts/2014_add_x_0_5_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Lin. x*1.3</td>
<td><img src="bernoullicharts/2014_lin_x_1_3_mean.svg" alt="**Simulated Mean for 2014 Lin. x\*1.3**"></td>
<td><img src="bernoullicharts/2014_lin_x_1_3_bits.svg" alt="**Expected Bits Consumed by 2014 Lin. x\*1.3**"></td>
<td><img src="bernoullicharts/2014_lin_x_1_3_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Lin. x*1.5</td>
<td><img src="bernoullicharts/2014_lin_x_1_5_mean.svg" alt="**Simulated Mean for 2014 Lin. x\*1.5**"></td>
<td><img src="bernoullicharts/2014_lin_x_1_5_bits.svg" alt="**Expected Bits Consumed by 2014 Lin. x\*1.5**"></td>
<td><img src="bernoullicharts/2014_lin_x_1_5_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Lin. x*2.0</td>
<td><img src="bernoullicharts/2014_lin_x_2_0_mean.svg" alt="**Simulated Mean for 2014 Lin. x\*2.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_2_0_bits.svg" alt="**Expected Bits Consumed by 2014 Lin. x\*2.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_2_0_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Lin. x*4.0</td>
<td><img src="bernoullicharts/2014_lin_x_4_0_mean.svg" alt="**Simulated Mean for 2014 Lin. x\*4.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_4_0_bits.svg" alt="**Expected Bits Consumed by 2014 Lin. x\*4.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_4_0_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Lin. x*6.0</td>
<td><img src="bernoullicharts/2014_lin_x_6_0_mean.svg" alt="**Simulated Mean for 2014 Lin. x\*6.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_6_0_bits.svg" alt="**Expected Bits Consumed by 2014 Lin. x\*6.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_6_0_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2014 Lin. x*8.0</td>
<td><img src="bernoullicharts/2014_lin_x_8_0_mean.svg" alt="**Simulated Mean for 2014 Lin. x\*8.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_8_0_bits.svg" alt="**Expected Bits Consumed by 2014 Lin. x\*8.0**"></td>
<td><img src="bernoullicharts/2014_lin_x_8_0_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2016 1.200000 eps=0.050000</td>
<td><img src="bernoullicharts/2016_1_200000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2016 1.200000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_1_200000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2016 1.200000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_1_200000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2016 1.500000 eps=0.050000</td>
<td><img src="bernoullicharts/2016_1_500000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2016 1.500000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_1_500000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2016 1.500000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_1_500000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2016 2.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2016_2_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2016 2.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_2_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2016 2.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_2_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2016 3.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2016_3_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2016 3.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_3_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2016 3.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_3_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2016 5.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2016_5_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2016 5.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_5_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2016 5.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2016_5_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2019 1.200000 eps=0.050000</td>
<td><img src="bernoullicharts/2019_1_200000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2019 1.200000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_1_200000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2019 1.200000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_1_200000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2019 1.500000 eps=0.050000</td>
<td><img src="bernoullicharts/2019_1_500000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2019 1.500000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_1_500000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2019 1.500000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_1_500000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2019 2.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2019_2_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2019 2.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_2_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2019 2.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_2_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2019 3.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2019_3_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2019 3.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_3_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2019 3.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_3_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>2019 5.000000 eps=0.050000</td>
<td><img src="bernoullicharts/2019_5_000000_eps_0_050000_mean.svg" alt="**Simulated Mean for 2019 5.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_5_000000_eps_0_050000_bits.svg" alt="**Expected Bits Consumed by 2019 5.000000 eps=0.050000**"></td>
<td><img src="bernoullicharts/2019_5_000000_eps_0_050000_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>Bernstein 0.2,0.6,0.3</td>
<td><img src="bernoullicharts/bernstein_0_2_0_6_0_3_mean.svg" alt="**Simulated Mean for Bernstein 0.2,0.6,0.3**"></td>
<td><img src="bernoullicharts/bernstein_0_2_0_6_0_3_bits.svg" alt="**Expected Bits Consumed by Bernstein 0.2,0.6,0.3**"></td>
<td><img src="bernoullicharts/bernstein_0_2_0_6_0_3_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>arcsin(x)+sqrt(1-x*x)-1</td>
<td><img src="bernoullicharts/arcsin_x_sqrt_1-x_x_-1_mean.svg" alt="**Simulated Mean for arcsin(x)+sqrt(1-x\*x)-1**"></td>
<td><img src="bernoullicharts/arcsin_x_sqrt_1-x_x_-1_bits.svg" alt="**Expected Bits Consumed by arcsin(x)+sqrt(1-x\*x)-1**"></td>
<td><img src="bernoullicharts/arcsin_x_sqrt_1-x_x_-1_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>arcsin(x)/2</td>
<td><img src="bernoullicharts/arcsin_x_2_mean.svg" alt="**Simulated Mean for arcsin(x)/2**"></td>
<td><img src="bernoullicharts/arcsin_x_2_bits.svg" alt="**Expected Bits Consumed by arcsin(x)/2**"></td>
<td><img src="bernoullicharts/arcsin_x_2_bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>arctan(x) (Flajolet)</td>
<td><img src="bernoullicharts/arctan_x_flajolet__mean.svg" alt="**Simulated Mean for arctan(x) (Flajolet)**"></td>
<td><img src="bernoullicharts/arctan_x_flajolet__bits.svg" alt="**Expected Bits Consumed by arctan(x) (Flajolet)**"></td>
<td><img src="bernoullicharts/arctan_x_flajolet__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>arctan(x) (Two-Coin Special Case)</td>
<td><img src="bernoullicharts/arctan_x_two-coin_special_case__mean.svg" alt="**Simulated Mean for arctan(x) (Two-Coin Special Case)**"></td>
<td><img src="bernoullicharts/arctan_x_two-coin_special_case__bits.svg" alt="**Expected Bits Consumed by arctan(x) (Two-Coin Special Case)**"></td>
<td><img src="bernoullicharts/arctan_x_two-coin_special_case__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>cos(x)</td>
<td><img src="bernoullicharts/cos_x__mean.svg" alt="**Simulated Mean for cos(x)**"></td>
<td><img src="bernoullicharts/cos_x__bits.svg" alt="**Expected Bits Consumed by cos(x)**"></td>
<td><img src="bernoullicharts/cos_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>exp(-x) (Alg. 2)</td>
<td><img src="bernoullicharts/exp_-x_alg_2__mean.svg" alt="**Simulated Mean for exp(-x) (Alg. 2)**"></td>
<td><img src="bernoullicharts/exp_-x_alg_2__bits.svg" alt="**Expected Bits Consumed by exp(-x) (Alg. 2)**"></td>
<td><img src="bernoullicharts/exp_-x_alg_2__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>exp(-x) (Alt. Series)</td>
<td><img src="bernoullicharts/exp_-x_alt_series__mean.svg" alt="**Simulated Mean for exp(-x) (Alt. Series)**"></td>
<td><img src="bernoullicharts/exp_-x_alt_series__bits.svg" alt="**Expected Bits Consumed by exp(-x) (Alt. Series)**"></td>
<td><img src="bernoullicharts/exp_-x_alt_series__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>exp(-x) (Flajolet)</td>
<td><img src="bernoullicharts/exp_-x_flajolet__mean.svg" alt="**Simulated Mean for exp(-x) (Flajolet)**"></td>
<td><img src="bernoullicharts/exp_-x_flajolet__bits.svg" alt="**Expected Bits Consumed by exp(-x) (Flajolet)**"></td>
<td><img src="bernoullicharts/exp_-x_flajolet__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>exp(x)*(1-x)</td>
<td><img src="bernoullicharts/exp_x_1-x__mean.svg" alt="**Simulated Mean for exp(x)\*(1-x)**"></td>
<td><img src="bernoullicharts/exp_x_1-x__bits.svg" alt="**Expected Bits Consumed by exp(x)\*(1-x)**"></td>
<td><img src="bernoullicharts/exp_x_1-x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>ln(1+x) (Flajolet)</td>
<td><img src="bernoullicharts/log_1_x_flajolet__mean.svg" alt="**Simulated Mean for ln(1+x) (Flajolet)**"></td>
<td><img src="bernoullicharts/log_1_x_flajolet__bits.svg" alt="**Expected Bits Consumed by ln(1+x) (Flajolet)**"></td>
<td><img src="bernoullicharts/log_1_x_flajolet__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>ln(1+x) (Two-Coin Special Case)</td>
<td><img src="bernoullicharts/log_1_x_two-coin_special_case__mean.svg" alt="**Simulated Mean for ln(1+x) (Two-Coin Special Case)**"></td>
<td><img src="bernoullicharts/log_1_x_two-coin_special_case__bits.svg" alt="**Expected Bits Consumed by ln(1+x) (Two-Coin Special Case)**"></td>
<td><img src="bernoullicharts/log_1_x_two-coin_special_case__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>pow(x,1/3)</td>
<td><img src="bernoullicharts/pow_x_1_3__mean.svg" alt="**Simulated Mean for pow(x,1/3)**"></td>
<td><img src="bernoullicharts/pow_x_1_3__bits.svg" alt="**Expected Bits Consumed by pow(x,1/3)**"></td>
<td><img src="bernoullicharts/pow_x_1_3__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>pow(x,2/1)</td>
<td><img src="bernoullicharts/pow_x_2_1__mean.svg" alt="**Simulated Mean for pow(x,2/1)**"></td>
<td><img src="bernoullicharts/pow_x_2_1__bits.svg" alt="**Expected Bits Consumed by pow(x,2/1)**"></td>
<td><img src="bernoullicharts/pow_x_2_1__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>pow(x,2/4)</td>
<td><img src="bernoullicharts/pow_x_2_4__mean.svg" alt="**Simulated Mean for pow(x,2/4)**"></td>
<td><img src="bernoullicharts/pow_x_2_4__bits.svg" alt="**Expected Bits Consumed by pow(x,2/4)**"></td>
<td><img src="bernoullicharts/pow_x_2_4__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>pow(x,3/4)</td>
<td><img src="bernoullicharts/pow_x_3_4__mean.svg" alt="**Simulated Mean for pow(x,3/4)**"></td>
<td><img src="bernoullicharts/pow_x_3_4__bits.svg" alt="**Expected Bits Consumed by pow(x,3/4)**"></td>
<td><img src="bernoullicharts/pow_x_3_4__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>pow(x,4/5)</td>
<td><img src="bernoullicharts/pow_x_4_5__mean.svg" alt="**Simulated Mean for pow(x,4/5)**"></td>
<td><img src="bernoullicharts/pow_x_4_5__bits.svg" alt="**Expected Bits Consumed by pow(x,4/5)**"></td>
<td><img src="bernoullicharts/pow_x_4_5__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>pow(x,5/1)</td>
<td><img src="bernoullicharts/pow_x_5_1__mean.svg" alt="**Simulated Mean for pow(x,5/1)**"></td>
<td><img src="bernoullicharts/pow_x_5_1__bits.svg" alt="**Expected Bits Consumed by pow(x,5/1)**"></td>
<td><img src="bernoullicharts/pow_x_5_1__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>pow(x,5/4)</td>
<td><img src="bernoullicharts/pow_x_5_4__mean.svg" alt="**Simulated Mean for pow(x,5/4)**"></td>
<td><img src="bernoullicharts/pow_x_5_4__bits.svg" alt="**Expected Bits Consumed by pow(x,5/4)**"></td>
<td><img src="bernoullicharts/pow_x_5_4__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>sin(x)</td>
<td><img src="bernoullicharts/sin_x__mean.svg" alt="**Simulated Mean for sin(x)**"></td>
<td><img src="bernoullicharts/sin_x__bits.svg" alt="**Expected Bits Consumed by sin(x)**"></td>
<td><img src="bernoullicharts/sin_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
<tr>
<td>sqrt(x)</td>
<td><img src="bernoullicharts/sqrt_x__mean.svg" alt="**Simulated Mean for sqrt(x)**"></td>
<td><img src="bernoullicharts/sqrt_x__bits.svg" alt="**Expected Bits Consumed by sqrt(x)**"></td>
<td><img src="bernoullicharts/sqrt_x__bound.svg" alt="**Coin Flips for the Function**"></td>
</tr>
</tbody></table>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<ul>
<li><small><sup id=Note1>(1)</sup> Flajolet, P., Pelletier, M., Soria, M., &quot;<a href="https://arxiv.org/abs/0906.5560v2"><strong>On Buffon machines and numbers</strong></a>&quot;, arXiv:0906.5560v2  [math.PR], 2010.</small></li>
<li><small><sup id=Note2>(2)</sup> Keane,  M.  S.,  and  O&#39;Brien,  G.  L., &quot;A Bernoulli factory&quot;, <em>ACM Transactions on Modeling and Computer Simulation</em> 4(2), 1994.</small></li>
<li><small><sup id=Note3>(3)</sup> Huber, M., &quot;<a href="https://arxiv.org/abs/1308.1562v2"><strong>Nearly optimal Bernoulli factories for linear functions</strong></a>&quot;, arXiv:1308.1562v2  [math.PR], 2014.</small></li>
<li><small><sup id=Note4>(4)</sup> Nacu, Şerban, and Yuval Peres. &quot;Fast simulation of new coins from old&quot;, The Annals of Applied Probability 15, no. 1A (2005): 93-115.</small></li>
<li><small><sup id=Note5>(5)</sup> Mendo, Luis. &quot;An asymptotically optimal Bernoulli factory for certain functions that can be expressed as power series.&quot; Stochastic Processes and their Applications 129, no. 11 (2019): 4366-4384.</small></li>
<li><small><sup id=Note6>(6)</sup> Łatuszyński, K., Kosmidis, I.,  Papaspiliopoulos, O., Roberts, G.O., &quot;<a href="https://arxiv.org/abs/0907.4018v2"><strong>Simulating events of unknown probabilities via reverse time martingales</strong></a>&quot;, arXiv:0907.4018v2 [stat.CO], 2009/2011.</small></li>
<li><small><sup id=Note7>(7)</sup> Devroye, L., <a href="http://luc.devroye.org/rnbookindex.html"><strong><em>Non-Uniform Random Variate Generation</em></strong></a>, 1986.</small></li>
<li><small><sup id=Note8>(8)</sup> Shaddin Dughmi, Jason D. Hartline, Robert Kleinberg, and Rad Niazadeh. 2017. Bernoulli Factories and Black-Box Reductions in Mechanism Design. In <em>Proceedings of 49th Annual ACM SIGACT Symposium on the Theory of Computing</em>, Montreal, Canada, June 2017 (STOC’17).</small></li>
<li><small><sup id=Note9>(9)</sup> Gonçalves, F. B., Łatuszyński, K. G., Roberts, G. O. (2017).  Exact Monte Carlo likelihood-based inference for jump-diffusion processes.</small></li>
<li><small><sup id=Note10>(10)</sup> Another algorithm for this function uses the general martingale algorithm, but uses more bits on average as &lambda; approaches 1.  Here, the alternating series is <code>1 - x + x^2/2 - x^3/3 + ...</code>, whose coefficients are 1, 1, 1/2, 1/3, ...</small></li>
<li><small><sup id=Note11>(11)</sup> Vats, D., Gonçalves, F. B., Łatuszyński, K. G., Roberts, G. O. &quot;<a href="https://arxiv.org/abs/2004.07471v1"><strong>Efficient Bernoulli factory MCMC for intractable likelihoods</strong></a>&quot;, arXiv:2004.07471v1 [stat.CO], 2020.</small></li>
<li><small><sup id=Note12>(12)</sup> Huber, M., &quot;<a href="https://arxiv.org/abs/1507.00843v2"><strong>Optimal linear Bernoulli factories for small mean problems</strong></a>&quot;, arXiv:1507.00843v2 [math.PR], 2016.</small></li>
<li><small><sup id=Note13>(13)</sup> Morina, G., Łatuszyński, K., et al., &quot;<a href="https://arxiv.org/abs/1912.09229v1"><strong>From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of Markov Chains</strong></a>&quot;, arXiv:1912.09229v1 [math.PR], 2019.</small></li>
<li><small><sup id=Note14>(14)</sup> One of the only implementations I could find of this, if not the only, was a <a href="https://github.com/derekelkins/buffon/blob/master/Data/Distribution/Buffon.hs"><strong>Haskell implementation</strong></a>.</small></li>
<li><small><sup id=Note15>(15)</sup> Huber, M., &quot;<a href="https://arxiv.org/abs/1907.06748v1"><strong>Designing perfect simulation algorithms using local correctness</strong></a>&quot;, arXiv:1907.06748v1 [cs.DS], 2019.</small></li>
<li><small><sup id=Note16>(16)</sup> Lee, A., Doucet, A. and Łatuszyński, K., 2014. Perfect simulation using atomic regeneration with application to Sequential Monte Carlo, arXiv:1407.5770v1  [stat.CO].</small></li>
<li><small><sup id=Note17>(17)</sup> Mossel, Elchanan, and Yuval Peres. New coins from old: computing with unknown bias. Combinatorica, 25(6), pp.707-724.</small></li>
<li><small><sup id=Note18>(18)</sup> Smith, N. A. and Johnson, M. (2007).  Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics, 33(4):477–491.</small></li>
<li><small><sup id=Note19>(19)</sup> Icard, Thomas F., &quot;Calibrating generative models: The probabilistic Chomsky–Schützenberger hierarchy.&quot; Journal of Mathematical Psychology 95 (2020): 102308.</small></li>
<li><small><sup id=Note20>(20)</sup> Thomas, A.C., Blanchet, J., &quot;<a href="https://arxiv.org/abs/1106.2508v3"><strong>A Practical Implementation of the Bernoulli Factory</strong></a>&quot;, arXiv:1106.2508v3  [stat.AP], 2012.</small></li>
<li><small><sup id=Note21>(21)</sup> Goyal, V. and Sigman, K., 2012. On simulating a class of Bernstein polynomials. ACM Transactions on Modeling and Computer Simulation (TOMACS), 22(2), pp.1-5.</small></li>
<li><small><sup id=Note22>(22)</sup> Wästlund, J., &quot;<a href="http://www.math.chalmers.se/%7Ewastlund/coinFlip.pdf"><strong>Functions arising by coin flipping</strong></a>&quot;, 1999.</small></li>
<li><small><sup id=Note23>(23)</sup> An algebraic function is a function that can be a root of a polynomial system.</small></li>
<li><small><sup id=Note24>(24)</sup> The probability given in Theorem 3.2 of the Flajolet paper, namely just &quot;&Sigma; <sub><em>k</em> = 0, 1, 2, ... </sub> (W(<em>k</em>) * (&lambda;/2)<sup><em>k</em></sup>)&quot;, appears to be incorrect in conjunction with Figure 4 of that paper.</small></li>
<li><small><sup id=Note25>(25)</sup> Brassard, G., Devroye, L., Gravel, C., &quot;Remote Sampling with Applications to General Entanglement Simulation&quot;, Entropy 2019(21)(92), doi:10.3390/e21010092.</small></li>
<li><small><sup id=Note26>(26)</sup> Bill Gosper, &quot;Continued Fraction Arithmetic&quot;, 1978.</small></li>
<li><small><sup id=Note27>(27)</sup> Borwein, J.M., Calkin, N.J., et al., &quot;Continued logarithms and associated continued fractions&quot;, 2016.</small></li>
<li><small><sup id=Note28>(28)</sup> Canonne, C., Kamath, G., Steinke, T., &quot;<a href="https://arxiv.org/abs/2004.00010v2"><strong>The Discrete Gaussian for Differential Privacy</strong></a>&quot;, arXiv:2004.00010v2 [cs.DS], 2020.</small></li>
<li><small><sup id=Note29>(29)</sup> Forsythe, G.E., &quot;Von Neumann&#39;s Comparison Method for Random Sampling from the Normal and Other Distributions&quot;, <em>Mathematics of Computation</em> 26(120), October 1972.</small></li>
<li><small><sup id=Note30>(30)</sup> von Neumann, J., &quot;Various techniques used in connection with random digits&quot;, 1951.</small></li>
<li><small><sup id=Note31>(31)</sup> Pae, S., &quot;Random number generation using a biased source&quot;, dissertation, University of Illinois at Urbana-Champaign, 2005.</small></li>
<li><small><sup id=Note32>(32)</sup> Peres, Y., &quot;Iterating von Neumann&#39;s procedure for extracting random bits&quot;, Annals of Statistics 1992,20,1, p. 590-597.</small></li>
<li><small><sup id=Note33>(33)</sup> Kozen, D., <a href="http://www.cs.cornell.edu/%7Ekozen/Papers/Coinflip.pdf"><strong>&quot;Optimal Coin Flipping&quot;</strong></a>, 2014.</small></li>
<li><small><sup id=Note34>(34)</sup> Devroye, L., Gravel, C., &quot;<a href="https://arxiv.org/abs/1502.02539v5"><strong>Sampling with arbitrary precision</strong></a>&quot;, arXiv:1502.02539v5 [cs.IT], 2015.</small></li>
<li><small><sup id=Note35>(35)</sup> As used here and in the Flajolet paper, a geometric random number is the number of successes before the first failure, where the success probability is &lambda;.</small></li>
<li><small><sup id=Note36>(36)</sup> Flajolet, P., Sedgewick, R., <em>Analytic Combinatorics</em>, Cambridge University Press, 2009.</small></li>
<li><small><sup id=Note37>(37)</sup> Monahan, J.. &quot;Extensions of von Neumann’s method for generating random variables.&quot; Mathematics of Computation 33 (1979): 1065-1069.</small></li>
</ul>

<p><a id=Appendix></a></p>

<h2>Appendix</h2>

<p>&nbsp;</p>

<p><a id=Randomized_vs_Non_Randomized_Algorithms></a></p>

<h3>Randomized vs. Non-Randomized Algorithms</h3>

<p>A <em>non-randomized algorithm</em> is a simulation algorithm that uses nothing but the input coin as a source of randomness (in contrast to <em>randomized algorithms</em>, which do use other sources of randomness) (Mendo 2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup>.  Instead of generating outside randomness, a randomized algorithm can implement a <a href="https://peteroupc.github.io/randextract.html"><strong><em>randomness extraction</em></strong></a> procedure to generate that randomness using the input coins themselves.  In this way, the algorithm becomes a <em>non-randomized algorithm</em>.  For example, if an algorithm implements the <strong>two-coin special case</strong> by generating a random bit in step 1, it could replace generating that bit with flipping the input coin twice until the flip returns 0 then 1 or 1 then 0 this way, then taking the result as 0 or 1, respectively (von Neumann 1951)<sup><a href="#Note30"><strong>(30)</strong></a></sup>.</p>

<p>In fact, there is a lower bound on the average number of coin flips needed to turn a coin with one bias (&lambda;) into a coin with another bias (&tau; = <em>f</em>(&lambda;)).  It&#39;s called the <em>entropy bound</em> (see, e.g., (Pae 2005)<sup><a href="#Note31"><strong>(31)</strong></a></sup>, (Peres 1992)<sup><a href="#Note32"><strong>(32)</strong></a></sup>) and is calculated as&mdash;</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;((&tau; &minus; 1) * ln(1 &minus; &tau;) &minus; &tau; * ln(&tau;)) /<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;((&lambda; &minus; 1) * ln(1 &minus; &lambda;) &minus; &lambda; * ln(&lambda;)).</p>

<p>For example, if <em>f</em>(&lambda;) is a constant, non-randomized algorithms will generally require a growing number of coin flips to simulate that constant if the input coin is strongly biased towards heads or tails (the bias is &lambda;).  Note that this formula only works if nothing but coin flips is allowed as randomness.  (For certain values of &lambda;, Kozen (2014)<sup><a href="#Note33"><strong>(33)</strong></a></sup> showed a tighter lower bound of this kind, but this bound is non-trivial and assumes &lambda; is known.)</p>

<p><a id=Simulating_Probabilities_vs_Estimating_Probabilities></a></p>

<h3>Simulating Probabilities vs. Estimating Probabilities</h3>

<p>A Bernoulli factory or another algorithm that produces heads with a given probability acts as an unbiased estimator for that probability (Łatuszyński et al. 2009/2011)<sup><a href="#Note6"><strong>(6)</strong></a></sup>.  (In this note, an <em>unbiased probability estimator</em> is an unbiased estimator whose estimates are in [0, 1] almost surely.) As a result&mdash;</p>

<ol>
<li>finding in some way an unbiased estimate of the input coin&#39;s probability of heads (&lambda;), such as by flipping the coin many times and averaging the results;</li>
<li>calculating <em>v</em> = <em>f</em>(&lambda;);</li>
<li>generating a uniform random number in [0,1], call it <em>u</em>; and</li>
<li>returning 1 if <em>u</em> is less than <em>v</em>, or 0 otherwise,</li>
</ol>

<p>will simulate the probability <em>f</em>(&lambda;) in theory.  In practice, however, this method is prone to numerous errors, including estimation error in step 1, and rounding and approximation errors in steps 2 and 3.  For this reason and also because &quot;exact sampling&quot; is the focus of this page, this document does not cover algorithms that directly estimate &lambda; (such as in step 1). As (Mossel and Peres 2005)<sup><a href="#Note17"><strong>(17)</strong></a></sup> says: &quot;The difficulty here is that [&lambda;] is unknown.  It is easy to estimate [&lambda;], and therefore [<em>f</em>(&lambda;)].  However, to get a coin with an exact bias [<em>f</em>(&lambda;)] is harder&quot;, and that is what Bernoulli factory algorithms are designed to do.</p>

<p>As also shown in (Łatuszyński et al. 2009/2011)<sup><a href="#Note6"><strong>(6)</strong></a></sup>, however, if <em>f</em>(&lambda;) can&#39;t serve as a factory function, no unbiased probability estimator of that function is possible, since sampling it isn&#39;t possible.  For example, function A can&#39;t serve as a factory function, so no simulator (or unbiased probability estimator) for that function is possible.  This <em>is</em> possible for function B, however (Keane and O&#39;Brien 1994)<sup><a href="#Note2"><strong>(2)</strong></a></sup>.</p>

<ul>
<li>Function A: 2 * &lambda;, when &lambda; lies in (0, 1/2).</li>
<li>Function B: 2 * &lambda;, when &lambda; lies in (0, 1/2 &minus; &#x03F5;), where &#x03F5; is in (0, 1/2).</li>
</ul>

<p><a id=Convergence_of_Bernoulli_Factories></a></p>

<h3>Convergence of Bernoulli Factories</h3>

<p>The following Python code illustrates how to test a Bernoulli factory algorithm for convergence to the correct probability, as well as the speed of this convergence.  In this case, we are testing the Bernoulli factory algorithm of <em>x</em><sup><em>y</em>/<em>z</em></sup>, where <em>x</em> is in the interval (0, 1) and <em>y</em>/<em>z</em> is greater than 0.  Depending on the parameters <em>x</em>, <em>y</em>, and <em>z</em>, this Bernoulli factory converges faster or slower.</p>

<pre># Parameters for the Bernoulli factory x**(y/z)
x=0.005 # x is the input coin&#39;s probability of heads
y=2
z=3
# Print the desired probability
print(x**(y/z))
passp = 0
failp = 0
# Set cumulative probability to 1
cumu = 1
iters=4000
for i in range(iters):
  # With probability x, the algorithm returns 1 (heads)
  prob=(x);prob*=cumu; passp+=prob; cumu-=prob
  # With probability (y/(z*(i+1))), the algorithm returns 0 (tails)
  prob=(y/(z*(i+1)));prob*=cumu; failp+=prob; cumu-=prob
  # Output the current probability in this iteration,
  # but only for the first 30 and last 30 iterations
  if i&lt;30 or i&gt;=iters-30: print(passp)
</pre>

<p>As this code shows, as <em>x</em> (the probability of heads of the input coin) approaches 0, the convergence rate gets slower and slower, even though the probability will eventually converge to the correct one. In fact, when <em>y</em>/<em>z</em> is less than 1:</p>

<ul>
<li>The average number of coin flips needed by this algorithm will grow without bound as <em>x</em> approaches 0, and Mendo (2019)<sup><a href="#Note5"><strong>(5)</strong></a></sup> showed that this is a lower bound; that is, no Bernoulli factory algorithm can do much better without knowing more information on <em>x</em>.</li>
<li><em>x</em><sup><em>y</em>/<em>z</em></sup> has a slope that tends to a vertical slope near 0, so that the so-called <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity"><strong><em>Lipschitz condition</em></strong></a> is not met at 0.  And (Nacu and Peres 2005, propositions 10 and 23)<sup><a href="#Note4"><strong>(4)</strong></a></sup> showed that the Lipschitz condition is necessary for a Bernoulli factory to have an upper bound on the average running time.</li>
</ul>

<p>Thus, a practical implementation of this algorithm may have to switch to an alternative implementation (such as the one described in the next section) when it detects that the first few digits (after the point) of the uniform random number&#39;s fractional part are zeros.</p>

<p><a id=Alternative_Implementation_of_Bernoulli_Factories></a></p>

<h3>Alternative Implementation of Bernoulli Factories</h3>

<p>Say we have a Bernoulli factory algorithm that takes a coin with probability of heads of <em>p</em> and outputs 1 with probability <em>f</em>(<em>p</em>).  If this algorithm takes a partially-sampled uniform random number (PSRN) as the input coin and flips that coin using <strong>SampleGeometricBag</strong> (a method described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>), the algorithm could instead be implemented as follows in order to return 1 with probability <em>f</em>(<em>U</em>), where <em>U</em> is the number represented by the uniform PSRN (see also (Brassard et al., 2019)<sup><a href="#Note25"><strong>(25)</strong></a></sup>, (Devroye 1986, p. 769)<sup><a href="#Note7"><strong>(7)</strong></a></sup>, (Devroye and Gravel 2015)<sup><a href="#Note34"><strong>(34)</strong></a></sup>.  This algorithm assumes the uniform PSRN&#39;s sign is positive and its integer part is 0.</p>

<ol>
<li>Set <em>v</em> to 0 and <em>k</em> to 1.</li>
<li>Set <em>v</em> to <em>b</em> * <em>v</em> + <em>d</em>, where <em>b</em> is the base (or radix) of the uniform PSRN&#39;s digits, and <em>d</em> is a digit chosen uniformly at random.</li>
<li>Calculate an approximation of <em>f</em>(<em>U</em>) as follows:

<ol>
<li>Set <em>n</em> to the number of items (sampled and unsampled digits) in the uniform PSRN&#39;s fractional part.</li>
<li>Of the first <em>n</em> digits (sampled and unsampled) in the PSRN&#39;s fractional part, sample each of the unsampled digits uniformly at random.  Then let <em>uk</em> be the PSRN&#39;s digit expansion up to the first <em>n</em> digits after the point.</li>
<li>Calculate the lowest and highest values of <em>f</em> in the interval [<em>uk</em>, <em>uk</em> + <em>b</em><sup>&minus;<em>n</em></sup>], call them <em>fmin</em> and <em>fmax</em>. If abs(<em>fmin</em> &minus; <em>fmax</em>) &lt;= 2 * <em>b</em><sup>&minus;<em>k</em></sup>, calculate (<em>fmax</em> + <em>fmin</em>) / 2 as the approximation.  Otherwise, add 1 to <em>n</em> and go to the previous substep.</li>
</ol></li>
<li>Let <em>pk</em> be the approximation&#39;s digit expansion up to the <em>k</em> digits after the point.  For example, if <em>f</em>(<em>U</em>) is &pi;, <em>b</em> is 10, and <em>k</em> is 2, <em>pk</em> is 314.</li>
<li>If <em>pk</em> + 1 &lt;= <em>v</em>, return 0. If <em>pk</em> &minus; 2 &gt;= <em>v</em>, return 1.  If neither is the case, add 1 to <em>k</em> and go to step 2.</li>
</ol>

<p>However, the focus of this article is on algorithms that don&#39;t rely on calculations of irrational numbers, which is why this section is in the appendix.</p>

<p><a id=Correctness_Proof_for_the_Continued_Logarithm_Simulation_Algorithm></a></p>

<h3>Correctness Proof for the Continued Logarithm Simulation Algorithm</h3>

<p><strong>Theorem.</strong> <em>The algorithm given in &quot;Continued Logarithms&quot; returns 1 with probability exactly equal to the number represented by the continued logarithm c, and 0 otherwise.</em></p>

<p><em>Proof.</em> This proof of correctness takes advantage of Huber&#39;s &quot;fundamental theorem of perfect simulation&quot; (Huber 2019)<sup><a href="#Note15"><strong>(15)</strong></a></sup>.  Using Huber&#39;s theorem requires proving two things:</p>

<ul>
<li>First, we note that the algorithm clearly halts almost surely, since step 1 will stop the algorithm if it reaches the last coefficient, and step 2 always gives a chance that the algorithm will return a value, even if it&#39;s called recursively or the number of coefficients is infinite.</li>
<li>Second, we show the algorithm is locally correct when the recursive call in step 3 is replaced with an oracle that simulates the correct &quot;continued sub-logarithm&quot;.  If step 1 reaches the last coefficient, the algorithm obviously passes with the correct probability.  Otherwise, we will be simulating the probability (1 / 2<sup><em>c</em>[<em>i</em>]</sup>) / (1 + <em>x</em>), where <em>x</em> is the &quot;continued sub-logarithm&quot; and will be at most 1 by construction.  Steps 2 and 3 define a loop that divides the probability space into three pieces: the first piece takes up one half, the second piece (step 3) takes up a portion of the other half (which here is equal to <em>x</em>/2), and the last piece is the &quot;rejection piece&quot; that reruns the loop.  Since this loop changes no variables that affect later iterations, each iteration acts like an acceptance/rejection algorithm already proved to be a perfect simulator by Huber.  The algorithm will pass at step 2 with probability <em>p</em> = (1 / 2<sup><em>c</em>[<em>i</em>]</sup>) / 2 and fail either at step 2 with probability <em>f1</em> = (1 &minus; 1 / 2<sup><em>c</em>[<em>i</em>]</sup>) / 2, or at step 3 with probability <em>f2</em> = <em>x</em>/2 (all these probabilities are relative to the whole iteration).  Finally, dividing the passes by the sum of passes and fails (<em>p</em> / (<em>p</em> + <em>f1</em> + <em>f2</em>)) leads to (1 / 2<sup><em>c</em>[<em>i</em>]</sup>) / (1 + <em>x</em>), which is the probability we wanted.</li>
</ul>

<p>Since both conditions of Huber&#39;s theorem are satisfied, this completes the proof. &#x25a1;</p>

<p><a id=Correctness_Proof_for_Continued_Fraction_Simulation_Algorithm_3></a></p>

<h3>Correctness Proof for Continued Fraction Simulation Algorithm 3</h3>

<p><strong>Theorem.</strong> <em>Suppose a generalized continued fraction&#39;s partial numerators are b[i] and all greater than 0, and its partial denominators are a[i] and all greater than 0, and suppose further that each b[i]/a[i] is 1 or less. Then the algorithm given as Algorithm 3 in &quot;Continued Fractions&quot; returns 1 with probability exactly equal to the number represented by that continued fraction, and 0 otherwise.</em></p>

<p><em>Proof.</em> We use Huber&#39;s &quot;fundamental theorem of perfect simulation&quot; again in the proof of correctness.</p>

<ul>
<li>The algorithm halts almost surely for the same reason as the similar continued logarithm simulator.</li>
<li>If the call in step 3 is replaced with an oracle that simulates the correct &quot;sub-fraction&quot;, the algorithm is locally correct.  If step 1 reaches the last element of the continued fraction, the algorithm obviously passes with the correct probability. Otherwise, we will be simulating the probability <em>b</em>[<em>i</em>] / (<em>a</em>[<em>i</em>] + <em>x</em>), where <em>x</em> is the &quot;continued sub-fraction&quot; and will be at most 1 by assumption.  Steps 2 and 3 define a loop that divides the probability space into three pieces: the first piece takes up a part equal to <em>h</em> = <em>a</em>[<em>i</em>]/(<em>a</em>[<em>i</em>] + 1), the second piece (step 3) takes up a portion of the remainder (which here is equal to <em>x</em> * (1 &minus; <em>h</em>)), and the last piece is the &quot;rejection piece&quot;.  The algorithm will pass at step 2 with probability <em>p</em> = (<em>b</em>[<em>i</em>] / <em>a</em>[<em>pos</em>]) * <em>h</em> and fail either at step 2 with probability <em>f1</em> = (1 &minus; <em>b</em>[<em>i</em>] / <em>a</em>[<em>pos</em>]) * <em>h</em>, or at step 3 with probability <em>f2</em> = <em>x</em> * (1 &minus; <em>h</em>) (all these probabilities are relative to the whole iteration).  Finally, dividing the passes by the sum of passes and fails leads to <em>b</em>[<em>i</em>] / (<em>a</em>[<em>i</em>] + <em>x</em>), which is the probability we wanted, so that both of Huber&#39;s conditions are satisfied and we are done.  &#x25a1;</li>
</ul>

<p><a id=The_von_Neumann_Schema></a></p>

<h3>The von Neumann Schema</h3>

<p>(Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup> describes what it calls the <em>von Neumann schema</em> (sec. 2).  Although the von Neumann schema is used in several Bernoulli factories given here, it&#39;s not a Bernoulli factory itself since it could produce random numbers other than 0 and 1, which is why this section appears in the appendix.  Given a permutation class and an input coin, the von Neumann schema generates a random non-negative integer <em>n</em> with probability equal to&mdash;</p>

<ul>
<li>(&lambda;<sup><em>n</em></sup> * V(<em>n</em>) / <em>n</em>!) / EGF(&lambda;),</li>
</ul>

<p>where&mdash;</p>

<ul>
<li>EGF(&lambda;) = &Sigma;<sub><em>k</em> = 0, 1, ...</sub> (&lambda;<sup><em>k</em></sup> * V(<em>k</em>) / <em>k</em>!) (the <em>exponential generating function</em> or EGF, which completely determines a permutation class), and</li>
<li>V(<em>n</em>) is the number of <em>valid</em> permutations of size <em>n</em> (and must be in the interval [0, <em>n</em>!]).</li>
</ul>

<p>Effectively, a geometric(&lambda;) random number <em>G</em><sup><a href="#Note35"><strong>(35)</strong></a></sup> is accepted with probability V(<em>G</em>)/<em>G</em>! (where <em>G</em>! is the number of <em>possible</em> permutations of size <em>G</em>, or 1 if <em>G</em> is 0), and rejected otherwise.  The probability that <em>r</em> geometric random numbers are rejected this way is <em>p</em>*(1 &minus; <em>p</em>)<sup><em>r</em></sup>, where <em>p</em> = (1 &minus; &lambda;) * EGF(&lambda;).</p>

<p>Examples of permutation classes include&mdash;</p>

<ul>
<li>single-cycle permutations (EGF(&lambda;) = Cyc(&lambda;) = ln(1/(1 &minus; &lambda;)); V(<em>n</em>) = (<em>n</em> &minus; 1)!)</li>
<li>sorted permutations (EGF(&lambda;) = Set(&lambda;) = exp(&lambda;); V(<em>n</em>) = 1),</li>
<li>all permutations (EGF(&lambda;) = Seq(&lambda;) = 1/(1 &minus; &lambda;); V(<em>n</em>) = <em>n</em>!),</li>
<li>alternating permutations of even size (EGF(&lambda;) = 1/cos(&lambda;); the V(<em>n</em>) starting at <em>n</em> = 0 is <a href="https://oeis.org/A000364"><strong>A000364</strong></a> in the <em>On-Line Encyclopedia of Integer Sequences</em>), and</li>
<li>alternating permutations of odd size (EGF(&lambda;) = tan(&lambda;); the V(<em>n</em>) starting at <em>n</em> = 0 is <a href="https://oeis.org/A000182"><strong>A000182</strong></a>),</li>
</ul>

<p>using the notation in &quot;Analytic Combinatorics&quot; (Flajolet and Sedgewick 2009)<sup><a href="#Note36"><strong>(36)</strong></a></sup>.</p>

<p>The following algorithm generates a random number that follows the von Neumann schema.</p>

<ol>
<li>Set <em>r</em> to 0. (This is the number of times the algorithm rejects a random number.)</li>
<li>Flip the input coin until the flip returns 0.  Then set <em>G</em> to the number of times the flip returns 1 this way.</li>
<li>With probability V(<em>G</em>)/<em>G</em>!, return <em>G</em> (or <em>r</em> if desired).  (In practice, the probability check is done by generating <em>G</em> uniform random numbers and determining whether those numbers satisfy the given permutation class, or generating as many of those numbers as necessary to make this determination.  This is especially because <em>G</em>!, the factorial of <em>G</em>, can easily become very large.)</li>
<li>Add 1 to <em>r</em> and go to step 2.</li>
</ol>

<p>A variety of Bernoulli factory probability functions can arise from the von Neumann schema, depending on the EGF and which values of <em>G</em> and/or <em>r</em> the Bernoulli factory algorithm treats as heads or tails.  The following Python functions use the SymPy computer algebra library to find probabilities and other useful information for applying the von Neumann schema, given a permutation class&#39;s EGF.</p>

<pre>def coeffext(f, x, power):
    # Extract a coefficient from a generating function
    # NOTE: Can also be done with just the following line:
    # return diff(f,(x,power)).subs(x,0)/factorial(power)
    px = 2
    for i in range(10):
      try:
        poly=Poly(series(f, x=x, n=power+px).removeO())
        if power == 0:
          return poly.coeff_monomial(1)
        return poly.as_expr().coeff(x**power)
      except:
        px+=2
    # Failed, assume 0
    return 0

def number_n_prob(f, x, n):
    # Probability that the number n is generated
    # for the von Neumann schema with the given
    # exponential generating function (e.g.f.)
    # Example: number_n_prob(exp(x),x,1) --&gt; x**exp(-x)
    return (x**n*coeffext(f, x, n))/f

def r_rejects_prob(f, x, r):
    # Probability that the von Neumann schema
    # with the given e.g.f. will reject r random numbers
    # before accepting the next one
    p=(1-x)*f
    return p*(1-p)**r

def valid_perm(f, x, n):
    # Number of valid permutations of size n for the
    # von Neumann schema with the given e.g.f.
    return coeffext(f, x, n)*factorial(n)
</pre>

<blockquote>
<p><strong>Note:</strong> The von Neumann schema can simulate any <em>power series distribution</em> (such as Poisson, negative binomial, geometric, and logarithmic series), given a suitable exponential generating function.</p>
</blockquote>

<p><a id=Probabilities_Arising_from_the_Forsythe_Method></a></p>

<h3>Probabilities Arising from the Forsythe Method</h3>

<p>The Forsythe method of random sampling (Forsythe 1972)<sup><a href="#Note29"><strong>(29)</strong></a></sup> gives rise to a class of interesting probability functions.</p>

<p>Let <em>D</em> and <em>E</em> be two probability distributions.  Draw one number from <em>D</em> (&delta;).  Then draw numbers from <em>E</em> (<em>e1</em>, <em>e2</em>, etc.) until a number drawn this way is greater than the previous drawn number (which can be &delta;).  Then count the numbers drawn from <em>E</em> this way, call the count <em>k</em>.  Then the probability that &delta; is less than <em>x</em> given that&mdash;</p>

<ul>
<li><em>k</em> is odd is (&int;<sub>(&minus;&infin;, <em>x</em>)</sub> exp(&minus;ECDF(<em>z</em>)) * DPDF(<em>z</em>) <em>dz</em>) / (&int;<sub>(&minus;&infin;, &infin;)</sub> exp(&minus;ECDF(<em>z</em>)) * DPDF(<em>z</em>) <em>dz</em>) (Formula 1; see Theorem 2.1(iii) of (Devroye 1986, Chapter IV)<sup><a href="#Note7"><strong>(7)</strong></a></sup>), or</li>
<li><em>k</em> is even is (&int;<sub>(&minus;&infin;, <em>x</em>)</sub> (1 &minus; exp(&minus;ECDF(<em>z</em>))) * DPDF(<em>z</em>) <em>dz</em>) / (&int;<sub>(&minus;&infin;, &infin;)</sub> (1 &minus; exp(&minus;ECDF(<em>z</em>))) * DPDF(<em>z</em>) <em>dz</em>) (Formula 2; see also (Monahan 1979)<sup><a href="#Note37"><strong>(37)</strong></a></sup>),</li>
</ul>

<p>where DPDF is the probability density function (PDF) of <em>D</em>, and ECDF is the cumulative distribution function  (CDF) of <em>E</em>.  For example, the algorithm to simulate <a href="#erf__x__erf_1"><strong>erf(<em>x</em>)/erf(1)</strong></a> uses the fact that when&mdash;</p>

<ul>
<li>DPDF is the uniform(0,1) distribution&#39;s PDF, which is 1 in the interval [0, 1] and 0 elsewhere, and</li>
<li>ECDF is the CDF for the maximum of two uniform(0,1) random numbers, which is simply <em>z</em><sup>2</sup>,</li>
</ul>

<p>then Formula 1 above becomes&mdash;</p>

<ul>
<li>(&int;<sub>[0, <em>x</em>]</sub> exp(&minus;(<em>z</em><sup>2</sup>)) <em>dz</em>) / (&int;<sub>[0, 1]</sub> exp(&minus;(<em>z</em><sup>2</sup>)) <em>dz</em>), &nbsp;&nbsp;&nbsp;(Formula 3)</li>
</ul>

<p>and thus erf(<em>x</em>)/erf(1).  If the last step in the algorithm reads &quot;Return 0&quot; rather than &quot;Go to step 1&quot;, then the algorithm simulates the probability erf(<em>x</em>)*sqrt(&pi;)/2 (and the denominator in Formulas 1 and 3 becomes 1).</p>

<p><a id=Probabilities_Arising_from_Certain_Permutations></a></p>

<h3>Probabilities Arising from Certain Permutations</h3>

<p>Consider the following algorithm:</p>

<ol>
<li>Generate a uniform(0, 1) random number <em>u</em>, then set <em>k</em> to 1.</li>
<li>Generate another uniform(0, 1) random number <em>v</em>.</li>
<li>If <em>k</em> is odd and <em>u</em> is less than <em>v</em>, or if <em>k</em> is even and <em>v</em> is less than <em>u</em>, return <em>k</em>.</li>
<li>Set <em>u</em> to <em>v</em>, then add 1 to <em>k</em>, then go to step 2.</li>
</ol>

<p>This algorithm generates an alternating sequence of a random length, and in doing so, it returns the number <em>n</em> with the following probability:</p>

<p><em>C</em>(<em>n</em>) = (1 &minus; <em>a</em><sub><em>n</em> + 1</sub>/(<em>a</em><sub><em>n</em></sub> * (<em>n</em> + 1)) ) * (1 &minus; &Sigma;<sub><em>j</em> = 0, ..., <em>n</em> &minus; 1</sub> <em>C</em>(<em>j</em>) )<br/>
&nbsp;&nbsp;&nbsp;&nbsp;= (<em>a</em><sub><em>n</em></sub> * (<em>n</em> + 1) &minus; <em>a</em><sub><em>n</em> + 1</sub>) / (<em>n</em> + 1)!,</p>

<p>where <em>a</em><sub><em>i</em></sub> is the integer at position <em>i</em> (starting at 0) of the sequence <a href="https://oeis.org/A000111"><strong>A000111</strong></a> in the <em>On-Line Encyclopedia of Integer Sequences</em>.</p>

<p>Inspired by the <a href="#The_von_Neumann_schema"><strong>von Neumann schema</strong></a> given earlier in this appendix, we can extend the algorithm to certain kinds of permutation as follows:</p>

<ol>
<li>Create an empty list.</li>
<li>Generate a uniform(0, 1) random number <em>u</em>, and append <em>u</em> to the end of the list.</li>
<li>If the items in the list do not form a valid permutation, return the number of items in the list minus 1.  Otherwise, go to step 2.</li>
</ol>

<p>This algorithm returns the number <em>n</em> with the following probability:</p>

<p><em>G</em>(<em>n</em>) = (1 &minus; <em>V</em>(<em>n</em> + 1)/(<em>V</em>(<em>n</em>) * (<em>n</em> + 1)) ) * (1 &minus; &Sigma;<sub><em>j</em> = 0, ..., <em>n</em> &minus; 1</sub> <em>G</em>(<em>j</em>) )<br/>
&nbsp;&nbsp;&nbsp;&nbsp;= (<em>V</em>(<em>n</em>) * (<em>n</em> + 1) &minus; <em>V</em>(<em>n</em> + 1)) / (<em>V</em>(0) * (<em>n</em> + 1)!),</p>

<p>where <em>V</em>(<em>n</em>) is the number of valid permutations of size <em>n</em>. For this algorithm, <em>V</em>(<em>n</em>) must be in the interval (0, <em>n</em>!] (thus, for example, this formula won&#39;t work if there are 0 permutations of odd size). <em>V</em>(<em>n</em>) can be a sequence associated with an <em>exponential generating function</em> (EGF) for the kind of permutation involved in the algorithm, and examples of EGFs were given in the section on the von Neumann schema.  For example, the first algorithm in this section expresses the special case of alternating permutations and corresponds to the EGF tan(&lambda;)+1/cos(&lambda;).</p>

<p>For either algorithm, the probability that the generated <em>n</em>&mdash;</p>

<ul>
<li>is odd is 1 &minus; 1 / EGF(1), or</li>
<li>is even is 1 / EGF(1), or</li>
<li>is less than <em>k</em> is (<em>V</em>(0) &minus; <em>V</em>(<em>k</em>)/(<em>k</em>!)) / <em>V</em>(0).</li>
</ul>

<p>For example, if the second algorithm treats sorted permutations as valid (making the EGF exp(&lambda;)), then the algorithm returns an odd number with probability 1 &minus; 1/exp(1). If that algorithm instead treats alternating permutations as valid (making the EGF tan(&lambda;)+1/cos(&lambda;)), then the algorithm returns an odd number with probability 1 &minus; 1/(tan(1)+1/cos(1)).</p>

<p><strong>Open Questions:</strong></p>

<ol>
<li>In the first algorithm, what is the probability of generating a random number <em>n</em> (or generating any of a set of values of <em>n</em>)&mdash;

<ul>
<li>if <em>u</em> (step 1) has an arbitrary (not necessarily uniform) distribution?</li>
<li>if <em>v</em> (step 2) follows the same but arbitrary distribution?</li>
<li>if the previous two conditions are given?</li>
</ul></li>
<li>In the second algorithm, what is the probability of generating a random number <em>n</em> (or generating any of a set of values of <em>n</em>)&mdash;

<ul>
<li>if the first random number in the list has an arbitrary (not necessarily uniform) distribution?</li>
<li>if each random number in the list beyond the first follows the same but arbitrary distribution?</li>
<li>if the previous two conditions are given?</li>
</ul></li>
<li>In the second algorithm, what distribution does the first number in the list follow when the algorithm returns <em>n</em> (or one of a set of values of <em>n</em>), with or without the conditions given in question 2?  For example, if the algorithm treats sorted permutations as valid, it is known since von Neumann&#39;s 1951 algorithm that that number has a truncated exponential distribution when the algorithm returns an odd value of <em>n</em>.</li>
</ol>

<p><a id=Other_Algorithms_for_exp_minus_lambda></a></p>

<h3>Other Algorithms for exp(&minus;&lambda;)</h3>

<p>The following two algorithms also simulate exp(&minus;&lambda;), but converge slowly as &lambda; approaches 1.</p>

<p>The algorithm in (Flajolet et al., 2010)<sup><a href="#Note1"><strong>(1)</strong></a></sup> calls for generating a Poisson(&lambda;) random number and returning 1 if that number is 0, or 0 otherwise.  The Poisson generator in turn involves generating a geometric(&lambda;) random number <em>G</em><sup><a href="#Note35"><strong>(35)</strong></a></sup>, then <em>G</em> uniform random numbers, then returning <em>G</em> only if all <em>G</em> uniform numbers are sorted (see &quot;<a href="#The_von_Neumann_Schema"><strong>The von Neumann Schema</strong></a>&quot; in the appendix).  The algorithm follows.</p>

<ol>
<li>Flip the input coin until the flip returns 0.  Then set <em>G</em> to the number of times the flip returns 1 this way.</li>
<li>If <em>G</em> is 0, return 1.</li>
<li>Generate a uniform(0, 1) random number <em>w</em>, and set <em>i</em> to 1.</li>
<li>While <em>i</em> is less than <em>G</em>:

<ol>
<li>Generate a uniform(0, 1) random number <em>U</em>.</li>
<li>If <em>w</em> is less than <em>U</em>, break out of this loop and go to step 1.</li>
<li>Add 1 to <em>i</em>, and set <em>w</em> to <em>U</em>.</li>
</ol></li>
<li>Return 0.  (<em>G</em> is now a Poisson(&lambda;) random number, but is other than 0.)</li>
</ol>

<p>An alternative version of the algorithm above doesn&#39;t generate a geometric random number at the outset.</p>

<ol>
<li>Set <em>k</em> and <em>w</em> each to 0.</li>
<li>Flip the input coin.  If the flip returns 0 and <em>k</em> is 0, return 1.  Otherwise, if the flip returns 0, return 0.</li>
<li>Generate a uniform(0, 1) random number <em>U</em>.</li>
<li>If <em>k</em> &gt; 0 and <em>w</em> is less than <em>U</em>, go to step 1.</li>
<li>Set <em>w</em> to <em>U</em>, add 1 to <em>k</em>, and go to step 2.</li>
</ol>

<p><a id=Sketch_of_Derivation_of_the_Algorithm_for_1_pi></a></p>

<h3>Sketch of Derivation of the Algorithm for 1 / &pi;</h3>

<p>The Flajolet paper presented an algorithm to simulate 1 / &pi; but provided no derivation.  Here is a sketch of how this algorithm works.</p>

<p>The algorithm is an application of the <a href="#Convex_Combinations"><strong>convex combination</strong></a> technique.  Namely, 1 / &pi; can be seen as a convex combination of two components:</p>

<ul>
<li><em>g</em>(<em>n</em>): 2<sup>6 * <em>n</em></sup> * (6 * <em>n</em> + 1) / 2<sup>8 * <em>n</em> + 2</sup> = 2<sup>&minus;2 * <em>n</em></sup> * (6 * <em>n</em> + 1) / 4 = (6 * <em>n</em> + 1) / (2<sup>2 * <em>n</em> + 2</sup>), which is the probability that the sum of two geometric(1/4) random numbers<sup><a href="#Note35"><strong>(35)</strong></a></sup> and one Bernoulli(5/9) random number, all of which are independent, equals <em>n</em>.  This corresponds to step 1 of the convex combination algorithm and steps 2 through 4 of the 1 / &pi; algorithm.  (This also shows that there may be an error in the identity for 1 / &pi; given in the Flajolet paper: the &quot;8 <em>n</em> + 4&quot; should probably read &quot;8 <em>n</em> + 2&quot;.)

<ul>
<li>Note 1: 9 * (<em>n</em> + 1) / (2<sup>2 * <em>n</em> + 4</sup>) is the probability that the sum of two independent geometric(1/4) random numbers equals <em>n</em>.</li>
<li>Note 2: <em>p</em><sup><em>n</em></sup> * (1 &minus; <em>p</em>)<sup><em>m</em></sup> * choose(<em>n</em> + <em>m</em> &minus; 1, <em>m</em> &minus; 1) is the probability that the sum of <em>m</em> independent geometric(<em>p</em>) random numbers equals <em>n</em> (a <em>negative binomial distribution</em>).</li>
<li>Note 3: <em>f</em>(<em>z</em>) * (1 &minus; <em>p</em>) + <em>f</em>(<em>z</em> &minus; 1) * <em>p</em> is the probability that the sum of two independent random numbers &mdash; a Bernoulli(<em>p</em>) number and a number <em>z</em> with probability function <em>f</em>(.) &mdash; equals <em>z</em>.</li>
</ul></li>
<li><em>h</em><sub><em>n</em></sub>(): (choose(<em>n</em> * 2, <em>n</em>) / 2<sup><em>n</em> * 2</sup>)<sup>3</sup>, which is the probability of heads of the &quot;coin&quot; numbered <em>n</em>.  This corresponds to step 2 of the convex combination algorithm and step 5 of the 1 / &pi; algorithm.</li>
</ul>

<p><a id=License></a></p>

<h2>License</h2>

<p>Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under <a href="https://creativecommons.org/publicdomain/zero/1.0/"><strong>Creative Commons Zero</strong></a>.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>
<p>
If you like this software, you should consider donating to me, Peter O., at the link below:</p>
<p class="printonly"><b>peteroupc.github.io</b></p>
<div class="noprint">
<a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=56E5T4FH7KD2S">
<img src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif"
name="submit" border="2" alt="PayPal - The safer, easier way to pay online!"></a>
<p>
<a href="//twitter.com/share">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
if("share" in navigator){
 document.getElementById("sharer").href="javascript:void(null)";
 document.getElementById("sharer").innerHTML="Share This Page";
 navigator.share({title:document.title,url:document.location.href}).then(
   function(){});
} else {
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
}
</script>
</body></html>
